---
layout : post
title : "BTS"
date : 2021-05-03 +0900
description : Local Planar guidance를 통해 monocular depth estimation에서 준수한 성능을 뽑아낸 BTS 논문의 간단한 리뷰입니다.
tag : [PaperReview]
---

### From Big to Small: Multi-Scale Local Planar Guidance for Monocular Depth Estimation



### Abstract

 single image에서 depth를 뽑는 것은 꽤나 도전적인 주제이다. 왜냐하면 많은 3D scene들이 같은 2D scene으로 projection되기 때문이다. 하지만, 최근 DNN base work들은 훌륭한 성능을 보여주고 있다. 보통 2가지로 구성이 되어있는데, dense feature extract를 위한 encoder와 desired depth를 뽑기 위한 decoder이다. encoder-decoder scheme에서 effective dense prediction을 위해 stride convolution, spatial pooling layer등 다양한 방법들이 사용되고 있다.

 이번 논문에서는 조금 더 효과적으로 feature를 extraction할 수 있는, local planar guidance layer를 소개한다. 이러한 방법으로 우리는 single image에서 dense depth를 뽑아내는 task에서 SOTA를 달성하였다.



### Introduction

 2D image에서 depth를 뽑아내는 것은 최근 robotics나 자율주행, scene understanding, 3D reconstruction에서 활발하게 사용이 되고 있다. multi-view, 서로 다른 light에서 촬영한 같은 이미지, stereo image pair등 다양한 방법으로 이를 해결해나가고 있지만, 아직 single image에서 dense depth estimation을 하는 것에는 많은 움직임이 필요하다.

 VGG, ResNet, DenseNet 등 다양한 강력한 network들이 dense depth estimation에 사용이 되고 있다. 이렇게 강해지는 것들 사이에서는 multi-scale network이나 skip connection 등 다양한 것들이 적용되어왔다. 최근, ASPP라는 것이 소개가 되었는데(image semantic segmentation을 위해서) 이러한 것들이 엄청 큰 receptive field를 불러오는 덕에 좋은 성능을 뽑아내어 주고 있다.

 우리는 이러한 과정을 통해 줄어든 image들을 다시 복원하고 확인하기 위해, 새로운 local planar guidance layer를 이용한다. 이는 현재까지의 network과 비교하면 다음과 같은 두 차이점이 있다. 첫번째는 output이 단순히 추가 되는 것이 아니라, 4-D의 plane coefficient를 학습해서 depth estimation에 도움이 되도록 한다. 두번째로, non-linear combination의 결과로 각자의 spatial cell은 training progress에서 분별이 되도록 학습이 된다.

 대충 결과는 아래와 같다.

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210503-1.PNG)



### Related Work

__2.1. Supervised Monocular Depth Estimation__

__2.2. Semi-Supervised Monocular Depth Estimation__

__2.3. Self-Supervised Monocular Depth Estimation__

__2.4. Video-Based Monocular Depth Estimation__



### Method

__3.1. Network Architecture__

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210503-2.PNG)

 feature extraction으로 뽑히게 되는 map은 H/8의 resolution이 되기 때문에, ASPP의 dilation rate를 조금 변경했다.

__3.2. Multi-Scale Local Planar Guidance__

 우리의 key idea는 효과적인 방법으로 internal feature와 final output사이의 explicit한 것을 바로 정의하는 것이다. 기존의 방식은 간단한 nearest neighbor 등의 방식을 이용해서 올린 반면, 우리가 제안하는 local planar guidance layer는 full resolution으로 마지막 depth estimation에서 사용이 된다.

 우리가 제안하는 LPG(Local Planar Guidance)는 직접적으로 depth vale를 크기에 상응하게 추측하는 것이 아니라는 점을 알아두자. 왜냐하면 final depth estimation의 term은 오로지 training loss에서만 게산이 되기 때문이다. 

 LPG를 만들면서 $k \times k$  region 에 대해서 우리가 왜 효과적인지 한번 생각해보자. 만약 일반적인 upconv를 사용하게 된다면, layer는 무조건 $k^2$  개수에 해당하는 updated value를 써야한다.(4개가 아니라) LPG의 경우에는 4개의 variable만 사용하기 때문에, upsampling에 비해 그 training time, inference time이 빠르다.

 그럼 어떻게 했는지 알아보자. 4D plane coefficient를 ray-plane intersection을 통해서 알아냈다.

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210503-3.PNG)

 위 식에서 $u, v$ 는 normalized coordinate이고, n의 경우 plane coefficient를 의미한다. 아래그림은 자세한 구조를 보여준다.

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210503-4.PNG)

 위 그림에 equation 4라고 되어있는데, 실수한 것 같다. 논문에서 equation 4는 loss계산하는 것이고, 아마도 아래 식을 사용하려했는듯하다.

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210503-5.PNG)

 생각을 조금 해보자. ray-plane intersection에서, plane의 마지막 성분(4번째, 즉 n4)는 depth를 의미하게 되고, n1~n3는 방향벡터이다. 여기에 u, v를 곱했다는 뜻은 plane의 normal vector에 camera image plane의 좌표, 즉 ray를 내적한 것이고, 이는 해당 plane에서 얼마나 떨어져있느냐를 의미하게 된다. n4는 plane이 축을 중심으로 얼마나 이동했느냐를 의미하게 되는데, 이 둘을 서로 나누게 되면 해당 pixel이 plane에서 얼마나 높이있는지를 비율로 알게 된다. 근데 이게 왜 depth를 도와주는지는 사실 잘 모르겠다. 아무튼 그렇다고 한다.

 채널은 총 3개인데, 첫번째와 두번째는 polar angle과 azimuthal angle을 구하고, 세번째 채널은 n4를 구하는데에 쓰인다. sigmoid를 통과한 n4는 $\kappa$ 와 곱해져 결과를 구하게 된다.

 마지막으로, guidance는 아래 식을 통해 이루어진다.

![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210503-6.PNG)

 위를 대충 그림으로 한번 더 그렸다.

![img10](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210503-10.PNG)



__3.3 Training Loss__

![img7](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210503-7.PNG)

 위 식을 다시 쓰면,

![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210503-8.PNG)

 위와 같이 된다. 여기서 추가적으로 square root를 씌웠다고 한다. $\alpha$ 는 10으로 설정했다.

 ![img9](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210503-9.PNG)



### Experiments

__4.1. Implementation Details__

__4.2. NYU Depth V2 Dataset__

![img11](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210503-11.PNG)

__4.3. KITTI Dataset__

![img12](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210503-12.PNG)

![img13](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210503-13.PNG)

__4.4 Evaluation Result__

 는 위 각 dataset에 기록

__4.5. Ablation Study__

![img14](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210503-14.PNG)

__4.6. Experiments with Various Base networks__

![img15](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210503-15.PNG)

__4.7. Qualitative Result__

![img16](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210503-16.PNG)



### Conclusions



### Reference

Lee, Jin Han, et al. "From big to small: Multi-scale local planar guidance for monocular depth estimation." *arXiv preprint arXiv:1907.10326* (2019).

