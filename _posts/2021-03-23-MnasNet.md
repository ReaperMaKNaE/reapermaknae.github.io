---
layout : post
title : "MnasNet"
date : 2021-03-23 +1100
description : 실용적인 NAS를 추구한 MnasNet 논문의 간단한 리뷰입니다.
tag : [PaperReview]
---

### MnasNet: Platform-Aware Neural Architecture Search for Mobile, CVPR 2019



### Abstract

 기존의 방식들은 hardware level에서 '가볍다'를 정의할 때 FLOPs를 기반으로 결정하는데, 해당 논문의 저자는 실제로 hardware에서 굴려봐야 알 것 아니냐! 라는 마인드로, FLOPs가 아닌 latency를 기반으로 mobile에 적합한 network을 찾았다고 한다. 이 방식을 __Automated Mobile neural architecture search(MNAS)__ 로 제안하였다고 한다. Google의 Pixel phone(구글에서 내는 스마트폰)에서 측정한 결과, MobileNetV2보다는 1.8배 빠르면서도 0.5% 높은 정확도를 보였고, NASNet보다는 2.3배 빠르면서도 1.2% 높은 정확도를 보였다고 한다.



### Introduction

 NAS에서 실제 hardware에 적용하기 위해 다양한 방식의 architecture search가 존재하는데, 대표적으로 depthwise convolution이나 group convolution 등이 있지만, 결국 그 한계는 존재했다. 그래서 이번에 논문의 저자가 제안하는 방식에서 이전 접근 방법들과의 다른 방법으로는 다음 두 가지를 뽑았다.

- __Latency aware multi-objective reward__
- __The novel search space__

 이전의 방식들은 대부분 FLOPs에 한해있는데, 실제 mobile 기기에는 들어가있는 칩마다 그 연산이 달라지기 때문에, reward를 accuracy와 inference latency의 compound 식으로 같이 주게 된다. 이 것이 위 첫 번째이고, 두 번째는 factorized hierarchical search space로, flexibility와 search space size 사이의 balance를 잘 맞춰나갈 수 있다.

 위와 같은 내용을 적용한 결과, inference latency와 mAP quality를 모두 improve했다고 한다.(기존의 방식들에 비해)



### Related Work

 몇몇 NAS에 대한 내용들도 있지만, MobileNetV2에서 사용한, 메모리에 효율적인 inverted residual and linear bottleneck인 MBConv block을 들고와서 이를 사용했다. 이전의 경우 모델을 가볍게 만들면서 CIFAR과 같은, 상대적으로 가벼운 dataset에서 NAS를 썼다면, 우리는 search process를 ImageNet이나 COCO와 같은 상대적으로 큰 것들에서 NAS를 썼다.



### Problem formulation

 기존의 hard constraint으로 잡은 경우, 일정 latency를 만족하는 순간 accuracy가 더 올라가지 않아서 reward로 주기에 적합하지 않았다. 이렇게 기존에 사용되던 방식을 hard constraint라 하면, 그 식은 아래와 같다.

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210323-1.PNG)

 위 식에 constraint을 조금 풀어, reward에 적합한 식으로 바꾼 것은 아래와 같다.

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210323-2.PNG)

 위 식들을 이용해 plot을 해보면 아래와 같이 나온다.

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210323-3.PNG)

 해당 논문에서 pareto optimal이라는 용어가 나오는데, 해당 용어의 경우 가장 최적의 solution을 말하는 것 같다. 이러한 방법을 soft constraint로 찾았다고 하는데, 저자의 의견으로는 위 식에서 사용한 weighted product도 좋지만 weighted sum도 괜찮을 것 같다는 의견이 있다.

 $\alpha, \beta$ 를 찾아야하는데, 간단한 실험 2번을 진행한 이후, 그 실험의 비례관계를 이용해서 두 parameter의 값을 찾았다고 한다.



### Mobile Neural architecture search

__4.1 Factorized Hierarchical search space__

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210323-4.PNG)

 RL base의 search algorithm을 사용했다고 한다. 위와 같이 7개의 block을 쌓고, 각 block의 내부 layer 개수는 independent하게 구성하며, 같은 block 내의 layer들은 모두 같은 구성을 띄고 있다. 이러한 구조를 __factorized hierarchical search space__ 라고 한다. 이러한 구조를 이용하였을 때 multiply-add의 개수를 논문에서 소개하고 있긴 한데, 해당 내용은 생략. 대충 kernel의 크기를 키우면, filter size는 그만큼 작아져야 balance가 맞다 라고 한다.(아마도 latency를 기준으로 잡아놓아서 그런 것 같다.)

 이 때 각 block의 search space는 다음과 같다.

- Convolutional ops __ConvOp__ : regular conv(conv), depthwise conv(dconv), MBConv
- Convolutional kernel size __KernelSize__ : 3x3, 5x5
- Squeeze-and-excitation ratio __SERatio__ : 0, 0.25
- Skip ops __SkipOp__ : pooling, identity residual, or no skip
- Output filter size
- Number of layers per block

__4.2 Search Algorithm__

 RL을 썼다고 하는데, evolution을 써도 search할 수 있다고 함. RL에서 자주 사용되는 PPO(Proximal Policy Optimization)을 썼다고 한다. 당시 사용한 reward는 다음과 같다.

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210323-5.PNG)

 

### Experimental Setup

 여러 평범한 setting이 있다고 생각했는데, 특별한 걸로는 __TPU v2__ 를 __64개__ 나 써서 __4.5 days__ 나 걸렸다고 한다.



### Results

__6.1 ImageNet Classification Performance__

![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210323-6.PNG)

 Target latency를 75ms로 두었다고 한다. 이를 A1이라 하고, 여기서 scaling만 진행한 model을 MnasNet-A2, MnasNet-A3라 한다. (A2, A3에 대한 자세한 내용은 나와있지 않음)

 SE를 왜 썼냐, 할 수 있는데, 이에 대한 ablation study를 진행하였더니 latency가 늘어나는 것에 비해 accuracy가 훨씬 늘어서 reasonable하다 라고 말하는 것 같다.

![img7](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210323-7.PNG)

__6.2 Model Scaling Performance__

![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210323-8.PNG)

 depth와 input size를 같은 model에서 scaling하는 것으로 실험을 해 본 결과, 그 성능이 더 좋다는 것을 확인할 수 있다. 해당 내용을 Table로 보면 아래와 같다.

![img9](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210323-9.PNG)

__6.3 COCO Object detection performance__

![img10](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210323-10.PNG)

 

### Ablation Study and Discussion

__7.1 Soft vs. Hard Latency Constraint__

 두 종류의 constraint이 실제로 어떻게 작용하는지를 그래프로 표현했다.

![img11](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210323-11.PNG)

  $\alpha, \beta$ 가 각각 0과 -1인 경우는 hard constraint을 의미한다. graph에서 볼 수 있듯, 여러 latency에서 포괄적으로 search했다는 것을 확인할 수 있다.

__7.2 Disentangling Search Space and Reward__

![img12](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210323-12.PNG)

 object는 reward를 줄 때 accuracy하나만 혹은 latency 하나만 주는 경우를 말한다. 위 table에서 두 가지를 모두 적용한 MNasNet이 더 좋은 결과를 보였음을 확인할 수 있다.

__7.3 MnasNet Architecture and Layer Diversity__

![img13](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210323-13.PNG)

 MnasNet-A1 architecture의 모습. 여기서 우측 (b)에 해당하는 block만으로 구성을 바꾸면 어떻게 되는지 아래 table에 나와있다. 당연하게도 search를 통해 찾은 network architecture가 accuracy와 latency 모두 좋은 것을 확인할 수 있다.

![img14](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210323-14.PNG)


### Conclusion



### Reference

Tan, Mingxing, et al. "Mnasnet: Platform-aware neural architecture search for mobile." *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2019.

