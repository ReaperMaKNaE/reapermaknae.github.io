---
layout : post
title : "3D RCNN"
date : 2021-03-02 +1100
description : 3D-RCNN논문의 간단한 리뷰입니다.
tag : [PaperReview]
---

### 3D-RCNN, CVPR 2018



__SUMMARY__ : 자동차의 형태가 다양하지 않기 때문에, shape prediction을 통해서 정해진 shape의 형태를 따오고, pose와 center, amodal box를 network에서 학습시킨 후 여러 loss를 통해 원하는 rendered image를 얻었다는 내용이 주로 이루어진 논문이다. Faster RCNN을 사용하긴 했으나, 단순히 뽑힌 RoI 만을 사용하지 않고, 그 오차를 보상해주기 위해 이런저런 것들을 추가했다고 한다. 그 결과 잘 나왔다고 한다.



### Abstract

 우리는 instance-level 3D scene understading을 위한 fast inverse-graphics framework을 만들었다. 우리는 image에서 모든 object instance의 full shape과 pose를 학습하는 Deep Convolutional network을 train한다. 우리의 방법은 autonomous driving과 같은 적용에 사용이 될 수 있는, scene의 간단한 3D representation을 제공한다. 많은 전형적인 2D vision output이 우리의 3D scene model의 output으로 rendering이 가능하다.(depth map이나 instance segmentation 등) 우리는 CAD model들에서 low dimensional shape-space를 배우는 것에서 어떤 class의 형태인지 알아내도록 한다. 이렇게 shape과 pose를 구하는 방법은 새로운 방법이며, 좀 더 좋은 3D 표현에 적용이 될 수 있다. segmentation과 같은 2D annotiations의 형태에서 오는 rich supervisory signal들을 구하기 위해, 우리는 2D supervision으로 학습하면서 3D 형태와 pose를 허용하는 differentiable Render-and-Compare loss를 제안한다. 우리는 우리의 방법을 Pascl3D+와 KITTI에서 평가했고, SOTA를 찍었다.



### Introduction

 복잡한 scene에서 scale을 하기 위한 inverse graphics approach의 방법으로, 우리는 4개의 key design을 선택했다.

- inverse graphics를 하는 것에 여러 network를 쓰는 것이 아니라, single unified end-to-end trained network를 사용했다.
- 우리는 scene을 shape과 pose를 가진 object instance들로 분해하여, network에 data 3D data가 available하던 말던 shape과 pose의 3D supervision으로 bootstrap이 될 수 있도록 한다.
- CAD model에서 오는 형태의 class-specific low-dimensional embedding을 학습함으로써 rich shape prior를 뽑아낸다.
- 2D RoI에 대해서 3D entities를 변경하는 것은 같은 방법으로 불가능하기 때문에, 우리는 RoI poolling layer를 이용해서 2D transformation을 수행하고 shape과 pose classifier로 feed한다.

 이와 같은 것들을 적용한 우리의 inverse-graphics network는 3D-RCNN이다.



### Related Work

 

### Method Overview

 우리의 목적은 given image에서 모든 object instance의 3D shape과 pose를 복원하는 것이다. 가장 큰 문제는 2D에서 어떻게 3D shape과 pose를 표현하는 것을 해결하느냐 이다. 우리는 일단 3D CAD model의 모임에서 low dimensional shape space를 만들어, class-specific shape prior로 사용하고, 이를 이용해 object shape을 찾아내는 것으로 방향을 잡았다. 이러한 표현은 작은 parameter set으로도 object class를 찾을 수 있게 해 준다. shape 유추에 있어서 문제는 특정 object instance에 대한 shape parameter의 set을 어떻게 예측할 수 있는지에 대한 frame을 만드는 것이다. 

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210302-1.PNG)

 우리는 2D image에서 3D shape과 pose를 구하는 문제를 해결하기 위해 deep network를 만들었다. 해당 network의 overview는 위 그림에 나타나있다. 마지막 pose와 shape prediction은 RoI에서 오는 fixed sized feature map cropped에서 구해지기 때문에, 타인중심의 것으로 기존의 ego-centric object pose representation을 re-parametrize하는 것은 중요하다.(이에 대한 내용은 이후에 나오게 되는데, 예를 들어 자동차의 문이 한쪽이 열려있다고 하면, ROI는 열린 문 까지 detect하는 경우가 생기게 되는데, 이런 경우 cropped 된 image의 중심은 object의 중심이 아니라 열린 차 문을 포함한 Area의 중심이 되기 때문에 둘의 차이는 꽤나 커지게 된다. 이를 해결해주기 위해서는 re-parametrize가 필요하다고 함.) 동일하게 중요한 것으로, network에 직접적으로 object의 location을 물어서는 안된다. 이는, 근본적인 ill-posed problem이기 때문이다. 우리는 자세한 object pose representation을 section 4.2에서 설명할 것이다. 실제 세상의 3D GT는 수집하는 것이 어렵다. 그래서 우리는 differentiable render-and-compare operation을 사용하여 학습 동안 image-level annotations와 존재하는 거대한 dataset을 이용했다. 우리는 ROI pooling에 의해 일어난 geometric distortion의 모델링에 의해 3D shape과 pose를 estimation하는 것을 사용했다. 결과적으로 나온 network는 end-to-end로 train이 가능하고, synthetic과 real image data모두 학습이 가능하다. pipeline의 첫번째 stage는 reder-and-compare operation을 따라서 scene의 간단한 3D parametrization을 얻기 위해 image를 de-rendering하는 과정을 수행한다. 한번 학습이 시작되면, model은 모든 object의 shape과 pose를 얻기 위해 매우 간단한 forward pass만을 필요로 한다.



### 3D Object Instance Representation

__4.1. Shape Space__

 우리는 3D CAD model의 거대한 collection 모음 형태를 사용할 수 있어 이를 사용하게 되었다. standard mesh 혹은 volumetric representation으로 되어 있는 3D model은 매우 높은 차원이다. 하지만, object instance에서 같은 category에 있는 모델들은 비슷한 형태의 모습을 취하는 경향이 있다. 따라서, 같은 object category의 instance에 해당하는 3D shape은 훨씬 낮은 dimensional manifold에 놓여있게 된다. 우리는 이러한 것을 3D CAD model의 collection에서 space를 embedding한 class-specific learning을 함으로써 얻어낼 수 있다. 학습된 embedding과 함께, 만들어진 형태의 문제는 관찰된 데이터(input)를 설명하기 위해 low dimensional embedding space로 단순히 연관된 점을 찾는다는 것이다.(학습된 data를 embedding 하면서, 단순히 keypoint matching만 하는 것을 의미하는 듯 하다.)

 주어진 CAD model의 collection에서, 우리는 그들의 axis를 align한다. 또한 shape vertices를 normalize하게 된다. mesh representation의 CAD model은 임의의 dimensionality와 topology를 가지고 있기 때문에, 우리는 각각을 고정된 voxel의 수 n을 가지는 volumetric representation $ s \in \mathbb{R}^n$ 로 변환했다. volumetric representation에서 각 voxel은 TSDF value로 저장이 된다.

 주어진 t개의 TSDF volume에 대해서, $S=[s_1, ..., s_t]$는 CAD mesh model에 의해 만들어지게 되고, 10가지의 dimensional shape basis인 $S_B \in \mathbb{R}^{n\times 10}$을 찾기 위해 PCA를 사용했다. n은 매우 크고 $n \gg t$ 이기 때문에, PCA의 dual form을 사용하는 것은 중요하다. (PCA는 아래 사이트를 참조하면 도움이 된다. 대충 아주 큰 dimension을 줄이기 위한 방법 중 하나인데, 대중적으로 사용되는 방법이란다.) 

PCA, Wikipedia, [https://en.wikipedia.org/wiki/Dimensionality_reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction)

우리가 $S_B$를 학습 하기만 한다면, 그 어떤 TSDF shape인 $s$라도 low dimensional shape parameter인 $\beta = S_B^Ts$ 로 mapping이 가능하다. 마찬가지로, 주어진 shape parameter $\beta \in \mathbb{R}^{10}$ 에 의해, 우리는 TSDF space로 다시 돌릴 수도 있다.($s=S_B \beta$를 이용해서) 몇몇 shape들이 아래 figure에 담겨져있다. 우리는 우리의 network가 image에서 low dimensional shape parameter $\beta \in \mathbb{R}^{10}$을 예측하도록 학습시켰다.

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210302-2.PNG)

 3D shape sapce modeling에 몇가지 다른 방법들이 있다. 우리는 간단하고 효율적이기에 PCA를 선택하게 되었다. 우리의 방법은 다른 어떤, low dimensional를 지원하는, 잘 표현된 shape을 포함한 parametric shape에도 유연하다. 우리는 위에 설명된 rigid object의 TSDF shape-space 뿐 아니라 articulated person에도 사용하기 위해 SMPL을 사용하였다.(SMPL은 아래 논문 참조, skinned multi-person linear model)

M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M. J. Black. SMPL: A skinned multi-person linear model. ACM Transactions on Graphics (TOG), 34(6):248, 2015.

 TSDF object 형태는 unit diagonal length이기 때문에, 우리는 KITTI의 3D box annotations의 average diagonal length로 계산한 calss-specific fixed scale을 적용했다.(대충 3D box에 scale을 추가한 내용) instance의 scale parameter를 학습하는 것은 중요함에도 불구하고, 우리는 현재의 framework을 단순하게 만들기 위해 이들을 사용하는 것을 피했다. 왜냐하면 multiple view를 사용하여 scale과 distance를 구하는 것이 조금 더 좋기 때문이다.

__4.2 Pose Representation__

 우리는 full-image camera frame에서 각 object instance의 pose parameter를 얻는 것이 흥미로웠다. 이는 3D orientation과 position을 결정하는 $P_{\epsilon} \in SE(3)$ 을 포함하게 된다. articulated object에 대해서, 이는 root pose $P_R$에 대해 상대적인 additional joint angle $j$를 포함하게 된다.

__Allocentric vs. Egocentric:__

 Object orientation은 camera에 대해서 쓰자면 egocentric(자기 중심적)이 되는 것이고, object에 대해서는 allocentric(타인 중심적)이 된다. orientation은 RoI feature-map의 top으로 예측이 되기 때문에, object의 중심을 잡는 것이 중요하다. 이 문장의 설명은 아래 그림을 참조하면 조금 더 이해가 잘 될 수 있다.

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210302-3.PNG)

 자동차가 camera의 axis에 수직인 straight line을 기준으로, 우측에서 왼쪽으로 움직인다고 하자. camera에 대한 자동차의 azimuth(방위각)은 변하지 않지만, cropped ROI에 나타난 자동차의 변화는 존재한다.(아래 그림의 (b)에서 보면, 자동차의 옆모습은 변하지 않음. 이를 타인중심(allocentric)이라 표현하는 것 같고, 자신 중심(egocentric)인 경우는 (a)처럼 되는 것.) 하지만 allocentric인 경우에는 ROI가 크게 변하지 않으므로, 학습에 적합하다. 우리는 view point의 관점에서 object orientation을 represent했다. viewpoint는 camera가 항상 object의 중심을 보고 있다고 가정하고, $v=[\theta, \phi, \psi]$ 로 표현이 되며, 각각 azimuth, elevation, tilt angle을 의미한다.

__Object Position:__

 cropped and resized RoI feature에서 온 것을 이용해 직접적으로 3D object position을 추정하는 것은 근본적으로 ill-posed problem이다. 사람들은 image에 잡힌 object가 무엇인지 알고, 배경은 그 object보다 큰 경우에만 single image에서 depth를 estimate 할 수 있다. 이러한 이유로 인해, 우리 역시 바로 object의 depth나 3D position을 구하지 않는다. 대신에, 우리는 2D projection을 알 수 있는지를 묻고, 2D amodal bounding box를 찾도록 묻게 된다. 이러한 방법은 훨씬 학습하기 쉽고, GT data도 얻기 쉬우며, KITTI나 Pascal3D+에서 이미 지원하는 것이다.

__Recovering Egocentric Pose:__

 주어진 object의 viewpoint v에 대해서, image의 object center c의 2D projection, amodal box a, camera intrinsics $K_c$에 대해서 우리는 쉽게 egocentric 3D object pose $P_E \in SE(3)$을 알 수 있다. 우리는 처음 rotation $R_C \in SO(3)$을 camera principal axis $[0,0,1]^T$ 와 object center projection $K_c^{-1}c$ 를 통한 ray 사이에서 계산하게 된다. 다음, $R_c = \Psi[0,0,1]^T, K_c^{-1}c$ 를 구한다. 여기서 $\Psi(p,q)$는 vector $p$가 $q$에 align하기 위한 rotation을 나타내고, 아래 식을 따른다.
$$
\Psi(p,q) = I + [r]_\times + [r]_{\times}^2/(1+p\cdot q), \qquad r = p \times q
$$
 $R_v \in SO(3)$ 를 viewpoint v의 rotation matrix form이라고 하자. camera d에서 object center distance는 amodal box a에 맞게 shape projection의 결과를 낳도록 연산이 된다. 그럼, camera에 대한 object pose $P_E$는 아래와 같다.
$$
P_E = \begin{bmatrix}R & t \\ 0^T & 1 \end{bmatrix} \; where \quad R=R_c R_v, \quad t=R_c[0,0,d]^T
$$


### 3D-RCNN Network Architecture

 우리의 방법은 Faster-RCNN network와 Network-on-Convolution meta-architecture를 적용했다.

__5.1. Striving for 3D Equivariance__

 그 어떤 Fast-RCNN++ system과 함께라면, 임의의 사이즈와 location $r = [x_r, y_r, w_r, h_r]$ 의 RoI에서 오는 feature는 shared feature map에서 오게 되고, fixed resolution $f_w \times f_h$ (대부분 14x14)로 정해지게 된다. RoI feature의 fixed size는 RoI feature의 맨 위에 있는 FC layer를 허용하게 해서 서로 다른 RoI에서 수행되는 같은 task에 대해 weight를 share하도록 한다. RoI-Pool 혹은 RoI-Align과 유사한 RoI feature extraction method는 original feature map을 fixed size로 가져가기 위해 2D transformation으로 변환한다. 이러한 2D transformation은 RoI box에 대해서 normalized 하기에, target들에 대해서는 필수적이다. network에 의해 target의 prediction을 가지게 되자마자, 그들은 final output을 위해 un-normalized back한다. 이는 2D instance segmentation에도 적용된다. 우리의 network에서 2D targets amodal-box와 center-proj를 위해서, 우리는 ROI box r에 대해 normalize를 진행했다.
$$
amodal-box \quad \hat{a} = [\frac{x_a-x_r}{w_r}, \frac{y_a-y_r}{h_r}, log \frac{w_a}{w_r}, log \frac{h_a}{h_r}]
$$

$$
center_proj \quad \hat{c} = [\frac{x_c-x_r}{w_r} \frac{y_c-y_r}{h_r}]
$$

 하지만, 2D normalization은 shape과 pose같은 3D target에는 불가능하다. 이러한 문제는 equivariance를 박살내게 된다. 우리는 이러한 문제를 아래 Figure에 담았다.

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210302-4.PNG)

 이러한 문제를 해결한 우리의 방법은 shape과 pose prediction을 위해 classifier에 2D transformation 정보를 전달하여 2D transform을 할 수 있도록 하는 것이다.

 우리는 물체를 직접적으로 관찰하는 가상의 RoI camera로 형성된 image에 RoI crop과 resize process를 설명한다.(다른 intrinsic을 가진) full-image camera intrinsics $K_c$를 알고 있다고 하면, 우리는 RoI의 camera intrinsic $K_r$은 다음과 같이 계산이 가능하다.
$$
K_c = \begin{bmatrix}f_x & 0 & p_x \\ 0 & f_y & p_y \\ 0 & 0 & 1 \end{bmatrix}, K_r = \begin{bmatrix} f_xf_w/r_w & 0 & f_w/2 \\ 0 & f_y f_h / r_h & f_h/2 \\ 0 & 0 & 1 \end{bmatrix}
$$
 full-image camera와 RoI camera $R_c$ 사이의 rotation은 section 4.2에 언급된 것과 같은 방법으로 계산이 된다. pure rotation $R_c$에 놓여진 두 카메라 $K_c$와 $K_r$은 infinite homography matrix와 연관이 있게 된다.($H_\infin = K_r R_c^{-1}K_c^{-1}$) $H_\infin$ 은 RoI pooling layer에 의해 2D transformation을 잡고, 추가적으로 original camera에 의해 생기게 되는 perspective distortion도 잡는다. 이후 우리는 $H_{\infin}^{-1}$의 9개 parameter를 3D shape과 pose prediction에 사용하기 전, RoI feature에 concat을 하게 된다. 우리는 이를 $H_\infin$ concat이라고 한다.(Figure 1 참조) shape and pose target인 우리의 network가 학습하는 것은 original 3D shape과 pose parameter $[v^T, j^T]^T$ 를 예측하도록 하는 것이다. $H_{\infin}^{-1}$ 의 추가적인 정보와 함께 3D shape과 pose를 학습하기엔 더 좋은 기회를 가지게 된다.

__5.2. Direct 3D supervision__

 pose와 shape에서 단순히 continuous regression을 사용하는 것도 가능한 반면에, 첫번째 discretizing output sapce에서 bin으로 수행되는 classification loss가 훨씬 좋긴 하다. classification은 문제를 과도하게 parametrize하기에, network가 task를 학습하는 것을 더욱 유연하게 만들어 준다. 또한, output이 정해진 boundary안에 국한되도록 만들 수도 있다. 예를 들어, Pose angle은 $[-\pi, \pi]$로 bound 되고, 각 shape parameter들은 $[-3\sigma, 3\sigma]$ 로 boundary가 잡혀있다. 하지만, classification의 단점은 정확도가 양자화되어있다는 것에 한계가 있다. 우리는 classification과 regression loss를 조합하여 최고를 취한다. 처음에는 soft arg max를 수행하고, cross-entropy classification loss와 L1 regression loss를 예측된 soft arg max probability에 씌우게 된다.

 각 shape parameter $\beta \in \boldsymbol{\beta}$ 에 해당하는 $b$ bins와 $\tilde{\beta_p}$ 가 $p$-th bin의 중심이라고 한다면, 우리는 $\beta$를 다음과 같이 계산할 수 있다.
$$
\beta = \sum_{p=1}^{b} P_{\beta}^p \tilde{\beta}_p, \quad P_{\beta}^{p} = \frac{exp(FC_{shape}^p / T_{shape})}{\sum_{q=1}^{b}exp(FC_{shape}^q / T_{shape})}
$$
  위 식에서 $P_{\beta}$는 $FC_{shape}$의 activation에 놓여있는 temperature $T_{shape}$의 soft arg max를 적용한 결과이다.

 Pose target은 periodic한 angle이기 때문에, 우리는 complex expectation을 취해야 한다. 따라서, angle estimate은 아래와 같다.
$$
\theta = arg(\sum_{p=1}^b P_{\theta}^p e^{i \tilde{\theta}_p}), \quad \tilde{\theta}_p = 2\pi \frac{p-0.5}{b}-\pi
$$
 위 식에서 $P_\theta$는 이전과 유사하게, $FC_{pose}$의 activation 에 있는 temperature의 soft max를 씌운 결과이고, $\tilde{\theta_p}$는 $p$-th bin의 중심이다.

  shape과 pose target 둘 다를 위해, 우리는 soft max output에 cross-entropy loss를 합치고 expectation 이후 연속적인 output에 L1 loss를 입혔다.
$$
L_{shape} = -log(P_{\beta}^* + ||\beta -\beta^* ||_{L1}) \\ L_Pose = -log(P_{\theta}^*)+||\theta - \theta^*||_{L1}
$$
 위 식에서 $\beta^*$ 와 $\theta^*$는 shape과 pose parameter의 GT이고, $P_\beta^*$와 $P_\theta^*$은 GT bin의 soft max probability이다.

  center-proj와 amodal-bbx target은 각도와 같이 bound가 될 필요가 없다. 이러한 것은 이미 detection module에서 anchor를 통해 이미 discretization process를 거친 RoI로 normalized 된다. 그래서 우리는 단순하게 L1 loss만을 추가했다.
$$
L_{center-proj} = ||\hat{c}-\hat{c}^*||_{L1}, \quad L_{amodal-bbx} = || \hat{a}-\hat{a}^*||_{L1}
$$
  우리는 학습동안 temperature parameter는 0.5로 초기화했다. soft arg max가 아닌 arg max를 이용한 shape and pose estimation은 Render-and-Compare layer에서 gradient back-propagate를 막아준다. 우리가 사용하는 loss는 다른 방법들과는 조금 다른데, arg max와 같은 non-differentiable operation을 피한 것이다.

__5.3. Render-and-Compare Loss__

 object의 단순한 3D representation을 잡자마자, known camera calibration에서 render될 수 있고 instance segmentation이나 depth-map 처럼 2D annotion을 비교할 수 있게 된다. 이러한 것은 network가 2D GT data에서 조금 더 쉽게 학습이 될 수 있는 것을 도와준다.

 각 RoI에 대해서, 우리는 GT 2D segmentation mask $G_s$와 2D depth map $G_d$를 가지게 된다. 각 RoI에서 오는 3D shape과 pose prediction에서 우리는 상응하는 segmentation mask $R_s$와 depth map $R_d$를 render한다. 또한 우리는 loss에 기여하지 않는 pixel들의 집합인 binary ignore mask $I_s$와 $I_d$를 알고 있다.이는 undefined depth value, occluded, no label과 같은 pixel을 무시하는 것에 유용하다. Render-And-Compare loss는 다음과 같이 계산될 수 있다.
$$
L_{render-and-compare}=d_J(R_s,G_s;I_s) + d_{L2}(R_d, G_d,;I_d)
$$
 위 식에서 $d_J = 1 - J(R_s, G_s;I_s)$ 는 Jaccard distance이다.

 하지만, standard 3D rendering은 differentiable 하지 않다. 우리는 gradient를 가정하는 것에 있어 유한한 차이를 사용했다. 이는 GPU에서 non-photorealistic rendering이 빠르고, 우리의 3D object representation의 차원이 상대적으로 낮기 때문에 실현가능하다. CUDA-OpenGL을 같이 사용하면 GPU-CPU를 왔다갔다 하기에, 이를 피하고 싶어 OpenDR이나 SPSA와 같은 scheme이 아닌 것들을 사용하고 싶었다. TSDF shape space를 사용했을 때, 우리는 volume ray-casting을 사용했다.

 Render-and-Compare loss는 그 어떤 새로운 learnable parameter를 유발하지 않고, object의 shape과 pose parameter에서만 joint structured loss를 제공한다. shape과 pose representation 둘 모두 low dimensional이고 rendered 될 수 있기 때문에, numerical derivatives를 이용한 gradient 계산은 실현이 가능하다. 우리는 object representation의 compactness(low dimensionality)와 빠른 rendering이 스스로 바람직한 property인 것을 알아야 한다.

__5.4. Training and Inference__

__Joint Multi-task Loss:__

 마지막 joint loss 목적인 $L_{joint}$는 모든 예측된 loss들의 합으로 이루어진다. (targets={shape, pose, center-proj, amodal-bbx, render-and-compare}). 그래서, $L_{joint}=\sum_{\tau \in targets} \lambda_\tau L_\tau$ 를 사용하고 여기서 각 term에 대한 balance를 맞추기 위해 hyper-parameter인 $\lambda_\tau$를 사용한다. data source에 의존한다 할 때, 몇몇 loss는 unavailable 하게 될 것이다. 예를 들자면, real-world data-sample에서 GT shape은 가지고 있지 않다.(synthetic은 가지고 있음)

__Training:__

 ImageNet으로 pre-training, KITTI, PASCAL 사용

__Inference:__



### Experiments

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210302-5.PNG)

__6.1. Analysis on Pascal3D+dataset__

![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210302-6.PNG)

![img7](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210302-7.PNG)

__6.2 Analysis on KITTI dataset__

![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210302-8.PNG)

![img9](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210302-9.PNG)



### Conclusion



### Reference

Kundu, Abhijit, Yin Li, and James M. Rehg. "3d-rcnn: Instance-level 3d object reconstruction via render-and-compare." *Proceedings of the IEEE conference on computer vision and pattern recognition*. 2018.

PCA(dimensionality reduction), Wikipedia, [https://en.wikipedia.org/wiki/Dimensionality_reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction)

M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M. J. Black. SMPL: A skinned multi-person linear model. ACM Transactions on Graphics (TOG), 34(6):248, 2015.