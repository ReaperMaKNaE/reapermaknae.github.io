---
layout : post
title : "OverFeat"
date : 2020-12-09 +0900
description : CNN을 이용하여 recognition, localization, detection을 해결했던 OverFeat에 대한 간단한 review입니다.
tag : [PaperReview]
---

### OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks



 이번에 살펴볼 논문은 OverFeat이다. RCNN에 밀려서 빛을 발하진 못했지만, 아무튼 한 번 볼만할 것 같아서 찾아보게 되었다.



 시작하기 전, ML/image 관련해서 쓰이는 용어.

 __bootstrapping__ : random으로 sampling하는 것(대충 아무거나 뽑아서 학습 시키는 것이 ML에서 쓰이는 의미, overfitting 방지. 이를 __bagging__이라고도 함.)

 __image crop__ : 이미지의 일부를 잘라내는 것. data augmentation을 하는 데 사용됨.(이 논문에선 256x256 pixel image의 좌상, 좌하, 우하, 우상, 가운데 221x221 pixel image crop + 각각의 좌우반전을 포함해 10배의 dataset을 사용)

 __subsampling ratio__ : layer들을 통과 하고 난 후의 output과 이전의 input을 비교하였을 때 output 한개의 data가 가지게 되는 원래 data들의 개수(말이 좀 이상하다. 예를 간단하게 들면, 10x10x3의 이미지를 CNN을 쭈우우욱 통과시킨 결과가 2x2(4개의 vector) 이라고 한다면, 기존에 10x10x3에 해당하는 300개의 데이터가 4개로 줄었으니, subsampling ratio는 300/4 = 75개이다.)

 

 일단 먼저 정리를 하면, 이 친구는 classification, localization, detection 세 분야를 모두 ConvNet을 적용해서 할 수 있다는 것을 보여주었다.

 이것을 어떻게?

1. __Classification은 AlexNet에서 약간 변형한 형태로, sliding window를 곁들인 방식__
2. __localization은 classification과 거의 유사한 방식에 bbox를 포함한 GT 데이터를 포함해서 학습__
3. __Detection은 localization과 유사하나, background를 detect할 수 있도록 negative 요소를 추가로 학습__

이다.

 위 세가지의 다른 점은,

 __classification__은 사진에 들어있는 것이 무슨 종류인지 분류를 하는 것이고,

 __localization__은 사진에 들어있는 종류가 어디에 위치하는지 파악하는 것이고,

 __detection__은 쓸데없는 것을 잡지는 않는지 확인하는 것이다.(기본적으로 localization과 유사할 수 밖에 없다)



 그럼 각 과정을 한번 살펴보자.

__첫번째, Classification__

 Classification의 경우는 두가지 모델을 사용했다. 하나는 fast model, 다른 하나는 accurate model. 차이는 아래에서 확인가능하다.

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20201209-1.png)

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20201209-2.png)

 accurate model은 한개를 더 넣은 케이스다. 논문에서 주로 이야기를 하는 것은 Accurate model에 대한 것 이므로, 이에 대해 살펴보면,

  input size 221x221x3을 시작으로, 그림에 적힌 filter를 넣고 굴리면 Table 3처럼 결과를 따라가게 된다. 

 *에게? 그냥 layer 조금 바꾼게 끝이냐?*

 아니다. 추가한게 있다. 논문을 읽다보면 저자가 __align__되어 있지 않다고 한다.

 저자가 주장하는 바에 따르면, CNN을 계속 통과시켜 나중에 나온 결과를 봤을 때 한 개의 pixel이 기존 이미지에서 수십 수백(~~아무튼 많은~~) pixel에 해당하는 정보를 가지게 되는데, 이 때 결과물의 subsampling ratio가 36이면 마지막 feature map(마지막 결과물) 1 pixel이 원래 이미지의 36pixel에 해당하는 value들의 포괄적인 의미를 가진다! 라는 것이 __틀렸다__ 라고 한다.

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20201209-3.png)

 위 그림을 보자.

 기존 방식의 결과물은 delta가 0이다. 저게 뭐냐면, 20 만큼의 길이에다가 3만큼의 길이인 kernel을 stride 3을 주고 pooling을 진행했다고 하는 것이다(빨간색). 그럼 결과는 6개의 데이터(빨간색)가 나오게 되는데, 이게 overfeat 가 쓰이기 전까지 주로 쓰이는 방법이었다고 한다.

 저자는 여기서, stride를 3으로 그대로 주되, delta를 넣어서 이미지의 모든 pixel에 대해 pooling을 한번씩 더 진행하고(파란색, 녹색), 얘내들을 또 layer (d)를 보낸 다음 2개의 데이터로 줄이고, 이걸 합친 결과물을 쓰면, __align__된 결과라고 하고 있다.

 stride를 1로 준 것과는 결과가 조금 다르긴 할 텐데, 음... 일리는 있다. 위 방식을 사용하면 모든 pixel에 대한 정보를 들고 올 수 있기에, 저자의 결과물이 조금 더 __resolution__이 좋다고 한다.

 그리고 그 결과는 아래와 같다.

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20201209-4.png)

 ![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20201209-5.png)

 Table 2에서 말하는 scale의 경우, Table 5에 나와있고, coarse는 기존의 방법(delta = 0인 케이스만 고려한 방법)이며 fine stride는 저자가 사용한 방법이다.

 fast model과 accurate model은 이 포스팅의 첫번째와 두번째 이미지이고, Table 2에서 아래쪽에 해당하는 7 @@@ models 는  7개의 @@@model을 돌린 결과의 평균을 사용했다는 뜻이다.



 그리고 기존의 CNN base model은 fully connected가 되기 위해 이전의 layer크기가 모두 fix되어 있어야 하고, 이로 인해 input image의 size가 고정이 된다는 문제점을 들었다.

 이걸 sliding window방식을 쓰게 되면, 결과물의 크기만 달라질 뿐 결국 같은 model을 이용해도 상관이 없다고도 주장하였다. 이건 좀 문제가 될 수 있지 않을까 한데, 위 Table 5를 보면 size가 종류별로 있으면서 각 input size마다 model이 달라지게 되는데, 결국 sliding window를 하게 되면 sliding window하는 만큼만 달라지기 때문에 같은 size의 image를 비교하는 것은 서로 다르게 나온 결과물끼리 비교하면 된다는 뜻이다.

![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20201209-6.png)

 그림을 더해서 조금 정리하면, 245x245 이미지는 245x245끼리만 비교하면 되기 때문에 결과물의 사이즈는 어짜피 같아서 상관이 없고, 389x461이미지는 389x461끼리만 비교하면 되기 때문에 결과물의 크기가 245x245랑 달라도 문제가 없단 뜻이다.

 왜 문제가없느냐 하면 그냥 대회에서 다른 사이즈의 이미지끼리 비교하는 케이스는 없었나보다. (RCNN을 공부하고 와서야 보이지만, 사실 굳이 이럴 필요가 없다는 것도 우리는 알고 있긴 하다.RCNN에서는 2000개의 proposal을 몽땅 227x227로 바꿔버렸으니)



__두번째, Localization__

 classification과 비슷한데, network가 약간 바뀌게 된다.

 바로 위 그림에서 classifier만 bbox를 포함해서 학습시킬 수 있는 layer로 바꾸는 것.

 ![img7](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20201209-7.png)

 그리고 바뀐 layer가 위 그림과 같다.

 그렇게 되면 나오게 되는 xmin, xmax, ymin, ymax(bbox 4개의 좌표)중에서 뭐가 더 맞느냐? 를 결정해야 하는데, 그 방법이 아래 방법이다.~~*(사실 나도 잘 모른다. 일단 대충 아다리 맞게 해석만 했을 뿐)*~~

 ![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20201209-8.png)

 (a) 일단 각 scale에서 가장 높은 빈도를 가진 class를 C_s에 저장한다.

 (b) 그리고 이 C_s에 들어있는 예측된 bbox들을 몽땅 B_s에 저장을 한다.

 (c) 이 bbox들을 또 새로운 곳인 B에 몽땅 저장한다.

 (d) 이 저장된 B에서 대충 아무거나 b1, b2를 뽑는다. 뽑고 나서 match_score([GT - ground truth- 와 거리차이] + [b1 b2의 IoU])가 가장 작은 b1* b2*을 뽑는다.

 (e) 이 최소값이 임계값(t)보다 크면, b1* b2*을 return하고 치우고, 만약 작거나 같다면

 (f) b1, b2를 B에서 빼고, b1* b2*의 평균(box_merge)을 다시 B에 집어넣고 또 아무거나 뽑은다음 loop를 돈다.

 위 방법을 계~~~~속 하다보면 결국 임계값 t를 넘어서는 b1* b2*은 우리가 원하는 값에 거의 딱 드러 맞게 된다! 가 localization에서 포인트로 잡은 bbox 치는 방법이다.



__세번째, Detection__

 Localization과 똑같다. background만 추가로 학습시켰다고 한다. 이 때 background는 아무거나 뽑거나 가장 아닌 것 같은것?(offending이 이런 뜻인질 모르겠다. 대충 사전을 뒤벼보면 가장 문제가 되는 부분을 의미하는 것 같다.)을 추가로 넣어서 학습 시켰더니 괜찮아졌다고 한다.



 마지막인 Discussion에서는 몇가지 방법을 추가하면 해결이 될 것이다! 라고는 했고 실제로도 그럴듯 하지만, 이후의 대회에서 더 좋은 방법들이 막 등장하면서 결국 OverFeat는 가라앉고 말았다. 그러므로 여기에서 끝.



### Reference

Arc Lab님의 OverFeat 논문 요약, [https://arclab.tistory.com/226](https://arclab.tistory.com/226)

Martin님의 OverFeat 논문 요약, [https://dhhwang89.tistory.com/135](https://dhhwang89.tistory.com/135)

리온피플, OverFeat 논문 요약, [https://m.blog.naver.com/PostView.nhn?blogId=laonple&logNo=220752877630&proxyReferer=https:%2F%2Fwww.google.com%2F](https://m.blog.naver.com/PostView.nhn?blogId=laonple&logNo=220752877630&proxyReferer=https:%2F%2Fwww.google.com%2F)

해외의 OverFeat 논문 리뷰, [https://wiki.math.uwaterloo.ca/statwiki/index.php?title=overfeat:_integrated_recognition,_localization_and_detection_using_convolutional_networks](https://wiki.math.uwaterloo.ca/statwiki/index.php?title=overfeat:_integrated_recognition,_localization_and_detection_using_convolutional_networks)

bootstrap, learning carrot 님의 '부트스트랩에 대하여(Bootstrapping)', [https://learningcarrot.wordpress.com/2015/11/12/%EB%B6%80%ED%8A%B8%EC%8A%A4%ED%8A%B8%EB%9E%A9%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC-bootstrapping/](https://learningcarrot.wordpress.com/2015/11/12/%EB%B6%80%ED%8A%B8%EC%8A%A4%ED%8A%B8%EB%9E%A9%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC-bootstrapping/)