---
layout : post
title : "DeMon"
date : 2021-01-07 +0900
description : 최초로 unconstrained 된 image pair에서 depth와 camera motion을 얻은 Depth and Motion Network for Learning Monocular Stereo 논문의 간단한 리뷰입니다.
tag : [PaperReview]
---

### DeMoN: Depth and Motion Network for Learning Monocular Stereo, CVPR 2017



 Fast RCNN에서 좀 훌쩍 넘어 뛰었는데, 연구실에서 연구 target으로 잡은 것을 부랴부랴 따라잡으려고 이논문 저논문 무작위로 읽다보니 정리를 좀 해야 할 필요성을 느껴서 다시 작성하게 되었다.



 DeMoN의 경우 최초로 Unconstrained 된 (서로의 위치를 모르는 상태) 한 쌍의 Image에서, Image의 Depth와 Pose를 estimation하는 것에 성공한 network이다.



 연구실에서 간단하게 정리했던 내용은 다음과 같다.



### Introduction

incorrect estimate of the camera motion -->> WRONG depth predictions.

보통 pose estimation할 때 keypoint detection같은거 쓰는데, outlier나 textureless에선 성능이 박살남.

그래서 우리는 전형적인 SfM pipeline에서 벗어나, unconstrained된 이미지쌍에서 depth와 motion을 얻는 것을 성공.

motion parallax(모션 시차)를 적용해서 single images만으로도 꽤나 준수한 depth map을 뽑아 냈음.

한개의 image만으로 training 시키는 것이 shortcut이다보니 문제가 발생했다는데... 잘 이해가 안간다.

아무튼 이러한 shortcut을 없애기 위해, accurate depth map과 camera motion estimate를 얻었음.

optical flow를 방지하기 위해, 우린 무조건 2개의 image input을 필요로 함. 따라서 FlowNet Architecture를 따옴.

FlowNet Architecture를 따왔다는데 내가 이걸 대충 봐서 그런지 전혀 유사해보이진 않음.

아무튼 아래는 해당 네트워크.

 ![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210107-1.png)



### Related Work
consolidate - 굳히다, 통합하다.

이런저런 사람들이 과거에서부터 이것저것 섞어서 연구해왔음. outlier를 없애기 위한 RANSAC, 새로운 방법을 추가한 LSD-SLAM, DTAM 등등. 이후 single image에서 depth estimation을 뽑는 ConvNet을 시작으로, SfM system을 추가하거나, 등등 했음.

bootstrap net : 데이터 셋 내 데이터 분포가 고르지 않은 경우, 랜덤 샘플링을 통해 training data를 늘리는 방법.

주로 통계쪽에서 사용이 되었는데, ML에도 적용을 할 수 있다고 함.

해당 내용은 learning carrot님 께서 작성하신 "부트스트랩에 대하여(Bootstrapping)" 내용을 참고하는 것이 좋은 것 같다.

[https://learningcarrot.wordpress.com/2015/11/12/%EB%B6%80%ED%8A%B8%EC%8A%A4%ED%8A%B8%EB%9E%A9%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC-bootstrapping/](https://learningcarrot.wordpress.com/2015/11/12/%EB%B6%80%ED%8A%B8%EC%8A%A4%ED%8A%B8%EB%9E%A9%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC-bootstrapping/)

### Network Architecture
 ![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210107-2.png)

architecture는 크게 3가지로 나누어져있음: bootstrap net, iterative net, refinement net.

Bootstrap Net: 첫번째 hourglass network는 optical flow와 confidence map 그림.

두번째 hourglass network는 이 data를 받아서 depth, surface normal, camera motion을 뽑아냄.

Iterative Net: 얘들은 일단 depth, normal, motion estimation의 성능을 올리기 위해 존재함.

 ![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210107-3.png)

iteration이 끝날 때 마다 loss가 좋아지는 걸 확인이 가능함(optical flow, depth, normal, motion 전부)

Refinement Net: 이전 network들의 결과는 resolution이 별로 안좋음. 그 때 이 refinement net으로 resolution을 복구함.

처음에 input으로 받은 full resolution image, NN upsampled depth and normal field를 input으로 받는 network.

그래서 low resolution image를 refine해서 상당히 좋은 품질의 image를 얻을 수 있음

 ![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210107-4.png)

### Depth and Motion Parameterization

Network는 first view -> depth map, second view -> camera motion 추출임. (그랬었나..?)

여튼 camera motion을 모른 채 image에서 뭔갈 뽑아낸다는 것(recon이라던지, depth라던지)는 scale에 따라 달림.

network는 depth가 아닌, inverse depth를 예측함.

이러면 엄청 멀리있는 point의 위치를 정확히 모르는 것 처럼 아무튼 대충 그냥 사람이랑 비슷하게 된다.

### Training Procedure
5.1. Loss functions

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210107-5.png)

Network가 내뱉는 output은 정말 다른 dimension을 가지고 있기 때문에, scaling을 잘 해 줘야함.

Point-wise losses. 이게 뭐지? loss를 output마다 따로 둔 걸 말하는 것 같다.

아무튼 inverse depth, surface normal, optical flow, optical flow confidence 4개의 loss가 있음.

optical flow confidence의 경우 GT를 계산해야하는데, optical flow에서 GT를 빼내고 거기서 추가로 계산함.

Motion losses도 따로 있다. rotation과 translation임. 근데 rotation은 저렇게 되나? magnitude라는데 rotation의 magnitude를...?

(rotation의 크기가 별로 안크다면 별 상관이 없긴 한데.... 3 parameters라는 걸 보면 RPY(Roll Pitch Yaw)인 듯 하다.)

Scale Invariant gradient Loss. 이거 DeepSFM에서도 본 것 같은데, 주변과의 차이를 약간 보상해주는 듯 하다.

depth map에 한정해서 이걸 쓰는데, 이걸 추가하면 아무튼 결과가 더 좋아짐. gradient를 보상해주는 듯.

Figure 10에서 각 Loss의 설정에 따른 결과물의 resolution을 확인할 수 있다.

5가지의 다른 spacing에 대해 썼다는데.... 저게 pixel을 나타낸 거겠지?

Weighting. 각 loss에 대한 weight를 따로 뒀음.

5.2. Training Schedule

Caffe framework를 썼다네...?

처음은 batch size 32로 250k 훈련시킴.(bootstrap net과 iterative net)

scale invariant loss는 10k iteration후에 적용 시작.

iterative net의 encoder-decoder part만 학습시킴.

### Experiments

6.1. Datasets - SUN3D, RGB-D SLAM, MVS, Scenes11, Blendswap, NYUv2 등.

6.2 Error Metrics - scale invarient error를 적용함. 는 사실 다 다름.



![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210107-6.png)



depth는 scale invariant error를 쓴게 맞는데, 다른 애들이랑 비교할 때에는 L1-rel이나 L1-inv를 썼음.

camera motion estimation에서는 단순한 차이만 본 듯.

optical flow의 정확도는 ERE(endpoint error)의 평균을 썼다고 함.(Euclidean norm of 예측과 실제 flow vector 차이)

6.3 Comparison to classic structure from motion

강력한 기준들을 바탕으로 비교를 좀 해봤단다. SIFT를 base로 한 놈들이나, FlowFields optical flow method를 base로 한 놈들.

normalized 8 point algorithm와 RANSAC을 이용해서 essential matrix 계산했음.

정확도를 조금이라도 더 높이기 위해, ceres library를 써서 reprojection error를 최소화.

plane sweep stereo와 Hirschmueller가 optimization할 때 썼던 방법을 이용해서 depth map을 만들긴 했음.

뭐 기타 더 있음. 근데 생략.

여러 데이터셋에서 비교를 했는데, MVS에서만 성능이 좀 그닥이고(그래도 훌륭했음), 나머지들에게선 좋았음.

근데 이건 MVS가 훌륭한 texture를 가진 dataset이라 아무래도 좋을 수 밖에 없는 것 같다.

그래도 우린 input size가 작은데에도 이정도면 뭐 괜찮지. 아니냐.

(다른 애들은 input이 640* 480인데, DeMoN은 256*192 input을 썼음.)

6.4 Comparison to depth from single image

motion parallax를 설명하기 위해, 다른 single image depth estimation method들과도 비교를 해봤다.

그게 Figure 8에 나와있음. Base-Oracle이라던지 Eigen이라던지 등이랑 비교했는데 결과는 DeMoN이 더 좋았음.



![img7](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210107-7.png)



6.4.1 Generalization to new data

scene-specific prios learned는 쓸모없거나 오히려 더 안좋음(??) 이게 뭐지?

뭐 아무튼 이전에 없던 data를 기준으로 test해본 결과 뭐 잘 나왔다. 끝.

6.5. Ablation studies - grad, norm, flow, confidence에 대해서 ablation studies 진행했음.

### Conclusions and Future Work

_생략



 

### Reference

Ummenhofer, Benjamin, et al. "Demon: Depth and motion network for learning monocular stereo." *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*. 2017.

Bootstrap, "부트스트랩에 대하여(Bootstrapping)", [https://learningcarrot.wordpress.com/2015/11/12/%EB%B6%80%ED%8A%B8%EC%8A%A4%ED%8A%B8%EB%9E%A9%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC-bootstrapping/](https://learningcarrot.wordpress.com/2015/11/12/%EB%B6%80%ED%8A%B8%EC%8A%A4%ED%8A%B8%EB%9E%A9%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC-bootstrapping/)

