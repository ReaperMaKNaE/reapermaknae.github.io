---
layout : post
title : "DISN"
date : 2021-02-13 +2000
description : DISN, Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction 논문의 간단한 리뷰입니다.
tag : [PaperReview]
---

### DISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction, NIPS 2019



### Abstract

 한개의 이미지에서 3D recon을 하는 것은 오랫동안 연구되어온 문제이다. 이번 연구에서, 우리는 DISN이라 불리는, underlying SDF를 예측하여 2D image로부터 detail이 풍부한 3D mesh를 뽑아내는 network를 제안한다. global image feature를 다루기 위해, DISN은 2D image에서 각 3D point의 위치를 구하도록 projected 된 location을 예측하게 된다. 그리고 image feature map에서 local feature들을 뽑아낸다. global한 feature와 local한 feature를 합치는 것은 SDF의 정확도를 엄청나게 향상시켰고, 특히 자세한 area(특징이 많은 지역)에 대해서는 그 정확도가 더 올라갔다. 우리의 지식에 최선을 다하기 위해, DISN은 single view의 3D shape에서 얇거나 구멍과 같은 detail을 지속적으로 잡아내는 최초의 방법이다.(? 진짠가?) DISN은 synthetic, real image에서 3D recon을 하는 분야에서 SOTA를 여러개 찍었다.



### Introduction

 딥러닝을 이용한 3D recon이 많이 이루어져왔는데, 이러한 방법들은 보통 voxel을 이용하거나 point cloud를 이용하는 방법이었다. 하지만 이러한 것들은 해상도의 문제가 있었고, 몇몇 surface를 기반으로 한 representation들도 있었지만 유연성과 같은 문제들에 limit이 걸렸다. 더욱이, point cloud나 mesh를 기반으로 하는 방법들은 CD 혹은 EMD를 사용하는데,(Chamfer Distance 혹은 Earth-mover Distance) 이러한 training loss들은 얼마나 닮았느냐를 제공할 뿐이다.

 위와 같은 voxel, point cloud, mesh로 인해 발생하는 문제들을 해결하기 위해 우리는 SDF를 제안한다. implicit으로 진행한 몇몇 network들도 있고, 다른 방법으로 진행한 친구들도 많지만, 좋은 detail을 얻지는 못했다.

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210213-1.PNG)

 많은 single-view 3D reconstruction 방법들이 2D image에서 shape을 구할 때, global한 shape properties를 구하고 detail한 부분(hole이나 thin structure)등을 무시하는 경향이 생기게 된다. (global한 feature이다 보니, loss에도 큰 영향이 없는 hole/thin structure등은 자연스럽게 무시되는 것을 말함) 이러한 것들로 발생하는 결과물은 visual적으로 만족스럽지 못하다.

 이러한 문제를 해결하기 위해, 우리는 local feature extraction module을 소개한다. 특히, 우리는 input image의 viewpoint parameter를 예측한다. local patch(음.. 사진의 일부 라고 보는 편이 좋은 것 같다)에 상응하는 부분을 잘라서, 여기서 local feature를 뽑고, 3D point에서 SDF value를 예측하게 된다. 이러한 module은 projected pixel들과 3D shape 사이에서의 관계에 대해서 학습할 수 있게 되고, 3D shape의 결과물을 조금 더 fine grain하게 만든다. 그림 1에서 보이는 것 처럼, DISN은 pattern이나 hole등의 자세한 표현도 가능하다.



### Related Work

  voxel, octrees, point cloud, mesh 등을 이용한 방법들에 대한 간략한 소개 및 AtlasNet, Pix2Mesh, DeepSDF 등과 같은 다른 entwork들의 간략한 소개



### Method

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210213-2.PNG)

 object의 image가 주어지면, 우리의 목적은 object의 전체적인 구조와 fine-grained detail 모두를 포함한 3D reconstruction을 하는 것이다. SDF는 위 그림에 나와있는 것 처럼, 물체의 내부는 0 미만, 외부는 0 초과로 표시한 continuous function이다.(0이 surface를 이루게 됨)

 우리는 이를 응용하여, input image에 대해 SDF를 예측하는 network인 DISN을 소개한다. fixed resolution으로 output을 내뱉는 기존의 3D CNN과 달리, DISN은 arbitrary resolution으로 된 continous field를 만든다. 더욱이, 우리는 shape detail을 회복하는 것을 개선한 local feature extraction method를 소개한다.

__3.1 DISN: Deep Implicit Surface Network__

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210213-3.PNG)

 DISN의 overview는 위 그림과 같다. DISN은 2가지로 구분되어 있는데, 하나는 camera pose estimation과 다른 하나는 SDF predection이다. DISN은 처음은 camera parameter를 estimate하고, 주어진 camera parameter를 이용해서, 3D query point를 image plane에 project하고, image patch에 상응하는 multi-scale CNN feature를 모으게 된다.

__3.1.1 Camera Pose Estimation__

 주어진 input image에 대해서, 우리의 첫번째 목적은 viewpoint를 예측하는 것이다. 우리는 network를 ShapeNet Core dataset에서 학습시켜서 모든 model이 align되도록 했다. 우리는 이렇게 학습된 모델의 space를 world space로 정하고, camera parameter가 이를 기반으로 정해지도록 했다. 그리고 우리는 intrinsic parameter를 fix시켰다. input image를 direct로 CNN에 넣게 되는 경우, camera parameter를 regressing하는 것이 종종 실패한다는 것을 아래 논문에서 확인이 가능하다.

Eldar Insafutdinov and Alexey Dosovitskiy. Unsupervised learning of shape and pose with differentiable point clouds. In NeurIPS, 2018.

 이러한 문제를 극복하기 위해, 그들은 몇몇 pose candidate을 합침으로써 camera pose를 regress하는 distilled ensemble approach를 소개했다. 하지만, 이러한 방법은 수많은 network parameter를 필요로 했고, 학습 역시 복잡하다. 우리는 아래 그림과 같이 더 효율적이고 효과적인 network를 소개한다.

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210213-4.PNG)

 추가로 아래 논문에 따르면, quaternion이나 euler를 이용하는 방법보다 6D rotation representation이 훨씬 neural network가 학습하기 좋다는 것임을 보여주고 있다.

Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. On the continuity of rotation representations in neural networks. arXiv preprint arXiv:1812.07035, 2018.

 그래서 우리는 6D rotation representation을 사용하기로 했다.
$$
b=(b_x, b_y), \; where \; b\in \mathbb{R}^6,\; b_x \in \mathbb{R}^3 \; b_y \in \mathbb{R}^3
$$
 위와 같이 주어진 b(6D rotation representation)은, rotation matrix $R = (R_x, R_y, R_z)^T \in \mathbb{R}^{3\mathbf{x}3}$ 에 의해 다음과 같은 방법으로 얻어질 수 있다.
$$
R_x = N(b_x), R_z = N(R_x\times b_y ), R_y = R_z \times R_x
$$
 위 식에서 $N(\cdot)$ 은 normalization function을 말하고, $\times$는 cross product를 말한다. translation matrix인 $t \in \mathbb{R}^3$은 network에 의해 바로 predict된다.

 바로 camera parameter의 loss를 구하는 방법 대신, 우리는 predicted camera pose를 이용해서 world space의 point를 camera coordinate space로 project하는 방식을 선택했다. 우리는 이렇게 구해진 loss를 아래와 같이 계산했다.
$$
L_{cam} = \frac{\sum _{p_w \in PC_w}||P_G -(Rp_w +t)||_2 ^2}{\sum _{p_w \in PC_w}1}
$$
 위 식에서 $PC_w$는 world space에서 point cloud를 말하고, $N$은 point cloud의 수를 말한다.(근데 이게 어딨지? 아마도 그냥 SUM하는 개수를 말하는 듯 하다.) 각각의 $p_w \in PC_w$에 대해서, $P_G$는 GT를 의미하고, $||\cdot || _2 ^2 $ 는 squared $L_2$ distance를 의미한다.

__3.1.2 SDF Prediction with Deep Neural Network__

 input image $I$ 에 대해서, 우리는 GT SDF를 $SDF^I (\cdot)$ 이라하고, 이를 통해 얻게 되는 network의 목표를 $f(\cdot)$이라 하자. CD나 EMD loss에서 자주 사용되는 것들과는 달리, 우리의 방법은 true GT를 얻을 수 있는 방법이다.

 DeepSDF에서는 SDF를 regress하는 것으로 direct approach를 이용했다. DeepSDF에서는 SDF를 얻기 위해 3D point의 위치와 depth map 혹은 point cloud에서 뽑힌 shape를 합친 후 auto-decoder를 이용하는 방법을 사용했다. 여기서 auto-decoder structure는 각 object의 형태를 최적화하는데 필요하다. 우리의 초기 실험에서, feed forward 방법으로 유사한 network architecture를 적용했을 때, 우리는 convergence 문제를 확인했다. 이에 반해, Chen and Zhang이 제안한 방법에서는 input image의 global feature를 모든 decoder의 layer의 query point 위치에 concat을 했다. 이러한 방법은 효과적이었으나, network parameter의 수가 급증하는 것을 동반했다. 우리의 해결방법은 주어진 point location을 조금 더 높은 차원의 feature space로 map하는 것에 MLP를 사용했다. 이러한 high dimensional feature는 global feature, local feature와 합쳐진 이후 SDF value로 regress되는 것에 사용이 된다.

__Local Feature Extraction__

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210213-5.PNG)

 위 그림에서 나와있듯이, local feature를 뽑아내지 않는 경우엔 hole이나 thin structure를 recon하지 못하는 것을 확인할 수 있다. 우리는 위치 $q$에 상응하는 feature map을 찾고, 이를 local image feature에 concat을 한다. later layer에 있는 feature map은 원래 image보다 차원이 작기 때문에, bilinear interpolation을 통해서 원래 사이즈로 확장하고 난 이후, location $q$에서 resized된 feature를 추출한다.

 2개의 decoder가 각각 global, local image feature를 받게 된다. 이렇게 얻게 된 각 output은 합쳐져 SDF로 만들어지게 된다.

__Loss Functions__

 우리는 binary classification problem을 사용하는 것 대신 continuous SDF를 regress한다. 이러한 전략은 다른 iso-value에 상응하는 표면을 뽑아낼 수 있도록 한다.(?) iso-surface $S_0$ 의 내부 혹은 근처에서 detail을 회복하는 것에 집중할 수 있도록, weighted loss function을 사용했다.
$$
L_{SDF} = \sum _p m|f(I,p)-SDF^I (p)|, \; m=
\begin{cases}
m_1, \qquad if \;SDF^I(p)<\delta \\
m_2, \qquad otherwise,
\end{cases}
$$
 위 식에서 $|\cdot|$은 $L_1$ norm을 말하고, $m_1, m_2$는 different weights를 말한다.

__3.2 Surface Reconstruction__

 우리는 iso-surface $S_0$에 대해서 mesh를 만들기 위해 marching cube를 사용했다.



### Experiments

 우리는 정량적으로, 질적으로 single-view 3D recon에 대해서 여러 SOTA model들과 비교했다. 또한, 우리의 camera pose estimation module을 다른 model들과도 비교했다.

__Dataset__

 ShapeNet을 사용했고, 3D-R2N2에서 사용한 rendered view를 이용했다.

__A New 2D Dataset__

 ![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210213-6.PNG)

__Data Preparation and Implementation Details__

__Evaluation Metrics__

 4가지의 주로 사용되는 방법을 사용했음.

- Chamfer Distance
- Earth Mover's Distance
- Intersection over Union on voxelized mesh
- F-Score

__4.1 Single-view Reconstruction Comparison With State-of-the-art Methods__

![img7](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210213-7.PNG)

![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210213-8.PNG)

__4.2 Camera Pose Estimation__

![img9](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210213-9.PNG)

__4.3 Ablation Studies__

![img10](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210213-10.PNG)

![img11](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210213-11.PNG)

__Camera Pose Estimation__

__Binary Classification__

 surface의 내부와 외부를 나누는 binary classification을 사용하는 경우와 그렇지 않은 경우. binary classification을 사용한 경우에는 위 그림에서 'Binary'로 표현

__Local Feature Extraction__

 Global feature만 뽑아낸 경우는 'Global'로 표현

__Network Structures__

 2개의 decoder를 사용했는데, 이를 'Two-stream'이라 표현하고, 만약 global과 local을 합친 다음 한개의 decoder를 통과하는 경우에는 'One-stream'으로 표현.

__4.4 Applications__

__Shape interpolation__

![img12](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210213-12.PNG)

__Test with online product images__

![img13](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210213-13.PNG)

__Multi-view reconstruction__

![img14](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210213-14.PNG)



### Conclusion



### Reference

Xu, Qiangeng, et al. "Disn: Deep implicit surface network for high-quality single-view 3d reconstruction." *arXiv preprint arXiv:1905.10711* (2019).

Eldar Insafutdinov and Alexey Dosovitskiy. Unsupervised learning of shape and pose with differentiable point clouds. In NeurIPS, 2018.

Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. On the continuity of rotation representations in neural networks. arXiv preprint arXiv:1812.07035, 2018.