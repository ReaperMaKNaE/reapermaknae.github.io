---
layout : post
title : "MoCo V3"
date : 2021-04-08 +0900
description : ViT에서 다루었던 Self-supervised learning의 문제를 조금 더 깊게 연구하여, neural network의 조금 더 깊은 부분을 관찰한 MoCo v3를 다룬 논문의 간단한 리뷰입니다. 
tag : [PaperReview]
---

### An Empirical Study of Training Self-Supervised Visual Transformers



### Abstract

 새로운 방법을 연구한 논문이 아니다. Visual Transformer(ViT)가 self-supervised로 학습을 하며 보여준 결과에서, 모델이 커질수록 self-supervised가 잘 되지 않는 이유에 대한 조사를 진행하였고, 그 결과 이를 해결할 수 있는 몇가지 trick을 소개한다. 이로 인해 이전의 방식에 비해 1.1% 더 높은 accuracy를 얻을 수 있었다.

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210409-3.PNG)



### Introduction

 Unsupervised pre-training의 경우 NLP에서 발전이 많이 되었고, computer vision task에서도 많이 진행이 되고 있다. computer vision과 NLP의 차이점으로는, self-supervised를 할 때 siamese network(computer vision)을 쓰느냐 masked auto-encoder(NLP)를 쓰느냐의 차이와, backbone architecture로 self-attention transformer를 쓰느냐 아니면 convolution을 쓰느냐 등의 차이가 있다.

 하지만 최근 work에서 transformer를 vision task에 성공적으로 결합한 사례가 나왔고, 이로 인해 NLP에서의 self-supervised 방식을 vision task에도 적용할 수 있게 되었다. 하지만 이를 직접 적용을 해 본 결과, accuracy의 instability가 관찰이 되어, 이를 조사하기로 하였다.

 이러한 현상을 계속 조사하면서, 이는 어떠한 문제로 인해 발생한 문제가 아닌 내부의 문제로 인해 발생한 것임을 알게 되었다. 예를 들자면 batch size, learning rate, optimizer 등이다. 우리는 이러한 것들을 몇몇 trick을 통해 확인을 하였고, 이러한 것들을 알만한 가치가 있다고 판단한다.

 몇몇 trick의 예로 들자면, learnable patch projection이 아닌, fixed random match projection등이 있다.

 추가적으로 우리가 ViT에서 조금 potential이 있다고 보는 점은, fewer inductive bias이다. 예를 들면, position embedding을 ViT에서 없애더라도 정확도에 큰 차이가 없는데, 이는 inductive bias에 큰 영향을 받지 않는다는 의미와 같다. 그래서 self-supervised로도 어떻게 성능을 잘 끌어올릴 수 있는 potential을 확인할 수 있다.

 요약하자면, 우리는 우리가 알만한 가치가 있는 것들에 대한 증거, 그 증거에 대한 우리의 실험 및 도전, 그리고 열린 결말의 질문들이다. 이러한 것들이 미래에 해결이 되길 빈다.



### Related Work

__Self-supervised visual representation learning.__

contrasitve learning등의 방법들에 대해 설명.

__Transformers.__

 Transformer에 대한 설명. 최근 vision task에 적용한 ViT에 대한 설명 포함.

__Self-supervised Transformers for vision.__

 masked auto-encoding paradigm을 설명. 추가로 iGPT에서 사용한 방법도 설명했다.



### MoCo v3

 알고리즘은 다음과 같다.

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210409-1.PNG)

 오히려 MoCo 보다 간단해졌다. 위 식을 보면 알겠지만, contrastive loss(ctr)이 약간 변형된 것(symmetrized loss)과, 단순한 loss를 return하는 것이 아닌 temperature parameter와 2를 곱한 결과를 반환하는 것을 확인할 수 있다.(이는 다른 논문을 참조한 것)

 그리고, encoder의 경우 prediction MLP와 projection MLP를 추가로 사용하였고, momentum encoder의 경우에는 projection MLP를 제거한 형태만을 취하고 있다. 이러한 변경을 ResNet-50 Backbone에서 800 epochs로 진행한 결과, accuracy는 아래와 같다.(linear probing accuracy)

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210409-2.PNG)



### Stability of Self-Supervised ViT Training

 epoch에 따른 accuracy를 측정하기 위해, kNN curve를 사용했다고 한다.

__4.1. Empirical Observations on Basic Factors__

__Batch size__

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210409-4.PNG)

 ViT 관련 모델들은 그 크기가 상당히 크고, large-batch size로 training 하는 것을 확인할 수 있다.

 그럼 kNN curve를 이용해 track한 epoch에 따른 accuracy는 아래와 같다.

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210409-5.PNG)

 위 그림을 보면 알겠지만, batch size가 작을 때에는 그래도 accuracy가 좀 잘 증가하는 것 처럼 보이지만, batch size가 커질 수록 drop이 중간중간 일어나는 것을 볼 수 있다. 이러한 문제가 instability를 보이게 되고, 결과적으로 성능 저하를 유발하게 되었다. 이러한 내부적인 문제라고 생각되는 것들에 대해 다른 방법들을 더 실험해보았다.

__Learning rate__

 일단 처음은 learning rate에 대한 instability에 대한 조사이다.

![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210409-6.PNG)

 위 그림에서 볼 수 있듯이, 작은 learning rate일 수록 조금 더 stable한 것을 확인할 수 있다. 하지만 lr이 작다는 것은 그만큼 under fitting이 일어난다는 것이다. 참고로 lr은 lr*(batch_size)/256 을 사용했다.

__Optimizer__

 다음으로 test한 것은 Optimizer이다. 과연 사용했던 AdamW optimizer가 올바른 선택인가?를 고려해봤고, LAMB optimizer를 선택해서 실험을 했다고 한다. 그 결과는 아래와 같다.

![img7](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210409-7.PNG)

 optimal value를 사용하지 않는 한, learning rate의 drop이 보이는 것을 확인할 수 있다.

__4.2. A Trick for Improving Stability__

 stability를 증가시킬 만한 몇 방법을 생각했다고 한다. 참고로, instability를 추가적으로 확인할 수 있는 방법으로, 아래와 같이 epoch에 따른 첫번째 layer와 마지막 layer에서의 gradient를 측정해봤다고 한다.

![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210409-8.PNG)

 보다시피 몇몇 epoch에서 튀는 것을 확인할 수 있다. 일단 이러한 문제가 shallower layer에서만 일어난다고 확인하고 있는데, 그래서 freezing the patch projection layer를 생각했다고 하고, 그래서 patch를 embedding 할 때 learning based가 아닌 fixed random patch projection을 적용했다고 한다.

__Comparison__

![img9](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210409-9.PNG)

 결과를 보면 위와 같다. learning-based가 아닌 fixed random이 훨씬 더 잘 학습이 되는 것을 확인할 수 있다. 참고로, 비슷한 self-supervised learning method인 SimCLR이나 BYOL에서 확인한 결과는 아래와 같다.

![img10](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210409-10.PNG)

 다른 self-supervised learning method에서도 learning-based로 patch projection을 진행하는 경우 accuracy drop이 확인되었다. 이를 fixed random으로 넣으니 accuracy의 drop이 발견되지 않은 것을 확인할 수 있다. 즉, 이러한 것은 모든 self-supervised learning에 적용될 수 있는 trick임을 확인할 수 있다.

 추가적으로, BN, WN(WeightNorm), gradient clip 등을 적용해보았으나 모두 효과적이진 않았다.

__Discussions.__

 위 과정에서 보면 stable하게 보여도 사실 learning rate가 커지게 된다면 다시 unstable한 결과를 유발했다.



### Implementation Details

 기본적인 base network는 ViT + MoCo v3이다.

__Optimizer.__

 결국 optimizer는 AdamW에 batch size는 4096이다. warmup을 썼는데, warmup이 은근 도움이 된다고 한다.

__MLP heads.__

 projection head는 3-layer MLP를, prediction head는 2-layer MLP를 사용했다.

__Loss.__

 temperature parameter는 0.2를 사용했다고 한다.

__ViT architecture.__

 일반적인 ViT와 architecture는 거의 유사함.

__Linear probing.__

 self-supervised learning이 끝난 이후에는 wd를 0으로 두고, SGD optimizer를 이용해서 학습시켰다고 한다.

(wd는 weight decay)



### Experimental Results

![img11](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210409-11.PNG)

 우리가 사용한 ViT variant는 위와 같다. ablation study에서 기본적으로 사용한것은 ViT-B model이다.

__Training time.__

![img12](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210409-12.PNG)

 training time, TPUs 등의 과정은 위에서 확인할 수 있다. 참고로 GPU 128개로도 해봤는데, ViT-B의 경우 24h가 걸렸다고 한다.

__6.1. Self-supervised learning frameworks__

 MoCo v3, SimCLR, BYOL, SwAV 등을 사용했다고 한다. 이를 비교한 결과는 아래와 같다.

![img13](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210409-13.PNG)

 ViT에 조금 더 어울리는 것은 SimCLR, MoCo v3인 것을 확인할 수 있다.(근데 사실 MoCo v2가 SimCLR 방식을 MoCo에 때려넣은거라, 저걸 적어놓은 것도 좀 웃기긴 하다.)

__6.2. Ablations of ViT + MoCo v3__

 몇몇 ablation study를 진행함.

__Position embedding.__

![img14](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210409-14.PNG)

 먼저 position embedding을 없애봤다. 근데 은근 성능저하가 별로 없음. 이는 future work에서 고려할만한 사항이라고 본다. model이 permutation-invariant하다고도 할 수 있음.

__Class token.__

![img15](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210409-15.PNG)

 class token을 없애봤는데, 이것도 은근 떨어지지 않음. LN(Layer normalization)과 class token을 모두 없앴더니 성능저하가 거의 없는 수준임. 즉, 은근 쓸모가 없을 수도 있다. 라는 것이 결과로 나온다.

__BatchNorm in MLP heads.__

![img16](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210409-16.PNG)

 위 table에서 볼 수 있듯이, 의외로 prediction MLP head에서 BN을 없앴는데, 큰 저하가 없다. 이러한 모델의 장점으로, 완전히 BN-free model이 되게 된다.

__Prediction head.__

![img17](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210409-17.PNG)

 prediction MLP head를 MoCo v3에 오면서 추가했는데, 그 결과로는 위와 같다. 조금의 성능 향상이 있음을 확인할 수 있다.

__Momentum encoder.__

![img18](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210409-18.PNG)

 momentum encoder의 경우 위와 같은 방법을 통해 m=0.99가 좋은 값임을 확인할 수 있었다. 사실 이 m을 0으로 만들면 SimCLR이다.

__Training Length.__

![img19](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210409-19.PNG)

 training epoch이 많을 수록 성능이 더 좋아지는 것을 확인할 수 있다. 하지만 diminishing이 좀 빨라서, ViT-L/H에 대한 실험을 추가적으로 했다고 한다.(이 포스팅의 맨 처음 그림 참조할 것)

__6.3. Comparisons with Prior Art__

__Self-supervised Transformers__

 포스팅의 맨 첫 그림 참조.

__Comparisons with big ResNets.__

![img20](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210409-20.PNG)

 다른 모델들과 비교한 결과. BYOL이 꽤나 강한 모습을 보여주고 있으나, MoCo-v3와 ViT-BN의 경우 그 성능이 더 좋은 것을 확인할 수 있다. BN은 batch normalization을 MLP head에서 사용한 것.

__Discussion.__

 bigger self-supervised ViT model일수록 그 성장이 무궁무진한 것을 확인할 수 있다. potential solution은 더 많은 data를 이용해 학습을 시키는 것인데, saturation은 여전히 일어날 것이다. 아마도 그래서 더 복잡한, pretext task가 vision에서 필요할 것이다.

 ViT에 대해서 조금 더 긍정적으로 보는 것은, fewer-inductive bias이다. position embedding이 없어도, BN이 없어도 의외로 결과는 크게 떨어지지 않았다. 이로 인해 우리는 ViT가 CNN에 비해 positional information에 대한 제약을 덜 받는다 라는 것을 확인할 수 있다.

__6.4. Transfer Learning__

 Linear probing에 있어, 우리는 CIFAR-10/100, Oxford flowers-102, Oxford-IIIT-Pets 에 transfer learning을 수행했다. 그 결과는 아래와 같다.

![img21](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210409-21.PNG)

 오히려 supervised보다 성능이 높은 것을 확인할 수 있다.



### Conclusion



### Additional Implementation Details

__Data augmentation.__

 random resized cropping, horizontal flipping, color jittering, grayscale conversion, blurring, solarization 사용

__BatchNorm.__

 SyncBN을 사용하였음. 그리고, batch 안에는 서로 다른 image가 들어가도록 했음.

__AdamW implementation__

 $-lr*wd*weight$ 식을 이용해 AdamW optimzer를 적용함. 참고로 tensorflow는 좀 다르니 논문 참조.

__MLP heads in BYOL and SwAV__

 BYOL과 SwAV에서의 MLP head 설정인데, 크게 관심이 없으므로 pass

__kNN monitor__

 Self-supervised learning research에서는 크게 사용이 되는 방식. disp를 발견하기에 좋음. 근데 사실 epoch마다 보는 것 보다, iteration마다 보는게 좋긴 함.

__MoCo v3 for ResNet-50__

 LARS optimizer, 4096 batch size, lr=0.3 ,wd = 1.5e-6, temperature = 1.0, m=0.996에서 cosine schedule에 따라 1로 증가시킴.



### Reference

Chen, X., Xie, S., and He, K., “An Empirical Study of Training Self-Supervised Visual Transformers”, <i>arXiv e-prints</i>, 2021.