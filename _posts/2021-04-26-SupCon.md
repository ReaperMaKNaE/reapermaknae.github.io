---
layout : post
title : "Supervised Contrastive Learning"
date : 2021-04-26 +0900
description : 흔히 loss로 사용되는 CrossEntropy를 분석하여, 여러 loss를 통합한 Supervised Contrastive loss를 제안한 논문의 간단한 리뷰입니다.
tag : [PaperReview]
---

### Supervised Contrastive Learning



### Abstract

 contrastive learning이 요새 뜨면서 여러가지 loss들이 등장했음. 예를 들자면 triplet, max-margin, N-pairs loss 등등. 그런데 알고보면 얘내들이 하나로 합쳐질 수 있음. 우리는 분석을 통해 현재 존재하는 cross entropy의 contrastive learning 적용에서 문제점이 무엇인지 발견하고 이를 어떻게 고칠 수 있는지를 이번 논문에서 증명한다. 이를 SupCon loss라고 하고, 그 결과로 같은 baseline network(ResNet-200) 기준으로 top-1 accuracy를 찍었다.(81.4%, 기존보다 0.8% 높은 수치)



### Introduction

 Cross entropy loss가 정말 다양한 곳에서 사용되고 있지만, 몇몇 문제가 있었음. 그래서 우리가 고쳤는데, 이를 그림으로 표시하면 아래와 같음.

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-2.PNG)

 위 그림에서 말하고 싶은 것은, 기존의 cross entropy의 경우에는 같은 'dog'에 속해있음에도 불구하고 negative로 분류시킨 반면, 새로 제안한 supervised contrastive에서는 'dog'은 'dog'끼리 묶을 수 있는 방법을 찾았다는 것을 의미한다. 이를 적용 시킨 결과의 성능은 아래와 같다.

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-1.PNG)

 그래서 논문의 contribution은,

- naive extension들의 cross-entropy loss들이 모두 supervised contrastive loss보다 좋지 못함(예를 들자면 triplet, max-margin, N-pairs loss 등등)
- 우리의 loss로 학습한 애들은 좀 더 robust함.
- 우리 loss function의 gradient가 hard-positive/negative로부터 학습이 되는 것을 분석함
- cross-entropy에 비해 우리의 SupCon(supervised contrastive loss)가 hyperparameter의 변화에 덜 민감함

(참고로 hard-positive/negative의 경우, positive/negative 임에도 불구하고 negative/positive로 잘못 판단되는 것들을 말한다.)



### Related Work

 전반적인 cross-entropy에 대한 내용.



### Method

__3.1. Representation Learning Framework__

 우리의 framework은 다음과 같다.

- Data Augmentation module, _Aug_
- Encoder network, _Enc_
- Projection Network, _Proj_

__3.2. Contrastive Loss Functions__

 notation을 정리하고 들어가는데, N samples를 'batch'라 표현하고, 2N augmented sample들을 multiviewed batch라 하기로 한다.

__3.2.1. Self-Supervised Contrastive Loss__

 기존 Contrastive learning에서 사용되는 loss는 다음과 같다.

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-3.PNG)

 여기서 i와 j(i)는 positive pair이고, 나머지들이 다 negative pair가 된다. 즉, negative pair의 수는 2N-2개가 존재함. 

__3.2.2 Supervised Contrastive Losses__

 위와 같은 식의 단점으로는, positive pair가 여러개인 경우는 그럼 어떡하는가? 가 문제다. 같은 'dog'에서 서로 다른 image가 나올 수 있는데, 이들을 전부 augmentation이 한 source에서 오지 않았다고 negative pair로 때려버리는 것은 손해가 크다. 그래서 우리는 아래와 같은 새로운 loss 2가지를 제안한다. 둘의 차이는, log가 sum 안에 있느냐 밖에 있느냐의 차이가 있는데, 이에 대한 자세한 분석은 appendix에서 다룬다.

(지금부터 이해하려면 appendix를 보고 와야한다. 그래서 먼저 보고 오는 것을 추천함.)

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-4.PNG)

 위 식에서 $|P(i)|$ 는 cardinality를 의미한다.

 위와 같은 방식으로 식을 변경하게 되면 아래와 같은 장점들이 있다.

- Generalization to an arbitrary number of positives.
- Contrastive power increases with more negatives
- Intrinsic ability to perform hard positive/negative mining

 두 loss는 전적으로 다르다. 실제로도 성능을 비교해보면, 아래와 같은 결과를 보인다.

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-5.PNG)

 결과에 따른 gradient는 아래와 같다. ( $z_i$ 는 마지막 MLP를 거쳐 나오는 결과물이라고 보면 됨)

![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-6.PNG)

 위 식에서 $N(i)$ 는 negative pair를 말한다. 그 외 $P_{ip}, X_{ip}$ 등도 있는데, 자세한 내용은 appendix 참조.

 (지금부터는 appendix에서 증명 과정을 보고 왔다고 가정하고 시작할 것이다.)

 여하튼, 각 $z_p$ 를 mean positive representation이라고 두면, $X_{ip}$ 는 아래와 같이 표현이 가능하다.

![img7](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-7.PNG)

__3.2.3. Connection to Triplet Loss and N-pairs Loss__

 이 부분도 appendix에 나오는데 one-positive이고 one-negative인 경우를 위 식에 적용하면 triplet loss가 되고, one-positive에 negative의 수가 엄청 많아지게 되면 N-pair loss가 된다.



### Experiment

Data augmentation module의 경우, AutoAugment, RandAugment, SimAugment, Stacked RandAugment의 variant를 사용하였고(그 중에서는 AutoAugment가 가장 성능이 좋음), Dataset으로는 CIFAR-10, CIFAR-100, ImageNet을 사용했다. Encoder network의 경우에는 ResNet-50, ResNet-101, ResNet-200이 사용되었다.

__4.1. Classification Accuracy__

![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-8.PNG)

 몇몇 결과들을 얕게 말하고 있는데, 맨 위 figure 1에 대한 내용도 담겨있다. 일단 loss는 SupCon을 쓰는 경우가 가장 좋다는 것을 말하고 있고, ResNet 50에서는 AutoAugment를, ResNet-200에서는 Stacked RandAugment를 사용한 것이 효과가 더 좋았다고 한다. 근데 왜 위에는 AutoAugment가 가장 성능이 좋다고 적어놨냐?라고 하면, 나중에 appendix에서 나온다. GPU는 8개의 V100을 썼다는데 이건 왜 여기에 있는지 잘 모르겠음. 참고로 training details는 논문의 마지막에 나온다.

__4.2. Robustness to Image Corruptions and Reduced Training Data__

 ImageNet C에 대해서 실험을 진행했다.

![img9](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-9.PNG)

__4.3. Hyperparameter Stability__

 hyper parameter들에 대해서 얼마나 덜 sensitive한지를 분석한 결과이다. 기존의 cross-entropy에 비해 hyper parameter의 변화에도 큰 변화가 없음을 보여준다.

![img10](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-10.PNG)

__4.4. Transfer Learning__

 12개의 image dataset에 대해서 transfer learning을 시도한 결과는 다음과 같다.

![img11](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-11.PNG)

__4.5. Training Details.__

 대충 training setting에 대해 나와있다. 생략.



 Conclusion이 없다. 일단 생략.



### Supplementary



### Training Setup

![img12](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-12.PNG)

 조금 더 그림으로 표현하면 위와 같다.



### Gradient Derivation

 gradient에 대해서 loss를 미분한 결과를 담고 있다. 결과는 아래와 같다.

![img13](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-13.PNG)

 먼저 log 안에 들어가있는 in형태에 대해서 계산을 해보자.

![img14](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-14.PNG)

 위 식에서 $P$ 와 $X$ 는,

![img15](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-15.PNG)

 다음은 out 형태에 대해서 계산을 한다.

![img16](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-16.PNG)

 그런데 in 형태와 out 형태의 꼴이 비슷하지 않은가? 아래와 같이 정리가 된다.

![img17](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-17.PNG)



### Intrinsic Hard Positive and Negative Mining Properties

 projection network의 output이 normalization되기 전 output을 $w$ 라고 한다면, 아래와 같이 정리가 된다.

![img18](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-18.PNG)

 여기서 위에 나온 식들을 적절히 섞어보면,

![img19](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-19.PNG)

 위 식과 같이 나오게 되는데, easy positive/negative의 경우 $z_i \cdot z_p$ 는 거의 1/-1에 수렴한다. 그래서,

![img20](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-20.PNG)

 위와 같은 형태가 나오게 되고, hard positive/negative의 경우에는 $z_i \cdot z_p$가 거의 0이 된다. 따라서,

![img21](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-21.PNG)

 이를 이용해 조금 정리를 해보면 아래와 같다.

![img22](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-22.PNG)

 그래서 위 식을 잘 분석해보면, hard positive/negative인 경우에만 gradient가 영향을 받고, easy positive/negative에 대해서는 gradient가 영향을 받지 않음을 알 수 있다. 즉, 어려운 task에 대해서만 학습이 잘 되므로 성능이 더 좋아지는 것이라고 볼 수 있다.



### Triplet Loss Derivation from Contrastive Loss

![img23](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-23.PNG)

 사실 triplet loss에 대해서 모르긴하는데, 위 식에서 $2\tau$ 가 빠지면 triplet loss라고 한다.



### Supervised Contrastive Loss Hierarchy

![img24](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-24.PNG)

 일단 SupCon을 다시 쓰면 위와 같다.

  positive pair의 경우 2가지 루트에서 오게 되는데, 첫번째는 같은 이미지에서 augment된 거랑, 다른 이미지에서 augment된 것이다.(same label을 가지고) 만약 $P(i) = j(i)$ , 즉 positive pair가 한개라면,

![img25](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-25.PNG)

 위 식과 같이 self-supervised contrastive loss form으로 나타나고, 여기서 positive의 개수가 늘고 temperature hyperparameter가 1이면,

![img26](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-26.PNG)

 N-pairs loss가 된다.(사실 N-pairs loss도 잘 모름)

 조금 흥미로웠던 점으로는, N-pairs loss를 쓰게 되면 57.4%이던게, SupCon을 쓰면 78.7%로 뛰어오른것.



### Effect of Temperature in Loss Function

 이건 좀 개인적으로 중요하다고 본 포인트 중 하나였다.

- Smoothness
- Hard positives/negatives

 temperature가 높아지게 되면 gradient의 변화가 적어지게 되는데(반비례 관계, 위 증명 과정을 살펴볼 것), 이로 인해 smooth한 학습 과정이 유도가 된다고 한다.

 그 아래의 경우, temperature가 낮아질수록, anchor에 대해 inner product값이 큰 애들은 $P_{ip}$ 가 커지고, 작은애들은 $P_{ip}$ 가 작아지게 된다고 한다. 결국, temperature가 작을수록 gradient가 hard positive/negative에 더 반응을 잘 하고, 커질수록 그 반응이 줄어든다 라고 한다. 저자는 training 과정에서는 0.1이 optimal이라는 것을 찾았다고 한다.



### Effect of Number of Positives

![img27](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-27.PNG)

 따로 제한을 걸지 않았을 때 성능이 가장 좋았다고 한다.



### Robustness

 ImageNet C dataset만 사용한 결과.

![img28](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-28.PNG)

 근데 이 결과를 왜 앞에 안실었는지는 모르겠다. 아무튼 잘 됬다고 함.



### Two stage training on Cross Entropy

 이건 뭐 어디서 해서 여기서도 한 것 같은데, 의미는 잘 모르겠음. 아무튼 결과는 아래와 같다.

![img29](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-29.PNG)



### Training Details.

 training details라기 보단 사실 좀 노가다뛴 걸 보여주는 것 같다.

__14.1 Optimizer__

![img30](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-30.PNG)

 보면, contrastive learning 도중엔 LARS를, Linear(classification task) 도중엔 RMSProp을 사용하는 것이 가장 효과가 좋은 것임을 확인하였음.

__14.2. Data Augmentation__

![img31](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-31.PNG)

 일단 간단한 설명.

![img32](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-32.PNG)

 둘 다 autoaugment를 사용한 것이 좋은 것을 확인 할 수 있다. 하지만 resnet50 기반이고, 사이즈가 큰 모델에서는 Stacked RandAugment를 사용하는 것이 좋다고 함. 근데 runtime에 있어서는 AutoAugment가 더 좋다고 한다.

 추가로 augmentation의 강도에 대해서도 실험을 했다고 하는데, 그 결과 robust하다고 함.

![img33](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210426-33.PNG)



### Reference

Khosla, Prannay, et al. "Supervised contrastive learning." *arXiv preprint arXiv:2004.11362* (2020).

