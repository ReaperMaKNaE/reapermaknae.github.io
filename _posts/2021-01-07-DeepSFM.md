---
layout : post
title : "DeepSFM"
date : 2021-01-06 +1000
description : Image Pair에서 depth와 pose를 estimation하는 것에 P-CV 개념을 도입하여 성능향상을 이끌어 낸 DeepSFM: Structure From Motion Via Deep Bundle Adjustment 논문의 간단한 리뷰입니다.
tag : [PaperReview]
---

### DeepSFM: Structure From Motion Via Deep Bundle Adjustment



 DeMoN 이후에 Cost Volume을 두 개 사용하여(D-CV: Depth Cost Volume, P-CV: Pose Cost Volume) depth와 pose estimation을 성공적으로 이끌어 낸 Network입니다.



기존에 있던 방식을 벗어나서, depth와 pose를 같이 구하는 방법을 추구, by using BA(Bundle Adjustment).



최근 논문들은 3D structure와 camera motion이 LM algorithm으로 최적화되어있음. 근데 무늬없거나 반사있으면 다 말짱도루묵

최근 DNN을 기존의 sfm에 적용시키니, geometric constaint(구조/모션)이 잘못된 결과에 오버피팅도 됨. 이게 3d cost volume이 나오고 나아졌는데(GCNet), 결국 camera geometry에 대해서는 알아야 함.



기존 3D cost volume은 photo-consistency한 점들을 사용, 근데 여기에다 geometric consistency를 더하면 괜찮아질듯?해서 이 친구들이 연구함. 초기 카메라 pose는 demon에서 썼던 방법으로 구할 수 있음.

여튼, 최근에 BANet이란게 나왔는데, 이 친구들도 우리랑 비슷함. LM optimization의 적분을 이용했는데, 이건 memory먹고 연산이 많은, 암튼 구데기 구조임. 근데 우리껀 그딴거없고 성능도 더 좋음. SfM에서 LM은 원래 point랑 camera pose optimization했음. 그래서 LM integration은 좋은 연관성을 보여주긴함. 이러한 연관성을 피하기 위해 그들은 regressor를 추가, 결국 모델을 더욱 구데기로 만듬. 반면에 우리껀? 아무튼 굿굿임



### Related Work

Single View Depth Estimation

Traditional Structure-from-Motion -> 많은 애들이 노력을 했지만 결과적으로 다~ 구데기였음

Deep Learning for structure-from-motion

SE3 transformer가 Deep2vd에서 나옴, geometry로 camera pose를 알아내는 걸로.

BANet이 depth랑 camera pose를 뭐 어떻게 잡아냈음.



### Architecture
Demon을 통해서 얻어낸 initialization은 은근 굿임.

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210107-8.png)

3.1 - 2D Feature Extraction

3.2 - Depth based Cost Volume(D-CV)

==> target image feature, warped source image feature, homogenous depth consistency maps 를 합친 개념.

Hypothesis Sampling: 가상의 inverse depth space를 만들어서 cost volume만드는데 기반을 둠

Feature warping: 카메라 parameter를... 뭐여튼 cost volume만드는데 또 뭐 더했음.
![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210107-9.png)

Depth consistency

실제로 feature warping은 photometric consistency라고 보면 됨. depth가 아니라 빛을 기반으로 동작하는 놈이거든.

우린 geometric consistency(depth)를 exploit하고 depth prediction의 정확도를 올리려고 채널 두 개를 더했음.

한 친구의 이름은 "The warped initial depth map"이고, 다른 친구 이름은 "projected virtual depth plane"임.

첫번째 친구를 살펴보면, source image를 down sample하고 feature warping한거랑 비슷함.

이게 그럼 위에 했었던 feature warping과 다른 점은, 얘는 depth warping인데다 NN sampling을 더해서 아무튼 다름. 그런갑다하셈.

좀 자세하게 들어가면, warped initial depth map은, l-th depth plane의 one-channel depth map을 의미

depth warping과 feature warping의 차이는, bilinear interpolation이 아닌, NNsampling이란 것(??)

-> 이러한 차이는 부록(5.2)에 있음. -> 이걸 왜 부록에 뺐냐? 그냥 넣으면 안되나?

-> depth map에서의 discontinuity때문에, bilinear interpolation은 좀 안좋은 영향을 끼칠 수 있음.

-> 실제로 우리 모델에서 NN sampling 말고 bilinear interpolation을 적용시키면 성능 드랍일어남.

-> 우린 안했지만, differentiable bilinear interpolation은 흥미로운 연구 결과가 될 수 있을 것 같음.

암튼 위가 첫번째 initial depth map을 추가한 이유임.

다음 친구는 "Projected virtual depth plane"임. 얘는 target view의 virtual plane에 있는 depth값을 source에서 봤을 때의 depth 값을 포함하고 있음.

얘도 필요한가...??

-> 결국 (2CH+2)*L*W*H의 구조를 얻는데, 333kernel을 통과시켜서 cost volume을 얻어냄.

3.3 - Pose based Cost Volume(P-CV) - 얘도 3가지 성분을 합침.

target image feature, wapred source image feature, homogenous depth consistency maps.

사실 잘 이해가 안가는데, 걍 D-CV처럼 했다! 이거인듯. 

단순히 target image feature만 학습 시키는 것이 아니라, source는 어떤 형태이고, depth consistency에 대한 cost volume은 어떤 형태인지를

잘 살펴보면, 결국엔 pose도 unknown, depth도 unknown이니 각각에 대한 layer를 하나씩 추가해준 셈...? 맞나?

~~근데 그럼 학습시킬때엔 왜 따로했냐? 코드 읽어보니 어짜피 따로 학습시키더만~~

3.4 Cost Aggregation and Regression : DPSNet에서 했던 것 처럼 했음.

3.5 Training - 아무튼 loss 대충 잡고 이래저래 했음.



### Experiments
4.1 - Dataset ==> DeMoN, ETH3D, Tanks and Temples 썼음.

4.2 - Evaluation

평가를 어떻게 했지?

ETH3D에서는 낮은 resolution에서도 COLMAP이나 R-MVSNet에 비해 좋은 성능을 보여주었음.

iteration 역시 성능 향상에 도움이 되었음.

Effect of P-CV: P-CV를 "Real-time visual odometry from dense rgb-d images"에서 사용했던 것으로 대체한 결과, 효과가 떨어짐. 결국 P-CV 굿

View Number: 이것도 살펴보면 View가 늘어날수록 abs_rel이 떨어지는 것을 확인할 수 있음.

Conclusion : 결국 D-CV, P-CV를 썼더니 굉장해짐. 굿.



### Supplement Data

어떤 코드를 짰는가. 환경이 어떠한가.

data augmentation을 실행하였더니 결과가 어떻게되었다? 망했다. unseen이다보니 그런듯.

사실 망한것 까진 아닌데 아무튼 성능 향상에 큰 도움이 되질 않았다.

5.5 Smoothness on depth map

-> Scale gradient loss를 넣어봤는데 뭐 큰 차이가 없음.



Discussion with DeepV2D 

DeepV2D도 sfm method이긴한데, 우리랑 달라서 뭐 아무튼 비교가 안됨.(추후 간단한 리뷰 예정)



Visualization

BANet은 코드 공개안해서 모름. 그래서 나머지랑 비교해봤음.(이 아래로는 그림들)

대충 보면... initialization에도 큰 차이가 없고... 그렇다.

 

### Reference

Wei, Xingkui, et al. "DeepSFM: Structure from motion via deep bundle adjustment." European conference on computer vision. Springer, Cham, 2020.