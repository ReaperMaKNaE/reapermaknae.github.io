---
layout : post
title : "DeepSFM"
date : 2021-01-07 +0900
description : Image Pair에서 depth와 pose를 estimation하는 것에 P-CV 개념을 도입하여 성능향상을 이끌어 낸 DeepSFM: Structure From Motion Via Deep Bundle Adjustment 논문의 간단한 리뷰입니다.
tag : [PaperReview]
---

### DeepSFM: Structure From Motion Via Deep Bundle Adjustment



 DeMoN 이후에 Cost Volume을 두 개 사용하여(D-CV: Depth Cost Volume, P-CV: Pose Cost Volume) depth와 pose estimation을 성공적으로 이끌어 낸 Network입니다.



기존에 있던 방식을 벗어나서, depth와 pose를 같이 구하는 방법을 추구, by using BA(Bundle Adjustment).



최근 논문들은 3D structure와 camera motion이 LM algorithm으로 최적화되어있음. 근데 무늬없거나 반사있으면 다 말짱도루묵

최근 DNN을 기존의 sfm에 적용시키니, geometric constaint(구조/모션)이 잘못된 결과에 오버피팅도 됨.
이게 3d cost volume이 나오고 나아졌는데(GCNet), 결국 camera geometry에 대해서는 알아야 함.



기존 3D cost volume은 photo-consistency한 점들을 사용
근데 여기에다 geometric consistency를 더하면 괜찮아질듯?해서 이 친구들이 연구함
초기 카메라 pose는 demon에서 썼던 방법으로 구할 수 있음.

여튼, 최근에 BANet이란게 나왔는데, 이 친구들도 우리랑 비슷함. LM optimization의 적분을 이용했는데,
이건 memory먹고 연산이 많은, 암튼 구데기 구조임.
근데 우리껀 그딴거없고 성능도 더 좋음. SfM에서 LM은 원래 point랑 camera pose optimization했음.
그래서 LM integration은 좋은 연관성을 보여주긴함. 이러한 연관성을 피하기 위해 그들은 regressor를 추가,
결국 모델을 더욱 구데기로 만듬. 반면에 우리껀? 아무튼 굿굿임



### Related Work

Single View Depth Estimation

Traditional Structure-from-Motion -> 많은 애들이 노력을 했지만 결과적으로 다~ 구데기였음

Deep Learning for structure-from-motion
SE3 transformer가 Deep2vd에서 나옴, geometry로 camera pose를 알아내는 걸로.
BANet이 depth랑 camera pose를 뭐 어떻게 잡아냈음.



### Architecture
Demon을 통해서 얻어낸 initialization은 은근 굿임.

![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210107-8.png)

3.1 - 2D Feature Extraction
3.2 - Depth based Cost Volume(D-CV)
==> target image feature, warped source image feature, homogenous depth consistency maps 를 합친 개념.
Hypothesis Sampling: 가상의 inverse depth space를 만들어서 cost volume만드는데 기반을 둠
Feature warping: 카메라 parameter를... 뭐여튼 cost volume만드는데 또 뭐 더했음.
![img9](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210107-9.png)
Depth consistency

이걸 하기 위해 각 virtual plane에다, warped initial depth maps + projected virtual depth plane을 더함.
좀 자세하게 들어가면, warped initial depth map은, l-th depth plane의 one-channel depth map을 의미
depth warping과 feature warping의 차이는, bilinear interpolation이 아닌, NNsampling이란 것(??)
-> 이러한 차이는 부록(5.2)에 있음. -> 이걸 왜 부록에 뺐냐? 그냥 넣으면 안되나?
-> depth map에서의 discontinuity때문에, bilinear interpolation은 좀 안좋은 영향을 끼칠 수 있음.
-> 실제로 우리 모델에서 NN sampling 말고 bilinear interpolation을 적용시키면 성능 드랍일어남.
-> 근데 반면에, differentiable bilinear interpolation은 흥미로운 연구 결과가 될 수 있을 것 같음.
암튼 위가 첫번째 initial depth map을 추가한 이유고, 두번째 depth values는 암튼 필요함.
-> 결국 (2CH+2)*L*W*H의 구조를 얻는데, 333kernel을 통과시켜서 cost volume을 얻어냄.
==================감으로 잘 안잡힘. 나중에 다시 확인할 것.
3.3 - Pose based Cost Volume(P-CV) - 얘도 3가지 성분을 합침.
target image feature, wapred source image feature, homogenous depth consistency maps.
사실 잘 이해가 안가는데, 걍 D-CV처럼 했다! 이거인듯. 맞지. 두 개가 다르면 네트워크가 학습이 잘 안되겠지.
3.4 Cost Aggregation and Regression : DPSNet에서 했던 것 처럼 했음.
3.5 Training - 아무튼 loss 대충 잡고 이래저래 했음.



### Experiments
4.1 - Dataset ==> DeMoN, ETH3D, Tanks and Temples 썼음.
4.2 - Evaluation
평가를 어떻게 했지?
ETH3D에서는 낮은 resolution에서도 COLMAP이나 R-MVSNet에 비해 좋은 성능을 보여주었음.
iteration 역시 성능 향상에 도움이 되었음.
Effect of P-CV: P-CV를 "Real-time visual odometry from dense rgb-d images"에서 사용했던 것으로
대체한 결과, 효과가 떨어짐. 결국 P-CV 굿
View Number: 이것도 살펴보면 View가 늘어날수록 abs_rel이 떨어지는 것을 확인할 수 있음.
Conclusion : 결국 D-CV, P-CV를 썼더니 굉장해짐. 굿.



### Supplement Data

어떤 코드를 짰는가. 환경이 어떠한가.

data augmentation을 실행하였더니 결과가 어떻게되었다? 망했다. unseen이다보니 그런듯.

사실 망한것 까진 아닌데 아무튼 성능 향상에 큰 도움이 되질 않았다.

5.5 Smoothness on depth map
-> Scale gradient loss를 넣어봤는데 뭐 큰 차이가 없음.



Discussion with DeepV2D 

DeepV2D도 sfm method이긴한데, 우리랑 달라서 뭐 아무튼 비교가 안됨.(추후 간단한 리뷰 예정)



Visualization

BANet은 코드 공개안해서 모름. 그래서 나머지랑 비교해봤음.(이 아래로는 그림들)
대충 보면... initialization에도 큰 차이가 없고... 그렇다.

 

### Reference

![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210107-8.png)