---
layout : post
title : "Fast RCNN"
date : 2020-12-28 +0900
description : R-CNN과 SPPNet의 단점을 극복하고 나타난 Fast RCNN에 대한 리뷰입니다.
tag : [PaperReview]
---

### Fast R-CNN



 RCNN과 SPPNet의 한계점을 극복한 것으로 유명한 Fast RCNN을 리뷰하게 되었다. 



 Fast RCNN에서 중요한 포인트는, 한 개의 Network만으로 classification과 bbox까지 마쳤다는 점이다.

 

 input image size가 국한되지 않는 다는 SPP Net의 SPP Layer를 응용해서 여기서는 RoI Pooling Layer라는 것을 이용하는데, 전체적인 형태는 아래와 같다.

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20201218-1.png)

 Training 시에는, 학습시킬 Image와 Object를 나타내는 영역을 나타내는 bbox(맨 왼쪽위 점의 좌표, bbox_width, bbox_height)를 input으로 받고, 해당하는 영역을 RoI Projection을 통해 Convolution feature를 뽑아내고, 이후 RoI Pooling Layer, FC를 지나서 이미지가 어떤 class인지 골라내는 softmax와 bbox 위치를 얻어낸다.

 그리고 이 때 얻어낸 loss를 바탕으로, weight를 수정하는 학습을 진행한다. 

 ![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20201218-2.png)

 당시 사용된 loss는 위와 같다. 총 값은 cls(아마도 classification)와 loc(아마도 localization)에 해당하는 value로, p는 해당 class의 probability, u는 해당 class를 나타내는 숫자, t는 bbox 좌표들, v는 GT이다.

 람다[u>=1]의 경우 Iverson bracket Indicator function이라 하여, 해당 식은 u가 1보다 크다면 항상 1을 의미하고, 그 외는 모두 0을 의미하는 function이다.

  ![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20201218-3.png)

 위와 같은 방법으로 loss의 변화를 측정하고, back propagation을 진행한다. 위 식의 결과는 x_i(i번째 feature vector value)에 대한 loss의 변화량을 측정할 수 있다. y_rj는 x_i가 i* (r,j) 영역을 통해 나온 pooling layer의 output이다. i*(r,j)는 sub window index j가 주어졌을 때 maximum feature value의 값의 index를 의미하고(== RoI Pooling Layer를 통과하는 index값)에 해당한다.

 위 식의 의미는 window가 stride를 통해 x_i를 지나가면서 x_i가 검출이 되었을 때 Loss에 대한 y_rj의 편미분 값을 축적하는 것이다. 조금 쉽게 말하면, output에 영향을 끼친 feature vector들이 얼마나 변화하느냐에 따라 Loss를 다시 계산하는 것.



 그리고 parameter를 좀 덜 쓰는 방법으로 truncated SVD와 같은 방법들을 고안했다고 하는데, 추후에 잘 안 쓰이는 방법이라고 하니 일단 Pass.



### 작성중인 내용

  그럼 Code로 한번 직접 살펴보자.

  살펴볼 Code는 홍콩중문대학(The Chinese University of Hong Kong, CUHK)의 Multimedia laboratory에서 만든, pytorch로 쓰여진 MM detector이다. 아래는 mm detector 링크.

[https://github.com/open-mmlab/mmdetection](https://github.com/open-mmlab/mmdetection)



 그리고 Fast RCNN의 model은 아래 링크에 있다.

[https://github.com/open-mmlab/mmdetection/blob/master/configs/_base_/models/fast_rcnn_r50_fpn.py](https://github.com/open-mmlab/mmdetection/blob/master/configs/_base_/models/fast_rcnn_r50_fpn.py)





 해당 사이트에서 제공하는 colab tutorial을 이용해서 분석을 해 볼 계획이다. 링크는 아래에 있다.

[https://github.com/open-mmlab/mmdetection/blob/master/demo/MMDet_Tutorial.ipynb](https://github.com/open-mmlab/mmdetection/blob/master/demo/MMDet_Tutorial.ipynb)

  코랩 파일의 위쪽에서는 필요한 패키지를 설치하고, 아래 Perform inference with a MMDet Detector section을 살펴보면, apis 폴더 내의 inference_detector, init_detector, show_result_pyplot 함수를 사용한다는 점을 알 수 있다. 해당 함수를 확인해보자.

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20201218-5.png)

 코드는 config file과 checkpoint file을 확인한 다음 model = init_detector(config, checkpoint, device='')로 시작하는데, init_detector의 경우, config, checkpoint, device, cfg_options 라는 변수를 받는다. config는 config file의 위치를, checkpoint는 checkpoint file의 위치를 나타낸다. 이후 model을 init_detector를 이용하여 알아내게 된다. init_detector는 아래와 같다.

  ![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20201218-4.png)

  isinstance의 경우, config가 'str' 형태가 맞는지 확인한다. 여기에선 str 형태로 입력을 주었기에, mmcv.Config.fromfile로 넘어가는데, 여기서 mmcv는 mmdetection을 만든 곳에서 통상적으로 쓰일 수 있게 만든 computer vision code이다. 다행히도 찾아본 결과 API가 존재한다.

 [https://mmcv-jm.readthedocs.io/en/stable/index.html](https://mmcv-jm.readthedocs.io/en/stable/index.html)

 대충 data 저장형태를 저렇게 한다는 것으로 알 수 있다. 다음 cfg_options의 경우 None이므로 35번째 줄의 if문은 건너가고, pretrained를 0으로 한 후 model로 바로 넘어가게 된다.

 여기서 build_detector를 또 쓰는데, 

  ![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20201218-8.png)

 그 형태는 위와 같다. 

 cfg 내의 type에 해당하는 값들을 받아서 ob_type에 저장하고, 이를 return을 한다. obj_cls(** args)의 의미는 딕셔너리 형태로 저장되어있는 obj_cls의 값들을 반환해준단 의미가 된다.(결국 model을 반환하게 됨) 이러한 받은 모듈들로 neural network(model)을 만드는 nn.Sequential이 호출된다. build_detector 내부의 DETECTORS와 같은 것은 Register class로 넘어가는 값들인데, 결국 다 정리하고 보면 위의 결과로 수렴한다.(model을 호출하고, nn.Sequential로 호출된 model을 만드는 것)



 model은 config를 따라가면 확인할 수 있다. 대개 Model의 형태를 나타내어 준다. 형태는 딕셔너리로 저장되어있다.

 다음은 checkpoint인데, 이는 mmcv의 load_checkpoint라는 함수를 통해서 얻어오게 된다. checkpoint는 학습된 모델로, 이를 불러와서 pytorch로 하는 것인데, 안 쓸 거면 학습을 시키는 과정을 거쳐야하는 것으로 보인다. 일단 tutorial을 그대로 따라가 보자.

'mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth'

 파일 이름에서도 확인할 수 있듯이, bbox mAP는 0.408, segmentation mAP는 0.37인가보다. 하지만 대상은 우리가 찾으려는 fast RCNN이 아닌 mask RCNN. 

[https://github.com/open-mmlab/mmdetection/blob/231387cd3f85a310f899cb5b4c948cd79787118d/configs/_base_/models/fast_rcnn_r50_fpn.py](https://github.com/open-mmlab/mmdetection/blob/231387cd3f85a310f899cb5b4c948cd79787118d/configs/_base_/models/fast_rcnn_r50_fpn.py)

(추가 작성 중)





### Reference

He, Kaiming, et al. "Spatial pyramid pooling in deep convolutional networks for visual recognition." *IEEE transactions on pattern analysis and machine intelligence* 37.9 (2015): 1904-1916.

hwkim94 님의 Fast RCNN 리뷰, [https://github.com/hwkim94/hwkim94.github.io/wiki/Fast-R-CNN(2015)](https://github.com/hwkim94/hwkim94.github.io/wiki/Fast-R-CNN(2015))

갈아먹는 머신러닝님의 Fast RCNN 리뷰, [https://yeomko.tistory.com/15](https://yeomko.tistory.com/15)

MM detector, open mmlab, [https://github.com/open-mmlab/mmdetection](https://github.com/open-mmlab/mmdetection)

MM Detector Fast RCNN, open mmlab, [https://github.com/open-mmlab/mmdetection/blob/master/configs/_base_/models/fast_rcnn_r50_fpn.py](https://github.com/open-mmlab/mmdetection/blob/master/configs/_base_/models/fast_rcnn_r50_fpn.py)

mmcv api, [https://mmcv-jm.readthedocs.io/en/stable/index.html](https://mmcv-jm.readthedocs.io/en/stable/index.html)

