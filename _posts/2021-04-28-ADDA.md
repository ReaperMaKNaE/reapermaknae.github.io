---
layout : post
title : "ADDA"
date : 2021-04-28 +0900
description : GAN을 응용하여 classification에 domain adaptation을 적용한 ADDA 논문의 간단한 리뷰입니다.
tag : [PaperReview]
---

### Adversarial discriminative domain adaptation, CVPR 2017



### Abstract

 adversarial learning 방법이 deep network에서 중요한 자리를 잡고 있다. domain shift나 dataset의 bias에도 불구하고 recognition을 잘 상승시켜왔다. 하지만, GAN이 강력한 visualization을 보였음에도 불구하고 smaller shift에서만 수렴하는 결과를 보였다. 반면에, discriminative한 접근방식들은 조금 더 큰 domain shift도 다룰 수 있었지만, model의 weight에 묶여있고 GAN-based의 loss를 이용할 수 없었다. 그래서 우리는 이 두 가지 방식의 장점을 하나로 융합한 generalized framework for adversarial adaptation 방법을 제안한다. 우리는 이를 Adversarial Discriminative Domain Adaptation(ADDA)라고 부르기로 하였다. 우리는 ADDA가 domain adversarial method 중에서 보다 간단하고 효과적인 것을 보여줄것이다. 또한, 단순한 domain adaptation 뿐 아니라 cross-modality object classification task에서도 적용이 가능하다는 것을 증명할 것이다.



### Introduction

 recognition task에서는 큰 dataset에 train을 시키게 되면 새로운 dataset이나 task에 쉽게 적응하지 못하는 것을 발견할 수 있다. 이러한 것의 해결법으로는 이러한 network을 task-specific한 dataset에 fine-tune을 시키는 등이 방법인데, 이는 너무 cost가 크다.

 보통 새로운 optimization 방법인 maximum mean discrepancy나 correlation distance등을 이용해서 해결하려고 했다.

 최근은 GAN이 등장하면서 여러 GAN을 base로 하는 model들이 등장했다. domain adaptation에서는, GAN의 원리를 이용하여 network가 training과 test set의 차이를 구별하지 못하도록 하는 것이었다. 하지만, 각 알고리즘은 서로 다른 design choice를 가지게 되는데,

- generator를 사용하느냐 마느냐
- loss function을 쓰냐 마냐
- domain간의 weight를 share하느냐 마느냐

이다.

 이번 work에서, 우리는 adversarial domain adaptation의 통합된 새로운 framework을 제안한다. 

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210428-1.PNG)

 위 그림에서 우리는 새롭게 제안한 ADDA를 볼 수 있다. asymmetric mapping을 이용하여 같은 space의 target domain과 source domain의 discriminative를 학습한다. 이렇게 학습된 모델은 MNIST, USPS, SVHN과 같은 곳에서 사용이 되거나 cross-modality shift와 같은 큰 차이에도 적응을 하는 것을 확인할 수 있다.



### Related Work



### Generalized adversarial adaptation

 우리는 adversarial unsupervised adaptation method의 일반적인 framework을 제안한다. unsupervised adaptation에서, 우리는 source domain distribution $p_s(x,y)$ 에서 source images $X_s$ 와 labels $Y_s$ , target distribution $p_t(x,y)$ 에 의해 target images $X_t$ 에 접근을 할 수 있다고 가정한다.(단, target에서는 label observation이 없다고 한다.) 우리의 목표는 domain annotation의 부족에도 불구하고 test time에서 K category 중 하나에 target image가 classify될 수 있도록 target representation $M_t$ 와 classifier $C_t$ 에 대해서 학습하는 것을 목표로 한다. target에 바로 supervised learning을 적용하는 것은 불가능 하기 때문에, domain adaptation은 source representation mapping $M_s$ 와 source classifier $C_s$ 를 학습한 다음 target domain에 적용하도록 한다.

 adversarial adaptive method에서, 주 목표는 source와 target mapping distribution인 $M_s(X_s)$ 와 $M_t(X_t)$ 를 최소화하도록 source와 target mapping의 학습을 regularize하는 것이다. source classification model $C_s$ 가 target representation에 바로 적용이 될 수 있다고 한다면, classifier 간의 학습은 별로 의미가 없어지게 된다. 즉, $C=C_s=C_t$ 가 된다.

 source classification model은 아래와 같은 standard supervised loss에 의해 학습이 된다.

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210428-2.PNG)

 이제 우리는 general framework에 대한 내용을 기반으로 설명을 할 것이다. 2가지 방법에서 source와 target representation distance를 최소화한다. 첫째, domain discriminator이다.  이 친구는 data가 source에서 온건지 target에서 온건지 구분한다. 그래서 우리는 아래와 같은 supervised loss를 사용한다.

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210428-3.PNG)

 두번째로 source와 target mapping은 constrained adversarial object에 대해서 optimize가 된다. 그래서 우리는 아래와 같은 일반적인 식을 얻을 수 있다.

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210428-4.PNG)

 다음 section에서 우리 framework의 가치에대해 설명을 할 것이다. 또한 potential mapping structure, mapping optimization constraints( $\psi(M_s, M_t )$ ) 선택, 그리고 마지막으로 adversarial mapping loss인 $L_{advM}$ 에 대해 설명한다.

__3.1. Source and target mappings__

 label $Y_s$ 를 알고 있어서 이를 이용해 loss를 계산하는 supervised training에서 source mapping $M_s$ 를 학습하는 것은 final source recognition에서 best representation이다. 하지만, 우리의 target domain의 경우에는 unlabeled 이므로, 어떻게 source와 target mapping에서 distance를 minimize하는 지는 여전히 열려있는 질문이다.(open question 상태) 그러므로 우리의 첫번째 선택은 이러한 mapping을 특별한 parameterization으로 두는 것이다.

 unsupervised domain adaptation이 classification과 같은 target discriminative task를 고려하기 때문에, 이전의 adaptation method는 domain 사이의 discriminative model을 adapting하는 것에 의존했다. 이하 기타 이야기들.

 mapping parameterization이 source에서 정해지기만 한다면, 우리는 이를 어떻게 target mapping $M_t$ 를 parameterize할 수 있을지 결정해야했다. 일반적으로, target mapping은 항상 source와 matching이 이루어진다.(specific functional layer에서) 하지만 다른 방법은 여러 regularization 기술을 제안하기도 했다. 모든 방법들은 target mapping parameter를 source로 initialize하고 source와 target mapping $\psi(M_s, M_t)$ 에 다른 constrain을 거는 방법을 선택했다. 이러한 행동의 목표는 source와 target domain 사이의 distance를 minimize하기 위한 것이다. 

 각 layer parameter의 layered representation을 $M_s^l$  혹은 $M_t^l$  등으로 표현한다고 하자. 그러면, constraint는 다음과 같이 layerwise equality constraint를 통해 설명이 될 수 있다.

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210428-5.PNG)

 여기서 각각의 layer는 constrain이 독립적일 수 있다. 가장 흔한 source와 target layerwise equality의 constraint의 형태는 다음과 같다.

![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210428-6.PNG)

 위 형태 또한 layer가 unconstrained 상태로 남아있게 된다. 이러한 equality constraint는 쉽게 CNN에서 weight sharing이 가능하다. 

 많은 이전의 adversarial adaptation method는 항상 모든 layer가 constrain하도록 하여 source와 target mapping의 consistency를 정확하게 하도록 강요했다. symmetric transformation을 학습하는 것은 모델의 parameter의 수를 줄이고 source domain으로 적용이 될 때 target이 discriminative하게 사용되는지 mapping을 확실하게 하는 것에 도움이 된다. 하지만, 이러한 것은 서로 다른 두 도메인에서 오는 image들을 반드시 같은 network에서 다루어야했기 때문에 좋지 않은 상태로 optimization이 되도록 한다.

 이에 대한 대책으로, partial alignment를 강요하기 위해 layer constrain의 이루만 사용하는 asymmetric transformation을 학습하는 것이다. 이러한 방법이 supervised setting과 unsupervised setting모두에서 효과적인 adaptation을 이끌어내는 것을 보여주었다. 결과적으로, 몇몇 최근 방법들은 두 domain 사이에서 각자 untying weight를 적용하는 방법을 선호한다.

__3.2. Adversarial losses__

 $M_t$ 의 parametrization을 결정하자마자, 우리는 actual mapping을 배우기 위해 adversarial loss를 사용했다. adversarial loss function을 선택하는 것에는 다양한 옵션들이 있다.

 우리가 GAN을 training할 때에는 mini-max loss를 직접적으로 사용하기보다는 inverted label을 이용한 standard loss function과 함께 generator를 학습시킨다. 이는 두 서로 독립된 objective로 optimization을 나누게 되는데, 하나는 generator이고 다른 하나는 discriminator이다. $L_{adv_D}$ 는 바뀌지 않은 채, $L_{adv_M}$ 은 다음과 같이 표현된다.

![img7](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210428-7.PNG)

 위와 같은 형태의 loss는 조금 더 target mapping에서 강한 gradient를 제공한다. 이렇게 수정된 loss function을 GAN loss function으로 표현한다.

 이러한 setting에서 우리는 source와 target의 mapping이 independent하고 $M_t$ 만 학습하게 된다.

 GAN loss function은  generator가 다른 변화하지 않는 distribution을 흉내내려고 시도할 때 setting을 선택하는 것이 standard 하다. 하지만, 두 distribution이 모두 바뀌는 경우에서는, 이러한 목표는 oscillation을 만들어낸다. 이러한 혼란을 막기 위해, 아래와 같은 cross-entropy loss function을 사용한다.

![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210428-8.PNG)



### Adversarial discriminative domain adaptation

 우리 방식의 일반화된 framework의 장점은 새로운 adaptive method를 바로 적용할 수 있다는 점이다. 실제로, 3 가지의 방식을 선택할 수 있다.

- generative 혹은 discriminative base model을 사용할 것인가
- tie혹은 untie weight를 사용할 것인가
- 어떤 adversarial learning objective를 사용할 것인가

 위와 같은 것으로 인해, 우리의 방법인 ADDA를 아래와 같이 정리할 수 있다.(이전의 work들과 같이)

![img9](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210428-9.PNG)

 이를 그림으로 표현하면 아래와 같다.

![img10](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210428-10.PNG)

 첫째로, 우리는 discriminative model을 선택했다. 이걸 선택한 것으로 CoGANs가 있는데, 매우 비슷한 domain에서만 유효하고, large distribution shift에서는 동작하지 않았다.

 다음으로, 우리는 untying weight를 이용한 source와 target mapping의 independent를 허용했다. 이는 보다 learning paradigm을 flexible하게 만들어준다. 하지만 target domain에 대한 label access가 없기 때문에, weight sharing이 없는 target model은 우리가 initialization을 잘 해 주지 않는다면 이상한 방향으로(잘 학습되지 않는 방향으로) 갈 가능성이 크다. 그러므로, 우리는 pre-trained source model을 target representation의 initialize로 사용한다.

 이를 사용하면서, 우리는 효과적으로 asymmetric mapping을 할 수 있었다. 이러한 방식은 original GAN과 유사하기에, 우리는 inverted label GAN loss 를 이전에 설명했었다.

 우리가 제안한 방법에서, 아래와 같은 unconstrained optimization이 따라온다.

![img11](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210428-11.PNG)



### Experiments

MNIST, USPS와 SVHN으로 domain을 비교하였고, NYUD, standard office dataset을 추가로 사용했다.

__5.1. MNIST, USPS and SVHN digits datasets__

![img12](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210428-12.PNG)

__5.2. Modality adaptation__

![img13](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210428-13.PNG)

__5.3. Office dataset__

![img14](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210428-14.PNG)



### Conclusion



### Reference

Tzeng, Eric, et al. "Adversarial discriminative domain adaptation." *Proceedings of the IEEE conference on computer vision and pattern recognition*. 2017.

