---
layout : post
title : "ProDA"
date : 2021-06-09 +0900
description : Depth estimation에 surface normal을 응용한 논문인 Normal Assisted Stereo Depth Estimation의 리뷰입니다.
tag : [PaperReview]
---

### Prototypical Pseudo Label Denoising and Target Structure Learning for Domain Adaptive Semantic Segmentation



### Abstract

 Self-training은 target domain에 대한 pseudo label의 network를 훈련하는 domain adaptive segmentation에서 좋은 접근법이다. 하지만 필연적으로 pseudo label들은 noisy하고 target feature에 대해서는 별로 좋지 못했다.(source와 domain사이의 차이 때문에) 이번 논문에서, 우리는 representative prototypes에 의존해서 unsupervised domain adaptation의 두 큰 문제를 해결할 것이다. 특히, 우리는 mere prototype보다 더 풍부한 정보를 제공하는 prototype들에서 feature distance를 계산한다. 특히, 우리는 학습 도중에 online correlation을 이용하기 위해 pseudo label간 유사도를 예측한다. 반면에, 우리는 같은 target의 다른 view에서 오는 상대적인 feature distance를 기반으로 한 prototypical assignment를 정렬해서 훨씬 간단한 target feature space를 만든다. 더욱이, 우리는 현재 알고 있는 지식의 distilling이 self-supervised trained model의 성능을 더욱 boosting 하게 된다. 우리의 방법은 여러 SOTA model들 중에서 가장 좋은 성능을 보였다.



### Introduction

 computer vision에서의 deep learning이 눈에 띄게 성공함에도 불구하고, 좋은 성능을 내기 위해선 아직도 많은 양의 데이터가 필요하다. semseg과 같은 data를 얻으려면 매우 비싼 값을 지불해야 한다. 그러므로, 사람들은 무료로 label을 만들어주는 photo-realistic synthetic image를 이용하기로 했다. 하지만, deep neural network는 domain misalignment에 대해서 극도로 민감하다. 이를 해결하기 위해, domain adaptation이 많이 나오고 있다.우리는 이번 work에서 challenge한 UDA를 다룬다. 더 구체적으로는, semantic segmentation에 대한 UDA이다.

 explicit하게 source와 target 사이의 distribution을 해결하려는 것 보다, self-training을 이용해서 UDA task에 적용한 사례가 많아지고 있다. 이는 network를 새로 train하기 위해 pseudo label들에 의존하기보다 target data에 대해서 가장 confident가 높은 것을 기반으로 한 pseudo label의 set에서 iterative하게 만들어지는 것에 의해 달성된다. 이러한 방법은, network가 점진적으로 self-paced curriculum learning에 적응하도록 학습시킨다. 하지만 그 성능은 여전히 supervised learning에 비해 많이 부족하다.

 self-training을 해부해보면, 우리는 이전의 work들에서 2개의 key 재료가 부족하다는 것을 알 수 있다. 첫번째로, 전형적인 방법들은 high score가 절대적인 지표가 아님에도 불구하고 confidence threshold를 엄격하게 따르는 pseudo label을 선택해서 network가 target domain에서 잘 안되는 경향을 보이기도 했다. 둘째로, domain gap때문에 network는 target domain에 맞지 않는 feature를 만드는 경향이 있다. 결국, source distribution에서 먼 곳에 위치하는(low score object) 경우에는 학습이 되지 않게 된다.

 이번 논문에서 우리는 online denoise the pseudo label을 online화 하고 간단한 target structure를 배울 수 있는 network을 제안한다. 우리는 prototype들을 다음 2 task를 성취하기 위해 재분류를 했다.

- 우리는 모든 class prototypes에 상대적인 feature distance를 이루기 위해 class-wise likelihodd를 예측함으로써 pseudo label을 rectify한다.

 이는 주어진 cluster의 실제 centroid에 얼마나 가까이 prototype이 놓여져있는지를 가정하는 것에 의존하여 false pseudo label을 진작에 걸러낸다. 이는 prototype이 on-the-fly로 연산이 되어서 training과정 중에 pseudo label이 점진적으로 잘 학습되는 것에 도움을 준다.

- 우리는 DeepCluster에 영감을 받아, target domain의 intrinsic structure를 학습한다.

 cluster assignment로 부터 직접 학습하는 것 대신에, 우리는 같은 target에 대해서 여러 view에 대한 prototypical assignment를 정렬하는 것을 제안한다. 이는 훨씬 compact한 feature space를 만든다. 우리는 우리의 방법을 __ProDA__ 라 한다.

 위 technique들의 큰 변화와 함께 ProDA는 이전의 work들을 압도한다. 더욱이, 우리는 domain adaptation이 task-agnostic pretraining에 적합하다는 것을 알 수 있었다. 이들은 self-supervised model의 knowledge distilling을 해결한다. 우리의 contribution은 아래와 같이 요약될 수 있다.

- 우리는 prototype들이 on-the-fly하게 update되는, prototype들의 상대적인 feature distance에 대한 soft pseudo-label들을 online 으로 correct하는 것을 제안한다.
- 우리는 soft prototypical assignment이 augmented view에서의 학습을 유도하여 compact target feature space를 얻을 수 있는 것을 제안한다.
- 우리는 self-supervised pretrained model의 knowledge를 distilling하는 것이 성능 향상을 이끌어 내는 것을 보여준다.
- ProDA는 다른 SOTA Model들을 우세하는 성능을 보여준다.



### Related Work

__Unsupervised domain adaptation.__

__Unsupervised representation learning.__

__Learning from noisy labels.__



### Preliminary

 주어진 source dataset $\chi$ 와 segmentation label $\mathcal{y}$ 에서 우리는 source에서 knowledge를 학습하고 target dataset의 ground truth label없이 학습하는 것을 목표로 한다. 일반적으로, network는 $h=g\cdot f$ 이며, $f$ 는 feature extractor, $g$ 는 classifier이다.

 전형적으로, network는 source model, 즉 source data에서 학습이 되면 target data의 domain gap에 잘 generalize하지 못한다. knowledge를 잘 transfer하기 위해, 전통적인 self-training technique들이 categorical CE와 pseudo label을 이용해서 optimize하게 된다.

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210609-1.PNG)

 위 식에서 $p = h(x)$ 를 말하며, $p^{i,k}$ 는 pixel i에서 class k에 대한 softmax probability를 말한다. 전형적으로, 대표적인 probable class predicted by source network는 다음과 같이 얻어진다.

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210609-2.PNG)

위 식에서 우리는 soft prediction에서 hard label로의 변경을 $\hat{y} = \xi(p)$ 로 표현한다. 실제로는 pseudo label이 매우 noisy하기 때문에, 단순히 threshold보다 높은 confidence를 가지는 pixel들은 retrain을 한다. 이러한 방법에서 network는 convergence가 될 때 까지 target domain에서 pseudo label을 이용해 학습을 하게 되고, update된 label은 다음 training stage로 넘어가게 된다.



### Method

__4.1. Prototypical pseudo label denoising__

 우리는 한개의 training stage 이후에 pseudo label을 update하는 것이 매우 느리고 noisy label에 overfitting이 된다고 추측한다. 반면에, network weight와 pseudo label을 즉시 update하는 것은 이러한 문제에 대한 trivial solution이 될 수 있다.

 이번 work에서 우리는 simple하지만 여전히 효과적인, trivial solution을 피하면서도 pseudo label을 online으로 update하는 방식을 제안한다.







### Reference

Zhang, Pan, et al. "Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation." *arXiv preprint arXiv:2101.10979* 2 (2021): 1.
