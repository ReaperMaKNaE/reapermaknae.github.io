---
layout : post
title : "MoCo"
date : 2021-03-31 +1200
description : 새로운 contrastive learning architecture에 대해 연구한 MoCo 논문의 간단한 리뷰입니다.
tag : [PaperReview]
---

### Momentum contrast for unsupervised visual representation learning, CVPR 2020



### Abstract

 contrastive learning에서 dynamic dictionary를 queue와 moving averaged encoder를 이용하여 만들었다. 이러한 방법은 contrastive unsupervised learning에서 탁월한 효과를 보였는데, 이번에 우리가 제안하는 MoCo는 linear protocol에서도 다른 모델에 뒤쳐지지 않는 성능을 보여주었고, downstream task에서도 좋은 성능을 보여주었다.



### Introduction

 Unsupervised representation learning은 GPT와 BERT등의 결과를 보면 나름 성공적으로 보인다. 대부분의 unsupervised learning은 encoder를 dictionary look-up으로 쓸 수 있도록 학습시킨다. encoded query의 경우, matching하는 key와는 유사하고 나머지와는 달라야 한다. 이런 구조에서 사용되는 loss는 contrastive loss이다.

 이러한 관점에ㅓ 봤을 때, 우리는 두 가지 조건을 만족하는 dictionary를 만들고 싶었는데,

- 크고
- consistent한

 dictionary이다. 이렇게 만든다면, 결과적으로 더 많은 key와 비교하게 되면서 정확한 loss를 얻을 수 있다고 생각하기 때문이다. 우리가 제안할 모델의 overview는 다음과 같다.

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210331-1.PNG)

 위 그림에서 우측의 queue에 들어가는 key는 현재 mini batch의 값이면 enque를 하고, 오래된 key들이 들어가있으면 deque를 하는 방식으로 queue를 update하게 된다. 여기서 key의 경우 그 값이 항상 일정하지는 않은데, 그렇기 때문에 encoder에서 생기게 되는 loss를 조금씩 momentum을 이용해서 update해주게 된다.

 이렇게 만들어진 MoCo는 여러 dataset에서 그 성능이 준수함을 보여주었다.



### Related Work

 Unsupervised/self-supervised에 대한 연구를 진행한 결과를 담은 논문의 경우 대개 pretext task와 loss function에 대해서 쓰여져있지만, 우리는 loss function에 조금 더 focus를 두었다.

__Loss functions.__

 여러 방식이 있는데, cross-entropy나 margin-based loss이다. 이것 말고도 GAN이나 NCE등도 쓰인다.

__Pretext tasks.__

 많은 종류의 pretext task가 나왔지만, 우리는 새로운 pretext task를 선보이진 않았다. (그래서 대충 원래 어떤 것들이 있었는지에 대한 설명들이 나옴)

__Contrastive learning vs. pretext tasks.__

 CPC(Contrastive predictive coding)에서 pretext task는 context auto-encoding의 방식이었고, contrastive multiview coding에서는 colorization과 연관이 있었다.

(이 위쪽으로는 사실 나만 알아보도록 쓴거라서 넘겨도 된다. 중요한 내용은 지금부터.)



### Method

__3.1 Contrastive learning as dictionary look-up__

 Contrastive learning에서 최근 동향은, encoder를 dictionary look-up 할 수 있도록 training 시키는 것이다.

 encoded query를 q라고 하고, encoded sample을 k라고 하자. 여기서 encoded query가 positive key에 matching이 되는 경우, 이 key를 k+라고 하자. 여기에 temperature hyper-parameter를 추가한 다음, infoNCE라는 contrastive loss function의 형태로 나타낼 수 있다.

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210331-2.PNG)

 맨 위 그림을 생각했을 때, matching이 되는 단 하나의 k만 존재하고 그걸 k+라고 한다면, encoded query와 k+를 dot product한 다음 위와 같은 loss식을 이용한다면, loss의 값은 query와 key+값이 matching이 잘 될 수록 그 크기가 작아지게 된다. 그렇게 하여, mini-batch 사이에서 정확하게 어떤 값과 matching이 되는지를 찾게 되고, 이렇게 encoder를 지난 query가 matching이 잘 되도록 encoder의 hyper-parameter에 back propagation을 진행하게 된다.

__3.2 Momentum Contrast__

 우리는 large dictionary를 가지고 있다면 좋은 feature를 학습할 수 있다는 생각을 가지게 되었고, 이로 인해 효율적인 구조를 생각하는 방향에서 momentum contrast를 제안하게 되었다.

__Dictionary as a queue.__

 현재 mini batch는 dictionary로 enque되고, 오래된 mini batch(아마도 이전에 학습하는데 썼던 batch를 말하는 듯)는 dictionary에서 deque가 된다. 이러한 방식은 실제로 outdate된 encoded key를 걸러내는 것에도 유용하다.

__Momentum update.__

 queue를 만들어내는 momentum encoder에 back-propagation을 하는 방법은 아주 다루기 힘든 문제다.(음... 1:1 matching이 아니라 1:다수 matching에서 encoder에 back-propagation을 해버리게 되면 momentum encoder의 경우 그 결과가 loss만 작아지게 만들면 되기 때문에, query가 key+에 matching이 되는 것에 focus를 맞추는게 아닌, 다른 값들이 발산하도록 만들어버리기 때문임) 그래서 우리는 key encoder를 query encoder에서 복사한다음 gradient를 무시하는 방법을 써봤는데, 성능이 박살났다. 그래서 우리는 새로운 momentum update를 사용하였다.(다 들고 오는 것이 아니라, 부분적으로 선택해서 들고온다는 뜻)

 여기서 parameter에 대한 update는 아래 식과 같다.

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210331-3.PNG)

 위 식에서 k는 key를, q는 query를 의미한다. 나중에 ablation study에서 나오기는 하지만, 논문의 저자는 key parameter에 훨씬 더 많은 weight를 주게 된다.

 __Relations to previous mechanisms.__

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210331-4.PNG)

 위 그림은 개념적으로 크게 나뉘어지는 3가지의 contrastive loss mechanism에 대한 비교이다. 위 그림에서 가운데에 해당하는 memory bank는 그냥 dataset을 어디에다 다 한군데에 모아두고 sampling해서 쓰는 것. encoder의 과정을 거치지 않는 걸 말한다.(즉, dataset이 크면 클수록 저장 용량이 엄청 많이 필요한 형태의 model)

 MoCo는 이러한 memory bank를 mini-batch size로 줄인 다음, queue를 통해서 구현한 방법이라고 보면 된다.

__3.3 Pretext task__

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210331-5.PNG)

  pretext task에 대한 내용은 아니지만, 아무튼 MoCo의 pytorch-style pseudo-code는 위와 같다. 어떻게 이루어지는지 확인할 수 있다.

__Technical details.__

 temperature hyper-parameter인 $\tau$는 0.07로 두었고, 224x224 pixel의 image를 randomly resized image, random horizontal flip, random grayscale conversion을 적용시켰다고 한다.

__Shuffling BN.__

 우리의 query encoder와 key encoder는 모두 batch normalization이 있는데, 이 batch normalization의 문제가 loss를 줄이는 방향으로 'cheat'하는 것을 확인했다.(batch normalization의 역할은 residual branch의 크기를 줄이고, mean-shift를 없애고, regularizing effect를 주며, large batch training에 효과적인 것을 기억하자.) Intra-batch communication으로 인해 information의 leak이 발생한 것이다. 저자는 이러한 방법을 shuffling BN을 통해서 해결했다고 한다. 해당 방법은 torch.nn.Parallel을 통해 GPU에 할당하기 전 한번 섞어서 넣고, 다시 나왔을 때 섞은걸 다시 되돌린다고 한다. 이걸로 BN을 avoid할 수 있는게 신기하긴 한데, 아무튼 잘 해결이 됬다고 한다.

 참고로 이러한 방식은 memory bank에는 적용이 되지 않았다고 한다. 막대한 양의 data가 다 들어있기 때문에, sampling해온 data는 어짜피 BN을 쓰나 안쓰나 똑같은 모양이다.



### Experiments

__ImageNet-1M(IN-1M)__

__Instagram-1B(1G-1B)__

__Training__

__4.1. Linear Classification Protocol__

 __Ablation: Contrastive loss mechanisms__

 크게 나뉘어지는 3가지의 contrastive loss mechanism에 대한 결과는 아래와 같다.

![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210331-6.PNG)

  위 그림에서 end-to-end의 경우 batch-size가 1024에서 멈췄는데, 8 Volta 32GB GPU로 돌린 한계라고 한다.(역시 facebook...) 그래서 그만큼만 했는데, MoCo의 경우 효율적인 결과를 보여주고 있음을 알 수 있다.

__Ablation: momentum.__

![img7](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210331-7.PNG)

 momentum에 대한 ablation study의 결과이다. 여기서 K는 4096을 사용했다고 한다. momentum이 0인 것은, key encoder와 query encoder가 같다는 것을 의미한다. 두 개가 같게 된다면 발산하게 되는 것을 확인할 수 있고, 0.999가 가장 효과가 좋았음을 알 수 있다.

__Comparison with previous results.__

![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210331-8.PNG)

 위 그림에서 backbone에 따른 결과를 확인할 수 있다. R50w4x의 경우는 ResNet50에서 width를 4배한 것이다. 보면 parameter와 accuracy의 비교에서 상당히 좋은 자리에 위치하고 있음을 알 수 있다.

(CMC-R50w2x가 trade off를 따졌을 땐 가장 좋아보이는 건 맞음, 하지만 이 친구들이 곧 MoCo v2를 발표하면서, R50에서의 accuracy가 60.6%에서 71.1%로 올라감)

__Transferring Features__

 Unsupervised learning의 main goal은 transferrable feature를 학습하는 것인데, 우리는 이러한 transferring feature에서 중요한 normalization과 schedule에 대해 한번 discuss 해보겠다.

__Normalization__

 우리는 feature normalization을 fine-tuning동안 적용했다.

__Schedules__

 학습하는 시간을 의미한다. x1 기준 12 epochs이다.

__4.2.1 PASCAL VOC Object Detection__

__Setup__

 Faster RCNN을 사용하였고, backbone은 R50-dilated-C5 혹은 R50-C4를 사용했다.

__Ablation: backbones.__

![img9](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210331-9.PNG)

 transferring accuracy가 model의 backbone에 영향을 받는다는 것을 확인할 수 있었다. pre-training과 detector structure에 대한 관계가 여태 veil이었는데, 미래에는 이러한 관계를 조금 더 확인할 수 있기를 바라고 있다고 한다.

__Ablation: Contrastive loss mechanisms.__

![img10](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210331-10.PNG)

 기존의 다른 방식들과 비교했을 때 MoCo가 좋은 성능을 뽐내는 것을 확인할 수 있음.

__Comparison with previous results.__

![img11](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210331-11.PNG)

 MoCo가 supervised method와 거의 유사한 정확도를 낼 정도로 훌륭한 성능을 보이는 것을 확인할 수 있고, dataset 마다 다르긴 하지만 1G-1B에서 큰 정확도 향상을 볼 수 있음.

__4.2.2. COCO Object Detection and Segmentation__

__Setup__

__Results__

![img12](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210331-12.PNG)

 MoCo가 supervised보다 우수한 성능을 낸 것을 확인할 수 있음.

__4.2.3 More downstream Tasks__

![img13](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210331-13.PNG)

 각 downstream task에서 어떤 결과를 냈는지를 확인할 수 있다. supervised를 제끼는 신기한 결과를 얻을 수 있다는 것을 확인할 수 있다.

__Summary.__

 MoCo가 기본 contrastive learning 방법들 보다도 성능이 뛰어나다는 것을 여러 실험을 통해 증명하였다. 특히, supervised learning 방식보다도 뛰어난 task를 수행하는 경우도 있었다.



### Discussion and Conclusion

 저자들은 advanced pretext task가 적절한 downstream task에서 성능을 끌어올릴 수 있다고 생각한다.



### Appendix

__A.1. Implementation: Object detection backbones__

__A.2. Implementation: COCO keypoint detection__

__A.3. Implementation: COCO dense pose estimation__

__A.4. Implementation: LVIS instance segmentation__

![img14](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210331-14.PNG)

__A.5. Implementation: Semantic segmentation__

__A.6. iNaturalist fine-grained classification__

![img15](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210331-15.PNG)

__A.7. Fine-tuning in ImageNet__

![img16](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210331-16.PNG)

__A.8. COCO longer fine-tuning__

![img17](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210331-17.PNG)

 여러 실험을 해 보니, fine-tuning을 오래 하는 것이 중요하다고 한다.

__A.9. Ablation on Shuffling BN__

![img18](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210331-18.PNG)

 위 그림과 같이 ShuffleBN이 없는 경우에는 train(점선) 과정에서 overfitting이 일어나고 valid(실선) 과정에서 성능저하가 있음을 확인할 수 있다.



### Reference

He, Kaiming, et al. "Momentum contrast for unsupervised visual representation learning." *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2020.

