---
layout : post
title : "ViT"
date : 2021-04-08 +0900
description : Vision task에 transformer를 접합시킨 Vision Transformer를 다룬 논문의 간단한 리뷰입니다. 
tag : [PaperReview]
---

### An Image is Worth 16x16 Words: Transformers for Image Recognition At Scale



### Abstract

 최근 Transformer를 이용한 model이 NLP에서 좋은 성능을 계속 뽐내고 있는 것을 확인할 수 있다. 하지만 컴퓨터 비전에서 이러한 것을 적용한 사례는 아직 없다. 대부분이 CNN에 아직도 의존하고 있는 상황이다. 그래서 우리는 이를 Transformer에 적용시킬 수 있는 연구를 진행했고, 그 결과 CNN에 준수한 성능을 내는 것을 확인할 수 있었다. large amount of data로 먼저 pre-train을 하고, 상대적으로 작은 size의 dataset에서 image recognition task를 수행했고, 그 결과로 좋은 성능을 뽐낼 수 있었다.



### Introduction

 사실 지금 NLP 시장은 사실 Transformer에 먹혔음. Transformer의 한계가 애매한 것이, 모델이 커지면 커질수록(Transformer를 많이 복붙해놓은 모델일수록) accuracy gain에서 diminishing한 point가 점점 뒤로 밀리게 됨(즉, 학습의 한계가 매우 높음)

 하지만 반대로, 불충분한 data로 학습을 하게 되는 경우 generalize가 잘 안되는 경향이 있음.

 하지만, 이러한 것도 pre-train만 어느정도 잘 시켜놓으면 괜찮음.



### Related Work

 Transformer의 경우 정말 다양한 방법을 통해 학습을 잘 시키게 되었음. Vision task에 transformer를 접목시키려는 사례도 많았는데, self-attention 등의 개념을 이용해서 하는 경우라던지 등의 접근 사례가 있었다. 일단 현재 우리가 진행하고 있는 것과 가장 가까운 것은 iGPT인데, generative model로 unsupervised fashion으로 training한 것이라 우리랑은 조금 다르다.

 Transformer가 작은 dataset에서는 잘 converge 하진 않아서, ImageNet-21k나 JFT-300M과 같은 large scale dataset에 focus를 두었다.



### Method

 model design에서 우리는 original Transformer를 더욱 자세하게 살펴봤다. 그 결과로, ViT가 나왔다.

__3.1 Vision Transformer(ViT)__

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210408-1.PNG)

 Model의 overview는 위와 같다. patch 단위로 잘라서 input으로 먹여줬다는 것이 특징이다. 그리고 transformer의 특징을 담아와, 각 patch의 위치를 알려주는 position embedding과 class를 추가로 넣어주었다. 위의 과정을 식으로 표현하면 아래와 같다.

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210408-2.PNG)

 식 (1)은 embedding된 patch들을 의미하고, 식 (2)는 MSA를 지나고 나온 값, 식(3)은 MLP를 지나고 나온 값, y는 결과값을 의미한다. 특이하게도, BN(Batch normalization)이 아닌 LN(Layer Normalization)을 사용했다.

__Hybrid architecture__

 Patch를 뽑을 때, image에서 뽑아내는 것이 아니라 CNN에서 뽑아낸 feature에서 patch를 뽑아내는 방식을 이용한 architecture이다.

__3.2. Fine-tuning and higher resolution__

 ViT를 large dataset에서 학습을 시키고 downstream task에 fine-tune을 했다고 한다. 이를 위해 pre-train에서 사용한 prediction head를 떼어버리고, downstream class 만큼의 layer를 추가로 붙였다고 한다. 또한, pretraining 보다 높은 resolution으로 fine-tuning시키는 것이 효과적이라는 결과가 있다. 그런데 이렇게 하는 경우 position-embedding을 그대로 사용하는 경우 큰 의미가 없어져, interpolation으로 늘려서 사용했다.



### Experiments

__4.1. Setup__

__Datasets.__

 Dataset으로는 ImageNet-21k와 JFT 300M을 사용했고, transfer learning을 위한 dataset으로는 ReaL labels, CIFAR-10/100, Oxford-IIIT Pets, Oxford flowers-102 등을 사용했다.

 추가적으로 VTAB classificiation에서도 평가했는데(그냥 transfer training의 새로운 지표 같음), Natural, Specialized, Structured 로 나뉘어진 구조이다. Natural은 Pets, CIFAR 등이고, Specialized는 medical, satellite image와 같은 것, structured는 localization과 같은 geometric한 구조의 이해에 대한 것이다.

__Model Variants.__

 ViT의 경우 Base, Large, Huge로 나뉘어져 있으며, patch를 얼마나 나누었느냐에 따라 슬래쉬기호와 함께 숫자를 덧붙여 표현했다. 예를 들어, ViT-L/16은 Large variant에 16x16 patch를 사용한 model.

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210408-3.PNG)

 Transformer의 patch size가 작아지게 되면 그만큼 sequence length가 커지게 되고, model은 좀 더 expensive하게 된다.

 Hybrid의 경우, BN을 GN으로 변경했다고 하고, standardized convolution을 사용했다고 한다.(??)

__Training & Fine-tuning__

 linear learning rate warmup과 decay를 사용했다. 자세한 내용은 appendix 참조.

__Metrics__

 few-shot or fine-tuning accuracy를 통해 downstream dataset의 결과를 뽑아냈다.

__4.2. Comparison to state of the art__

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210408-4.PNG)

 다른 network들과 비교하였을 때 적은 parameter임에도 불구하고 좋은 성능을 내고 있는 것을 확인할 수 있다.

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210408-5.PNG)

 각 Task에서 ViT-H/14 가 다른 model들과 비교하였을 때 성능이 더 좋은 것을 위 그래프로 알 수 있다.(Table에 있는 것을 조금 더 세분화해서 잘 표현함)

__4.3 Pre-training Data requirements__

 ViT가 large JFT-300M dataset에서 pretrain이 되었을 때 성능이 좋다는 것을 확인할 수 있었다. 그럼 적은 dataset으로 하면 어떻게 될까? 해서, 다음과 같은 실험을 했다고 한다.

- 작은 dataset, 중간 dataset, 큰 dataset에서 각각 학습 시키기
- JFT-300M dataset에서 random subset으로 학습시키기

 그 결과는 아래와 같다.

![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210408-6.PNG)

 보면 dataset이 커질수록 transformer의 성장은 눈에 띄게 보이는 것을 확인할 수 있다.  그리고 그 마지막 성능도 다른 model들에 비해 좋은 것을 확인할 수 있다.

__4.4. Scaling Study__

 pre-training cost에 비해 transfer learning에 얼마나 효과적인지를 연구한 곳이다. 그 결과는 아래와 같다.

![img7](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210408-7.PNG)

 몇가지 pattern들을 관찰할 수 있는데,

- ViT가 ResNet을 trade-off면에서 surpass
- Hybrid architecture가 large model로 갈수록 ViT architecture와 그 차이를 줄여나감(즉, feature를 patch로 줘도 안 꿀린다)
- saturate하는 경향이 보이질 않음.(즉, model의 크기만 더 키우면 더 높은 accuracy 가능함)

__4.5 Inspecting Vision Transformer__

 Transformer가 vision task에서 어떤 역할을 끼치는지 제대로 알기 위해 조금 조사를 했다고 한다.

![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210408-8.PNG)

 각 attention들이 어디에 focus를 하고 있는지를 보여준다. Left의 경우 attention이 어느쪽을 더 잘 파악하고 있는지를 확인하는 것이고, middle image의 경우 patch의 position embedding에 대한 것이다. 우측의 경우는 좀 헷갈릴수도 있는데, attention들이 focus하는 것이 서로 다른 것인지 아닌지를 의미하게 된다. 같은 곳에 attention이 쏠려있는 것이 아니라, 여러 곳에 퍼져 있음으로 인해 receptive field가 넓어진 것을 알 수 있다.

아래는 attention이 focus하고 있는 곳을 나타내준 것.

![img9](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210408-9.PNG)

__4.6. Self-supervision__

 Transformer도 self-supervision이 가능하다. 기존에 사용하던 방법들인 masked patch prediction의 방법을 이용해서(아마도 pretext task의 일종) self-supervised learning을 하게 된다. 이 결과 우리는 ViT-B/16 에서 79.9%의 accuracy를 달성하였는데, 문제는 이게 모델이 커질수록 accuracy가 떨어진다. 이는 future work을 위해 남겨 두겠다.(를 바로 MoCo v3가 연구했다.)



### Conclusion

 Transformer를 성공적으로 vision task에 영입했다. 정도의 내용.



### Appendix

__A. Multihead Self-Attention__

__B. Experiment details__

__B.1. Training__

![img10](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210408-10.PNG)

__B.1.1. Fine-tuning__

![img11](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210408-11.PNG)

__B.1.2. Self-supervision__

 masked patch prediction을 했다고 한다. patch embedding에서 50%를 타락시켰는데, 이 embedding 중 80%는 learnable mask로, 10%는 random other patch embedding으로, 나머지는 그냥 그대로 두었다고 한다. 이러한 setup은 BERT에서와 매우 유사하다. 결과적으로 3-bit, mean color를 예측하는 것이 가장 효과가 좋았다고 한다.

 하지만 model의 성능을 올릴수록 accuracy가 떨어졌다고 한다. 몇가지 실험 방법이 있긴한데, 자세한 것은 논문 참조.

__C. Additional results__

![img12](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210408-12.PNG)

![img13](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210408-13.PNG)

__D. Additional Analyses__

__D.1. SGD vs Adam for ResNets__

![img14](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210408-14.PNG)

__D.2. Transformer shape__

 Transformer의 scaling을 조금씩 진행했다고 한다. 그 결과는 아래와 같다. 결국 scaling해서 크기만키우면 뭐든 accuracy는 늘긴 느는 모양.

![img15](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210408-15.PNG)

__D.3. Positional Embedding__

 positional embedding을 쓰냐 안쓰냐를 실험했다고 한다. 조건은 아래와 같다.

- No positional Emb.: bag of patches인 경우
- 1-dim Positional Emb.: 현재 까지 한 default setting
- 2-dim Positional Emb.: X, Y를 따로 만든 후 concat해서 학습시키는 경우
- Relative Positional Emb.: 상대적인 거리를 이용한 경우.

![img16](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210408-16.PNG)

 결과는 위와 같다. default setting이 가장 성능이 좋은 것을 확인할 수 있다. 모델의 Base는 ViT-B/16 model. 

![img17](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210408-17.PNG)

 참고로, visualization을 해보면 위와 같다. 조금 더 position embedding이 학습이 될 수록 잘 되는 것을 확인할 수 있다.

__D.4. Empirical Computational Costs__

![img18](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210408-18.PNG)

 computational cost도 측정했다고 하는데, input size에 따른 속도라고 한다. ViT의 경우 다른 model들에 비해 특이한 구조로(Transformer) 그 효과를 받아서 좋은 성능을 보이는 것을 확인할 수 있다.

__D.5. Axial Attention__

![img20](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210408-20.PNG)

 각 축으로 attention을 적용하는 axial attention을 적용한 transformer를 이용하여 모델을 꾸려본 결과,, 성능이 더 좋아졌다. 정확히는 AxialResNet50에 transformer를 추가한 구조인데(각 patch만 땡겨온 듯하다), inference time/accuracy가 모두 좋아진 것을 확인할 수 있다.

__D.6. Attention Distance__

![img19](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210408-19.PNG)

 각 축으로 attention이 얼마나 잘 되는지를 보여주고 있는 형태이다. network가 깊어질수록 attention의 거리가 멀어지면서 receptive field가 넓어지는 것을 확인할 수 있다.(약간 position embedding과 같이 학습이 되는 것 같기도 하고?)

__D.7. Attention map__

 해당 논문 참조. 여기에 올려봐야 작아서 잘 안 보일듯 하다. 맨 마지막 페이지에 있다.

__D.8. VTAB Breakdown__

 VTAB-1k task에서 각 task의 score를 기록한 것.

![img21](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210408-21.PNG)



### Reference

Dosovitskiy, Alexey, et al. "An image is worth 16x16 words: Transformers for image recognition at scale." *arXiv preprint arXiv:2010.11929* (2020).