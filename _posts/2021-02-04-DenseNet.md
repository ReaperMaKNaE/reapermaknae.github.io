---
layout : post
title : "DenseNet"
date : 2021-02-04 +2200
description : Densely Connected Convolutional Networks 논문의 간단한 리뷰입니다.
tag : [PaperReview]
---

### Densely Connected Convolutional Networks, CVPR 2017



### Abstract

 최근 CNN 연구에서는, input과 output 사이를 연결해주는 것으로도 상당히 깊고, 더 정확하고, 더 효율적인 훈련을 할 수 있다는 것을 보여주고 있다. 우리는 모든 각 layer를 feed-forward로 연결해서 dense하게 연결한 dense convolutional network를 소개한다. 전통적인 L 개의 layer를 가진 network는 L개의 connection을 가지게 되지만, 우리의 경우는 $\frac{L(L+1)}{2}$ 의 connection을 가진다. preceding layer에서 모든 feature map은 input으로 사용이 되고, 다음 일련의 layer에 input으로도 사용이 된다. 이러한 구조의 DenseNet은 다음과 같은 강렬한 장점들이 있다.

- vanishing gradient problem을 해결함
- feature propagation을 강하게 함
- feature reuse를 장려함
- parameter의 수를 확실하게 줄임.

  우리가 제안한 densenet은 4개의 높은 경쟁적인 object recognition에서도 평가했다. SOTA를 찍었고, 적은 연산으로 높은 성능을 얻을 수 있었다.



### Introduction

 visual object recognition에서 CNN은 중요한 machine learning으로 자리잡고 있다. 20년 전에 소개가 되었음에도 불구하고, hardware의 발전과 network structure의 발전으로 깊은 CNN의 훈련이 가능해졌다.

 CNN이 점점 깊어지면서, input이 end로 갈수록 약해지는 결과가 보여지게 되었다. 많은 최근의 연구들이 이를 여러 방법으로 해결했다. ResNet, Stochastic depth, FractalNet등등. 이렇게 서로 다른, 다양한 방법들은 공통된 특징을 가지고 있다. 바로 layer끼리의 지름길을 만드는 것이었다.

 이번 논문에서 우리는 이러한 통찰을 증류할 수 있는(==다른 방법인) architecture를 소개한다. 바로, 최대의 information을 전달하기 위한, 모든 layer가 연결된 구조의 network이다. feed-forward nature를 보존하기 위해, 각 layer는 모든 layer에서 input을 받고, 이후의 layer로의 input이 된다.

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210204-1.PNG)

  결정적으로, ResNet과의 차이로, 우리는 feature를 summation하는 것이 아니라, concatenate한다. 그러므로 $ l^th $ layer는 $l$ 개의 input을 가지게 되고, 이는 이전의 모든 layer의 feature map을 포함하는 것이다. 이러한 network는 $L$ 개의 layer가 있다면, $\frac{L(L+1)}{2}$ 의 connection을 나타내게 된다. 이렇게 dense한 connectivity pattern을 따라, 우리는 dense convolutional network인 denseNet을 만들게 되었다.

 이러한 방법은 쓸모없는 feature map을 필요로 하지 않기 때문에, traditional한 CNN 방법에 비해 더 적은 parameter를 가지게 된다. ResNet과 달리 우리는 concat을 하기에 architecture는 network에 추가된 정보와 유지된 정보 사이에서 differentiate하다. 이렇기에 마지막 classifier는 모든 feature map을 고려해서 결정을 할 수 있게 된다.

 더 좋은 parameter efficiency에 반해, dense net의 가장 큰 이점 하나는 정보의 flow와 network의 gradient를 개선시켰다는 점이다.(즉, 학습하기 쉽다) 각 layer는 loss function과 original input signal에서 오는 gradient에 직접적으로 접근할 수 있다. 이러한 점은 network에서 더욱 깊은 부분까지 훈련이 가능하다. 더욱이, 우리는 이러한 dense connection이 regularizing effect에도 효과적인 것을 알 수 있었다.(overfitting 방지 등)

 

### Related Work

 network를 더 깊게 만드는 직관적인 방법은 network의 크기를 늘리게 된다. GoogLeNet에서 inception module이나 Resnet in ResNet 과 같은 곳에서 처럼 말이다. 충분한 크기를 제공받은 network의 경우, 각 layer의 filter 수가 증가함에 따라 그 성능이 좋아졌다. 

 이렇게 깊고 넓은 architecture에서 오는 power에 반해, DenseNet은 쉽게 학습할 수 있고 높은 parameter efficient를 보여주는 condensed model을 얻을 수 있는, feature reuse를 통한 잠재적인 network를 exploit한다.

 서로 다른 layer에서 학습된 feature map을 concatenating하는 것은 일련의 layer의 input에서 오는 다양성을 증가시킬 뿐 아니라 효율도 개선하게 된다. 이것이 DenseNet과 ResNet의 가장 중요한 차이점이다. Inception network와 비교했을 때, densenet은 훨씬 간단하고 효율적이다.

 몇몇 효과적인 다른 network들도 있다. Network In Network(NIN), Deeply Supervised Network(DSN), Ladder Network, Deeply Fused Net(DFN)등이 있다.



### DenseNets

 간단한 image인 $x_0$ 가 CNN을 통과한다고 가정하자. network는 $L$ 개의 layer로 이루어져있고, 각 layer는 non-linear transformation인 $H_l(\cdot)$ 을 수행하고, $l$은 layer index를 말한다. $H_l(\cdot)$ 는 BN과 ReLU, pooling 혹은 Conv로 구성되어있다. 우리는 $l^{th}$ layer의 output을 $\mathbf{x}_l$ 이라고 하자.

__ResNets.__

 전통적인 convolutional feed-forward network는 $l^{th}$ layer의 output을 $(l+1)^{th}$ layer의 input으로 넣게 된다. 즉, 이러한 방법은 $\mathbf{x}_l = H_l(\mathbf{x}_{l-1})$ 로 표시될 수 있다. ResNet은 중간에 skip-connection을 넣어서 아래와 같은 identity function을 만들었다.
$$
\mathbf{x}_l = H_l(\mathbf{x}_{l-1})+\mathbf{x}_{l-1}
$$
  ResNet의 이점으로는 이후 layer의 이전 layer로 identity function을 통해 gradient가 직접적으로 흘러갈 수 있다는 것이다. 하지만, identity function과 $H_l$ 의 output이 summation으로 합쳐지는 것은 information의 flow를 지연시키게 된다.(???)

__Dense connectivity.__

 layer 사이에서 information flow를 개선시키기 위해 우리는 다른 connecitivity pattern을 제안한다. 우리는 어떤 임의의 layer에서 다른 모든 layer로의 직접적인 connection을 소개한다. 결과적으로 보면, $l^{th}$ layer는 그 이전의 모든 layer의 feature를 input으로 받는다. 즉,
$$
\mathbf{x}_l = H_l([\mathbf{x}_0, \mathbf{x}_1, ...,\mathbf{x}_{l-1}])
$$
 여기서 $[\mathbf{x}_0, \mathbf{x}_1, ...,\mathbf{x}_{l-1}]$는 0번째 layer에서 $l-1$ 번째 layer에서 나온 output인 feature map을 concat한 것을 말한다. implementation을 쉽게 하기 위해, 우리는 $H_l(\cdot)$ 의 multiple input을 한개의 tensor로 만들었다.

__Composite function.__

 우리는 아래 논문에 영감을 받아, $H_l(\cdot)$을 BN, ReLU, 3 by 3 convolution으로 구성했다.

 K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In ECCV, 2016

__Pooling layers.__

Equation 2에서 사용된 concatenation operation은 feature map의 size가 변할 때에는 불가능하다. 하지만, CNN에서 가장 중요한 부분은 feature map의 사이즈를 변화시키는 down-sampling이다. 우리의 architecture에서 down-sampling을 유연하게 하기 위해, 우리는 network를 아래 figure2 에서 확인할 수 있는 것 처럼 multiple densely connected dense blocks로 나눴다.

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210204-2.PNG)

 block 사이에는 convolution과 pooling을 진행하는 transition layer를 두었다. 

__Growth rate.__

 만약 각 function $H_l$이 $k$ feature map을 만든다고 하면, $l^{th}$ layer의 input feature map은 $k_0 + k \times (l-1)$ 이 된다.($k_0$는 input layer의 channel 수) DenseNet과 현재 존재하는 다른 network의 차이는, densenet은 매우 좁은 layer를 가질 수 있다는 것이다. 예를 들자면, $k=12$ 정도이다. 우리는 여기서 이 hyperparameter $k$를 growth rate라고 정했다. 우리는 section 4(experiment section)에서 작은 grow rate가 얼마나 효율적인지를 알려줄 것이다. 한가지 단순한 설명으로는, 이전의 layer에서 feature map을 모두 들고온다는 것은, network의 "collective knowledge"에 접근한다는 것을 의미한다.(그냥 이전의 feature를 싹 다 들고와서 concat을 때려버리니 그 내용이 잘 전달되는 것을 말하는 듯 하다) 각 layer는 growth rate인 $k$ feature map을 자신의 state에 더하게 되고, 이로 인해 growth rate는 global state에서 얼마나 많은 정보를 regulate하는 지를 의미하게 된다.

 여기서 global state라 함은 network 어디에서든지 접근이 가능하고, 다른 layer로의 복제가 필요 없음을 의미한다.

__Bottleneck Layers.__

 각 layer가 $k$ 개의 output을 결과로 내뱉는 것에 비해 훨씬 많은 input을 받게 된다. ResNet에서 사용된 것과 같이, 3 by 3 convolution전에 1 by 1 convolution은 bottleneck으로 소개가 되었다.(input feature map의 수를 줄이기 위해, 그리고 효율적인 연산을 위해) 우리는 이러한 디자인(bottleneck과 같은 design)이 DenseNet에 상당히 효율적인 것을 발견했다. 그리고 이를 적용한 DenseNet을 DenseNet-B로 했다.

__Compression.__

 model의 compactness를 개선시키기 위해, 우리는 전통적인 layer에서 feature map의 수를 줄일 수 있었다. 만약 dense block이 m개의 feature map을 가지고 있다면, 우리는 전통적인 layer는 $\lfloor\theta m\rfloor$의 output feature map을 가지게 된다는 것을 알 수 있었다.(여기서 $\theta$는 0 초과 1 이하). $\theta$가 1인 경우는, feature map의 수가 layer를 통과해도 변하지 않는다. 여기서 $\theta$가 1보다 작은 경우를 DenseNet-C로 하고, 실험에서는 $\theta$를 0.5로 두었다. 또한, 추가로 $\theta <1$인 경우와 bottleneck을 사용한 network를 DenseNet-BC로 설정했다.

__Implementation Detials.__

 ImageNet을 제외한 모든 dataset에서 DenseNet은 같은 수의 layer를 포함한 dense block을 사용했다. 

 자세한 사항은 아래 Table에 나와있다.

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210204-3.PNG)



### Experiments

__4.1. Datasets__

__CIFAR.__

__SVHN.__

__ImageNet.__

__4.2. Training__

__4.3 Classification Results on CIFAR and SVHN__

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210204-4.PNG)

__Accuracy.__

__Capacity.__

__parameter Efficiency.__

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210204-5.PNG)

__Overfitting.__

__4.4. Classification Results on ImageNet__

![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210204-6.PNG)



### Discussion

__Model compactness.__

__Implicit Deep Supervision.__

__Stochastic vs. deterministic connection.__

__Feature Reuse.__

![img7](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210204-7.PNG)

 위 figure 5는 experiment에서 실험한 3개의 dense block 내에서, 연결된 각 layer 사이에서 얼마나 활성화가 잘 되었는지를 의미한다.(각 layer로 연결된 weight의 평균값을 계산한 것. 즉, 얼마나 잘 연결되어있느냐, 버려지는 feature는 없느냐 등등)

 위 plot에서 얻을 수 있는 해석으로는,

- 같은 block 내에서는 모든 layer가 그들의 weight들을 많은 input에 흩뿌리고 있다는 점(이는 가장 처음에 뽑힌 feature map이라 하더라도 실제로는 가장 깊은 layer까지도 연결이 된다는 뜻)
- transition layer의 weight들도 그들의 weight를 이전의 dense block에 spread한다는 것(spread한다는 게, 대충 경향을 잘 따라간다 라는 의미인듯 하다.)
- 2번째와 3번째 dense block에서 top layer를 보면, transition layer의 output이 거의 무시되는 것을 볼 수 있는데, 이는 transition layer가 쓸데 없는 feature를 많이 내뱉는 것을 의미한다. 이는 DenseNet-BC가 strong result를 가질 수 있는 것을 도와주었다.(?? -> 좀 쉽게 말하면, input feature는 상대적으로 redundant한 feature로 해석)
- 마지막 classification layer가 전체 dense block을 통한 weight를 사용함에도 불구하고, final feature map에서는 high-level feature가 가장 큰 영향을 주는 것으로 보이나, 전반적으로 많은 layer의 feature를 사용하는 것을 확인할 수 있음.



### Conclusion



### Acknowledgements



### Reference

Huang, Gao, et al. "Densely connected convolutional networks." *Proceedings of the IEEE conference on computer vision and pattern recognition*. 2017.

 K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In ECCV, 2016.