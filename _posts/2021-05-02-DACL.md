---
layout : post
title : "DACL"
date : 2021-05-02 +0900
description : Contrastive loss를 이용하여 Domain adaptation을 수행한 DACL 논문의 간단한 리뷰입니다.
tag : [PaperReview]
---

### Learning a Domain-Agnostic Visual Representation for Autonomous Driving via Contrastive Loss



### Abstract

 semantic segmentation이나 depth estimation과 같은 자율 주행에 DNN이 응용이 되고 있다. 하지만 supervised 방식을 이용하는 것은 너무 많은 annotated label을 필요로 하고, time-consuming하다. 최근 연구들이 synthetic data를 이용하여 real world에서 조금 더 정확하게 depth를 뽑아내는 연구를 하고 있다. 이를 해결하기 위해, 우리는 Domain-Agnostic Contrastive learning을 제안한다. DACL은 training과 test dataset 사이의 adaptation에서 성능저하를 막기 위해 contrastive loss를 이용하여 해결하는 접근 방식을 제안한다.



### Introduction

 ConvNet 에서는 vision task에 사용할 수 있는 여러 일들이 이루어지고 있다. 하지만 이러한 일들은 불행하게도 real world에서의 dataset을 얻기 위해 막대하고 엄청난 노력의 data가 필요하다. 특히 자율 주행에서는 sensor에서 얻은 data가 불확실하기 때문에, 추가적인 process가 필요하다.(noise가 많아서 이 noise를 좀 줄이는게 필요하다는 뜻 같음. 근데 이건 잘 모르겠다. 현실의 noise도 어느정도 학습을 시켜주는 것도 낫지 않은가? 그게 아니면 아예 빼는게 나은건가 싶기도 하다.)

 아무튼 이러한 것들을 해결하기 위해 LiDAR 등 여러 것들을 fusion하는 움직임들도 있었다. 하지만 최근에는 synthetic dataset들이 되게 잘 뽑히면서, domain adaptation을 이용한 방법이 많이 나왔다.

 이러한 performance의 성능을 올리기 위해, 우리는 Domain-Agnostic Contrastive Learning인 DACL을 제안한다. MoCo와 GAN based style의 transfer algorithm을 이용해서 이를 해결한다.

 source와 target domain에서 오는 unpaired image를 이용하여 학습시키는 방법을 찾아본다. function의 output은 contrastive learning에 사용되는 query data로 사용이 된다. 이는 encoder를 통해 domain-agnostic representation을 익히게 되고, 이후 encoder만 따로 빼서 feature extractor로 사용하게 된다.

 이전의 domain adaptation들에 비해 depth estimation과 semantic segmentation을 더 잘 수행해냈다.

 즉, 우리의 contribution은 아래와 같다.

- 2-stage unsupervised domain adaptation을 제안한다.
- adversarial style transfer function과 함께 contrastive representation learning을 보여준다.
- DACL의 output이 previous domain adaptation들 보다 효과가 좋음을 보여준다.

 데이터셋으로는 vKITTI와 KITTI를 사용했다.

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210502-1.PNG)



### Related Work

__A. Style Transfer__

__B. Contrastive Learning__

__C. Task-specific Domain Adaptation__



### Method

 우리는 domain-agnostic representation을 학습하도록 2 stage unsupervised domain adaptation framework을 제안한다. 우리가 제안하는 방법은 bidirectional style transfer algorithm과 MoCo를 이용해서 semantic이나 structural information을 추출한다. 우리의 목표는 target domain $T$ 를 unsupervised 방식으로 학습시키는 것이다.

__A. Stage 1: Bidirectional Style Transfer__

 첫째로, 우리는 두 style transfer network인 $G_{S->T}$ 와 $G_{T->S}$ 를 훈련시킨다. 우리는 $G_{S->T}$ 를 adversarial min-max strategy를 통해 최적화시킨다. 

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210502-2.PNG)

 하지만 instability가 존재해서, 아래 식과 같이 결국 cycle-consistency loss를 채용하게 되었다.

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210502-3.PNG)

 하지만 이걸로도 부족해서 identity reconstruction loss도 썼다고 한다.

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210502-4.PNG)

 따라서, full-loss는 아래와 같다.

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210502-5.PNG)

__B. Stage 2: Contrastive Representation learning__

 우리는 서로 다른 domain 간 mutual information을 최대화 하기 위해서, contrastive loss를 사용하였다. 여기서 positive pair와 negative pair를 어떻게 사용했느냐가 중요하게 된다. 각 domain을 전환하는 network에 사용이 되는 feature extractor를 뽑아내서 각자의 feature extractor( $f_S, f_T$ ) 의 positive pair, negative pair를 어떻게 뽑아내는지 알아본다.

 우리는 style transfer network $G_{S->T}$ 를 stage 1에서 학습시키고, contrastive learning에 필요한 image를 만들어낸다. fake target인 $G_{S->T}(x_s)$ 는 queray image로 사용이 되고, 이것의 pair source인 $x_s$ 가 positive key image로 들어가게 된다. negative data는 positive pair의 같은 domain에 있는 sample들로, network가 domain에 상관없이 구분을 잘 할 수 있도록 하기 위해 source image $x_s$ 를 제외한 source domain $S$ 에서 오는 여러 image들을 negative key pair로 사용했다. ( 이거 왜 잘 되는지 모르겠군. )

 아무튼 이를 식으로 쓰면 다음과 같다. $\psi$  는 SimCLR 에서 쓰이던 MLP를 말한다.

![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210502-6.PNG)

 contrastive learning이 끝나게 된다면, $\psi$ 는 버려지게 되고, $f_T(\cdot)$ 만 downstream task에 사용이 된다.

 그리고 평범한 InfoNCE 를 통해 loss를 계산하게 된다.

![img7](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210502-7.PNG)

 위와 같은 방법을 통해  $f_T(\cdot)$  도 학습을 시킨다. 이렇게 학습이 된 feature extractor들을 downstream task에서 사용한다. 우리의 모델을 정리해서 그림을 그리면 아래와 같다.

![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210502-8.PNG)



### Experimental Results

__A. Dataset__

vKITTI와 KITTI dataset에서 수행했다.

__B. Network Architecture__

 위 network overview 참조.

__C. Monocular Depth Estimation__

![img9](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210502-9.PNG)

 성능표가 2개이다. training image에서는 근데 왜 했는지 모르겠다. 결국 eval을 봐야하지않나?

![img10](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210502-10.PNG)

__D. Semantic Segmentation__

![img11](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210502-11.PNG)

 사실 semantic은 잘 모르겠다. 이거 pro DA 쓰면 훨씬 올라가지 않을까? 만약에 없다면 여기는 그나마 펼쳐진 시장인 것 같기도 하고...



### Conclusions



### Reference

Shim, Dongseok, and H. Jin Kim. "Learning a Domain-Agnostic Visual Representation for Autonomous Driving via Contrastive Loss." *arXiv preprint arXiv:2103.05902* (2021).

