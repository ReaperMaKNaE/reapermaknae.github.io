---
layout : post
title : "CyCADA"
date : 2021-04-29 +0900
description : Cycle GAN을 응용하여 classification에 domain adaptation을 적용한 ADDA 논문의 간단한 리뷰입니다.
tag : [PaperReview]
---

### CyCADA: Cycle-Consistent Adversarial Domain Adaptation, PMLR 2018



### Abstract

 Domain adaptation이, 본 적이 없는 환경에서 새로운 성공을 이끌고 있다. 최근의 발전들로 인해 pixel-level과 low-level domain shift 등에서 성공적인 결과를 가져오고 있지만, 여전히 실패하는 경우가 존재한다. 우리는 generative image space alignment와 latent representation space alignment 둘 다를 사용한 domain 사이의 적용할 수 있는 model을 제안한다. 이를 우리는 CyCADA라 하고, domain들 사이를 연결하고 adaptation 전과 후에 생기게 되는 consistency error를 줄일 수 있도록 한다. 우리는 다양한 visual recognition과 에측에서 이를 테스트한다.(digit classification, semantic segmentation 등등)



### Introduction

 최근 feature level unsupervised domain adaptation이 source와 target 사이에서 feature를 align하는 problem을 해결했다. 이에 대한 다양한 방법들이 있는데, 문제는 이러한 방식들의 단점들이 있다. 첫번째로, 어떤 semantic consistency를 강요하지 않아서 target feature의 car가 source feature의 bicycle로 들어가는 경우가 있었다. 둘째로, higher level에서 alignment가 low-level appearance에는 실패하는 경우가 생겼다.

 그 외 여러 방법들이 있긴했는데, 여튼 우리는 Cycle-Consistent Adversarial Domain Adaptation인 CyCADA를 발표한다. 이는 pixel-level과 feature-level 둘 다에서 강한 semantic consistency를 제안한다. structural, semantic consistency를 cycle-consistency loss와 semantic loss를 이용해서 adaptation할 수 있도록 한다.

 이를 표로 정리하면 아래와 같다.

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210429-1.PNG)

 이를 여러 환경에서 실험해봤다. 이는 추후 실험 내용 참조.



### Cycle-Consistent Adversarial Domain Adaptation

 unsupervised adaptation의 문제를 한번 고려해보았다. 이 문제는, source data $X_s$ , source label $Y_s$ , target data $X_T$ 가 다 있지만 target label이 없는 상황이다. 목표는 target data $X_T$ 의 label을 정확하게 에측하도록 model $f_T$ 를 학습시키는 것이다.

__Pretrain Source Task Model.__

 우리는 간단한 source model $f_S$ 를 source data에서 실험할 수 있다. K-way classification에서 cross-entropy loss를 적용하면, 이는 아래와 같다.

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210429-2.PNG)

 위 식에서 $\sigma$ 는 softmax function을 의미한다. 하지만, 위 식에서 학습한 $f_S$ 의 경우, source domain에서만 잘 되고 domain shift가 생기게 되는 순간 학습 능력이 떨어지는 것을 발견했다.

__Pixel-level Adaptation__

 domain shift의 효과를 완화시키기 위해, 우리는 이전 adversarial adaptation 접근을 따르고 adversarial discriminator가 서로 다른 domain을 구분할 수 없도록 학습시켰다. 우리는 우리의 모델이 target data에 generalizing하면서 source data로 학습할 수 있도록 하였다.

 이를 끝내기 위해, 우리는 source에서 target으로 mapping을 하는 $G_{S->T}$ 를 소개하고, target sample을 생성해내는 adversarial discriminator $D_T$ 를 학습한다. adversarial discriminator는 source target data에서 real target data를 분류할 수 있도록 한다. 이에 상응하는 loss function은 아래와 같다.

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210429-3.PNG)

 위 식이 좀 이해가 힘들 수도 있는데, 일단 아래 그림을 보면 이해가 된다. $x_t$ 는 target image이고, $x_s$ 는 source image이다. 따라서, source image stylized as Target을 $D_T$ 를 통해서 GAN Loss를 얻는 것.

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210429-4.PNG)

 이 목적은 주어진 source sample에 대해 $G_{S->T}$ 가 확실하게 target sample에 적응할 수 있도록 만들어준다. 이러한 능력은, 이후 source label과 source prediction 사이의 task loss도 줄이는 것을 보여준다.

 하지만, 이러한 방법들에서도, $G_{S->T}(x_s)$ 가 original sample $x_s$ 의 structure와 content를 보존한다는 보장이 없다.

 이러한 문제를 해결하기 위해, 우리는 adaptation method에 cycle-consistency를 집어넣기로 하였다. 우리는 $G_{T->S}$ 인 target에서 source로 다시 변환하는 다른 mapping을 소개하고, 같은 GAN loss를 이용해서 훈련시킨다. 다른 말로, 우리는 $G_{T->S}(G_{S->T}(x_s)) = x_s$ 가 되는 것을 목표로 한다.(그 반대인 target에 대해서도 성립) 따라서, 이를 이용한 cycle-consistency loss는 아래와 같다.

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210429-5.PNG)

 추가로, 우리가 source labeled data에 접근할 수 있기 때문에, image translation 전과 후에 높은 semantic consistency를 장려할 수 있다. 이는 label flipping을 완화시켜주는 역할을 한다. fixed classifier $f$ 에서 온 predicted label을 정의해보자. (여기서 $f$ 는 $p(f, X) = arg max(f(X))$ 를 만족) 따라서, semantic consistency는 다음과 같다.

![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210429-6.PNG)

 이는 Figure 2에서 black portion에 해당한다. 

__Feature-level Adaptation__

 pixel-level method에서, source와 target image 사이의 변환을 하며 구분할 수 없도록 adversarial discriminator를 만드는 것을 해왔다. 우리는 두 이미지 사이에서 feature level method를 고려할 수도 있다는 점을 생각해보자. 이는 추가적인 feature level loss가 생긴다는 것이다.(Figure 2의 orange portion)

![img7](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210429-7.PNG)

 이들을 모두 합치면,

![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210429-8.PNG)

 이를 이제 target model에 적용시키기 위한 optimization은 아래와 같다.

![img9](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210429-9.PNG)

 이제 이를 한번 실제로 적용을 시켜보자.



### Experiments

 우리는 우리의 CyCADA를 MNIST, USPS, SVHN에서 실험을 했고, GTA와 CityScapes 사이에서 domain adaptation을 진행했다. (추가적인 결과로 SYNTHIA도 있음)

__3.1. Digit Adaptation__

![img10](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210429-10.PNG)

![img11](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210429-11.PNG)

여러 ablation study를 진행했는데, 일단 목록은 아래와 같다.

__Ablation: Pixel vs Feature Level Transfer__

__Ablation: No Semantic Consistency__

 아래 Figure 4의 (a) 참조

__Ablation: No Cycle Consistency__

 Figure 4의 (b) 참조

![img12](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210429-12.PNG)

__3.2. Semantic Segmentation Adaptation__

 다음으로 우리는 CyCADA를 semantic segmentation에서 평가했다. 그리고 3가지 metric에 대해서 performance를 평가했다. (mIoU, fwIoU, pixel acc.)

__3.2.1. Synthetic to real adaptation__

![img13](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210429-13.PNG)

![img14](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210429-14.PNG)

![img15](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210429-15.PNG)



### Related Work

이게 왜 여기있지? 아무튼 생략.



### Conclusion



### Reference

Hoffman, Judy, et al. "Cycada: Cycle-consistent adversarial domain adaptation." *International conference on machine learning*. PMLR, 2018.

