---
layout : post
title : "RCNN"
date : 2020-12-07 +0900
description : Review
tag : [PaperReview]
---

### Rich feature hierarchies for accurate object detection and semantic segmentation



 이번에 볼 논문은 R-CNN으로 알려진 논문. 근데 논문 리뷰는 아니고, 대충 RCNN이 어떤 구조로 이루어져있는지 기록해두려고 했다. __태그는 논문리뷰지만 논문리뷰아님! 블로그 관리자의 개인 공부용도!__

 Computer Vision에서 논문을 읽으려니 기반부터 되는 것들을 제대로 이해하고 넘어가려고 처음부터 시작해봤다.



 일단 간단한 요약을 먼저 하면,



 input image에서 2천개의 물체가 있을 법한 공간(proposal)을 찾아내고, 2천개의 물체가 있을 법한 영역들을 CNN에 집어넣기 위해 각각 227x227 사이즈로 만든 다음, 마지막에 classification(여기서는 SVM을 이용)하여 물체를 detection하고, box로 라벨링하는 것이다.



 들어가기 앞서, 일단 object detection에서 쓰이는 몇 용어들을 알아보면,

SIFT - 특징점 검출. image matching에서 쓴 적 있음.(같은 매칭으로 SURF도 쓰임)

HOG : 보행자나 사람의 형태를 검출(SIFT와 HOG 둘 다 gradient를 이용함.)

Sliding Window : 다양한 scale의 window를 좌측 상단에서 우측 하단까지 슬라이딩하며 detect

Selective Search(SS) : 영역 탐색 및 그룹화 이후 객체의 위치를 proposal 해주는 것.

NMS(Non-Maximum Suppression) : 현재 pixel(혹은 bbox)를 주변의 pixel(혹은 bbox)와 비교했을 때, 살릴지 말지 결정하는 것.(최대가 아니면 없애는 것 -> 한개의 물체에 여러개의 bbox 방지)

bbox : Bounding Box(이미지 검출할 때 사각형 쳐지는 것)

image warp : 이미지 변형(사전적인 뜻은 휘게만들다인데, 이미지 변형은 이렇게 통틀어 말하는 듯하다)

FT : Fine - Tune

FG : Foreground(아마 용어를 쓰는 곳 마다 조금씩 다를 거긴 한데, 여기서는 foreground를 의미)

UVA model : 같은 algorithm을 적용한 object detection 방식 중 하나인 것 같은데, 찾아도 안나온다. 흠..

 조금 더 세밀한 내용은 아래에서 추가적으로 다룰 예정.



RCNN의 동작 방식에 대한 간단한 소개는 아래와 같다.



1. __처음 2000(2k)개의 proposal을 받는 것은 SS(Selective Search) method를 이용__

2. __이후 각 proposal들을 227 x 227 x 3 의 사이즈로 변형해서 CNN으로 넘겨주고, 4096개의 feature vector를 구함.__
3. __각 feature vector들을 이용해서 classification진행은 SVM을 사용하였음__

4. __이후 해당 Region이 무엇인지 나타낼 때에는 IoU를 0.5로 설정하고, 이를 넘어가는 bbox 중 가장 점수가 큰 bbox만 골라내는 NMS 방식 이용__



 다른건 음.. 그럴 듯 하군. 인데, 조금 의문이 드는 것은 왜 하필 4096개의 feature vector가 나오느냐 였다. 이에 대한 답은 Krizhevsky et al. 이 쓴 ImageNet Classification with deep convolutional neural networks에 있다. (== AlexNet)



 그런데 위 논문을 찾아보면 대표적으로 나오는 그림이 아래와 같다.

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20201207-1.png)

 그림을 보면 input이 224x224x3이라고 나와있으나, 실제로는 input이 227x227x3이여야 아다리가 맞다. ~~*그래서 실제로 AlexNet을 만든 사람들이 굴릴 때엔 아마도 padding을 해놓지 않았을까 싶다. padding이 없다고도 하는데, 이건 여러 글을 더 읽어봐야겠다. AlexNet을 쓴 사람의 글이 필요한데 나와있는게 뭐 있어야 알아보던가 말던가*~~ 227인 이유는, stride 4씩 움직여서 2번째 layer인 55x55x48로 만드려면, 55x55의 경우 stride 4를 곱하면 220, 여기에 커널 사이즈가 11x11이므로 stride 4를 뺀 7을 더하면 227이다.

 그리고 96개의 kernel을 이용하기 때문에, depth는 96이 된다.(96개의 서로 다른, 하지만 크기는 11x11x3으로 같은 kernel들을 이용했기 때문에 depth는 96)

 이 96을 2개의 GPU로 나눠서 계산했기 때문에 위 그림에서는 55x55x48이 위아래 두개로 첫번째 convolutional layer에 있다.

  이 다음 kernel은 5x5x48의 크기로, 256개의 서로 다른 kernel을 이용해서 convolutional layer를 다시 만든다. 그렇게되면 사이즈는 27x27x256이 되고, 이를 다시 GPU 2개로 나눠서 계산했기에 위 그림처럼 27x27x128이 위아래로 하나씩 존재한다.

 이를 위처럼 쭉쭉 반복하면, 결과는 4096개의 feature vector가 나오고, 이를 통해서 AlexNet은 1000개의 image를 분석했다.



 결국 정리를 하면,

1. __처음 2000(2k)개의 proposal을 받는 것은 SS(Selective Search) method를 이용__

- SS를 어떻게 하는데요? : Segmentation 방법 중 하나로, 다양한 방법을 통해 segmentation에 쓸 seed(어떤 물체의 특징을 나타내는 적은 수의 pixel들)를 구하고, 주변의 seed를 관찰해서 '아, 이 seed와 요 seed는 같은 물체의 seed이다' 임을 이용해 점점 그 크기를 키워나가는 region growing 방식 등을 이용해서 segmentation을 할 수 있도록 결정한다.

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20201207-2.png)

 (a)나 (b)를 보면, 맨 왼쪽 위 그림처럼 segmentation을 진행을 한 후 seed를 설정하면, 그 아래 그림처럼 정말 많은 후보들(파란색 사각형)이 만들어 진다. 이 때 어? 파란색 사각형이 너무 많네? 하면 적당한 seed들 끼리 합쳐서 다시 segmentation을 진행하고, 다시 후보들을 만들면서 과정을 반복한다. 그렇게 되면 어느정도 segmentation이 되게 된다고 한다.(이 부분은 추후 공부를 더 할 계획)

2. __이후 각 proposal들을 227 x 227 x 3 의 사이즈로 변형해서 CNN으로 넘겨주고, 4096개의 feature vector를 구함.__

1단계 : input은 227x227x3, kernel은 96개, kernel size는 11x11x3, stride 4, 결과물 55x55x96

2단계 : input은 55x55x48(원래는 55x55x96인데, GPU 2개로 나눠서 계산하기 때문에 반토막남), kernel은 256개, kernel size는 5x5x48(GPU로 두개를 나눠서 계산 하기 때문에 depth는 반틈이 됨), stride는 2, 결과물 27x27x256

3단계 : input은 27x27x128(이유는 위와 마찬가지), kernel 384개, kernel size 3x3x128, stride는 2. 결과물 13x13x384

4단계 : input은 13x13x192, kernel수 384개, kernel size 3x3x128, stride 1, zero padding 1. 따라서 output은 intput에다가 depth만 두배.

5단계 : kernel의 depth(kernel 수)만 256으로 줄이고, 4단계와 같다. output은 13x13x128.



그 다음 fully connected layer가 문젠데, 일단 아래 블로그를 참조했다.

[https://bskyvision.com/421](https://bskyvision.com/421)

일단 확인한 바에 따르면, 13x13x256의 output에 (5단계 output에서 2개를 합치면 256이 됨) 3x3 kernel로 stride2를 하면 6x6x256이 되는데, 이는 9216. 이를 4096개의 뉴런과 fully connected 함.

이후 7번째 layer, 8번째 layer로도 다 fully connected 하는데, 7번째는 마찬가지로 4096개의 뉴런이고, 8번째는 1000개의 뉴런이며, 7->8로 가면서 softmax를 적용한다.

 왜 하필 4096인지는 또 안나오긴한다. 개수는 사실 자기 마음대로 쓰기 나름인데, 그냥 AlexNet 설계자가 4096이란 숫자를 좋아했나 보다. 아무튼 또 이를 구현한 블로그는 다음과 같다.

[https://seongkyun.github.io/study/2019/01/25/num_of_parameters/](https://seongkyun.github.io/study/2019/01/25/num_of_parameters/)

간단하게 Net들을 소개한 유튜브 영상도 있다.

[https://youtu.be/8mI9zRdx2Es](https://youtu.be/8mI9zRdx2Es)

3. __각 feature vector들을 이용해서 classification진행은 SVM을 사용하였음__

 SVM은 Support Vector Machine으로, Classifier중 하나이다. 여기에서 사용한 이유는, AlexNet의 경우 Classifier를 만들 때 4096개의 neuron에서 1000개의 class를 구하는 방식을 선택했었는데, 대회에 나갈 땐 천개까지 쓸 필요가 없다보니, 마지막 1000개의 class를 대회에서 요구하는 class의 개수만큼 바꾸면서 SVM으로 바꿨다. 즉, 위의 8번째 layer를 없애고, 4096개의 neuron data(feature vector)를 SVM에 넘겨주면서 classification을 진행하라고 한 것.

 SVM을 간단하게 말하면, 분류가 가능할 때 까지 차원을 계속 올리는 것과 비슷하다. 자세한 내용은 엄청 수학적이라 일단 pass. 추가적인 computer vision논문을 계속 읽다가 SVM을 이용한 사례가 더 나오면 한번 공부를 제대로 해볼 계획.



4. __이후 해당 Region이 무엇인지 나타낼 때에는 IoU를 0.5로 설정하고, 이를 넘어가는 bbox 중 가장 점수가 큰 bbox만 골라내는 NMS 방식 이용__

  위에서 모두 설명을 끝냈기에 Pass.



 Code를 좀 보면서 제대로 이해를 해 보고 싶으나, 아쉽게도 요새 나오는 Detector들은 최소 ResNet은 되어야 쓰는 듯 하다. 그런의미에서 Pass.



### Reference

Girshick, Ross, et al. "Rich feature hierarchies for accurate object detection and semantic segmentation." *Proceedings of the IEEE conference on computer vision and pattern recognition*. 2014.

Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. "Imagenet classification with deep convolutional neural networks." *Communications of the ACM* 60.6 (2017): 84-90.

Uijlings, Jasper RR, et al. "Selective search for object recognition." *International journal of computer vision* 104.2 (2013): 154-171.

Idea Factory Kaist, Advanced Architectures of CNN - 딥러닝 홀로서기, [https://youtu.be/8mI9zRdx2Es](https://youtu.be/8mI9zRdx2Es)

TimeTraveler님의 RCNN 리뷰, [https://89douner.tistory.com/88](https://89douner.tistory.com/88)

리온 피플님의 AlexNet 리뷰, [http://blog.naver.com/PostView.nhn?blogId=laonple&logNo=220654387455](http://blog.naver.com/PostView.nhn?blogId=laonple&logNo=220654387455)

bskyvision님의 AlexNet의 구조, [https://bskyvision.com/421](https://bskyvision.com/421)

성균님의 블로그, CNN의 parameter 개수과 tensor 사이즈 계산하기, [https://seongkyun.github.io/study/2019/01/25/num_of_parameters/](https://seongkyun.github.io/study/2019/01/25/num_of_parameters/)

pre training, fine tuning, [https://eehoeskrap.tistory.com/186](https://eehoeskrap.tistory.com/186)

한글판 RCNN 번역, [http://blog.daum.net/sotongman/6](http://blog.daum.net/sotongman/6)

Sliding window, selective search, [https://hoya012.github.io/blog/Tutorials-of-Object-Detection-Using-Deep-Learning-what-is-object-detection/](https://hoya012.github.io/blog/Tutorials-of-Object-Detection-Using-Deep-Learning-what-is-object-detection/)

selective search, [https://m.blog.naver.com/laonple/220918802749](https://m.blog.naver.com/laonple/220918802749)

NMS, [https://dyndy.tistory.com/275](https://dyndy.tistory.com/275)

YOLO(인데 NMS 보충 설명에 효과적), [https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&loop=false&delayms=3000&slide=id.g137784ab86_4_3279](https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&loop=false&delayms=3000&slide=id.g137784ab86_4_3279)