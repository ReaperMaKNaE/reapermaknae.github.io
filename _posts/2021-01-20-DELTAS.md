---
layout : post
title : "DELTAS"
date : 2021-01-20 +0300
description : DELTAS, Depth Estimation by Learning Triangulation and Densification of Sparse Points 논문의 간단한 리뷰입니다.
tag : [PaperReview]
---

### DELTAS: Depth Estimation by Learning Triangulation and Densification of Sparse Points, 2020



### Abstract

 MVS는 active depth sensing의 정확도와 monocular depth estimation의 현실적인 측면 사이에 놓여있다. Cost volume base의 network들은 MVS system에서 accuracy를 향상시켰다. 하지만 이러한 accuracy는 높은 연산량을 요구했다. cost volume의 방법에서 벗어나, 우리는 효율적인 depth estimation 방법을 제안한다.

- interest point를 위한 descriptor들의 detect와 evaluate
- interest point들의 작은 set들을 match하고 triangulate할 수 있는 학습
- CNN을 이용한, sparse 3D point set들을 densify하는 것

 end-to-end network가 효율적으로 이 3가지 단계를 수행했고, 2D image와 3D geometric 사이에서 학습을 진행했다. 결정적으로는 우리의 first step인, interest point detection과 descriptor learning을 이용한 pose estimation이다. 우리는 이러한 것들을 이용하여 SOTA를 찍었다.



### Motivation

 몇몇 sensor들이 SfM이나 MVS에 많이 사용되고 있고, 이러한 것들은 AR/VR 등 다양한 곳에서 사용이 된다. 하지만 이러한 센서들의 문제가 있어서 새로운 센서들을 개발하는 것이 추구되고 있다.

 여러 방면에서 depth를 측정하는 방법이 있는데, camera의 image에서 부터 cost volume을 사용하는 방법 등이 자주 사용되어 왔다. MVS를 이용한 정확한 depth map은 memory consumption이 심하고 방대한 양의 연산에 문제를 두고 있다.

 최근에는 sparse-to-dense 방법을 이용해서 depth를 destimation하는 것이 좋은 방법으로 떠오르고 있다. 하지만 이러한 depth의 경우에는 한계가 존재하는 active sensor를 이용하게 된다. 하지만 이러한 방법도 어떤 것들은 무시하게 된다.

 우리는 end-to-end 방법으로 sparse-to-dense 방법을 3D landmark를 이용해서 학습하는 것을 제안한다.

- MVS 기술에서 cost volume을 없애 연산을 줄임
- sparse VIO나 SLAM을 이용한 camera pose estimation(interest point와 descriptor 이용)
- 설명가능하고 알고리즘을 guide하기 위한 geometry base의 MVS
- sparse-to-dense에서 오는 정확도와 효율에서 오는 benefit

 우리는 multi-task model이다. RGB image, sparse depth image의 encoder가 2개 있고, point detection, descriptor, dense depth prediction을 위한 decoder가 3개 존재한다. 우리는 interest point decoder, descriptor decoder, sparse depth encoder사이에서 중요한 연결을 하고 geometric 관계를 이용해 효율적인 triangulate module을 만들어 end-to-end training이 가능하게 했다.



### Related Work

__Interest Point Detection and Description__

 SIFT, ORB, SuperPoint, LIFT, GIFT

__MVS__

 DeepMVS, MVDepthNet, DPSNet, GP-MVSNet

__Sparse to Dense Depth Prediction__

 3D point의 sparse recon을 위해 triangulation module을 사용한다.



### Method

 ![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210120-1.PNG)

 우리는 3장의 image를 받는다. 전형적인 target image와 2 view-point이다. 첫번째 step에서는, encoder를 통해서 두 reference image에서는 descriptor를 뽑고, target image에서는 descriptor와 interest point의 위치도 뽑게 된다. 2번째 step에서는 받아온 descriptor들과 interest point를 이용하여, SVD를 이용해 triangulation을 진행한다. 그 output은 sparse depth image를 만드는 것에 쓰인다. 마지막 단계에서는, sparse depth encoder의 output + image feature를 이용한 depth decoder를 이용해서 dense depth를 뽑아내게 된다.

__3.1 Interest Point Detector and Descriptor__

 우리의 방식은 SuperPoint와 유사하다. encoder를 이용해 feature map을 뽑고, 2 가지의 서로 다른 task를 수행하는 head로 보내지게 된다. head 하나는 interest point를 찾고, 다른 head 하나는 descriptor를 찾는다.

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210120-2.PNG)

 위 module에서, SuperPoint에서 사용했던 shallow backbone을 ResNet-50 encoder로 변경해서 효율과 performance를 둘 다 높였다. fine과 coarse level image information을 융합하기 위해, 우리는 U-Net과 유사한 architecture를 descriptor decoder에 적용했다. SuperPoint와 유사하게, N-dimensional descriptor tensor는 원본 이미지의 1/8 사이즈에 해당한다. 이 network를 학습시키는 방법으로는 SuperPoint network에서 사용한 output을 어느정도 골라내서 사용했다.

__3.2 Point Matching and Triangulation__

 전통적으로 뽑아낸 descriptor와 interest point들의 관계를 이용해서 match를 하는 것은 연산량이 셀 수 없을 정도로 크다. 그러므로 우리는 search space를 제한하고 efficiency의 효율을 증가시키기 위해 geometrical constraint을 걸었다. 우리는 multi-view geometry의 개념을 응용하여, epipolar line에 있는 영역들만 search했다. epipolar line은 fundamental matrix인 F로 이루어지게 된다.

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210120-3.PNG)

하지만, 실제 상황에서는 Figure 3의 왼쪽처럼 딱 들어 맞는 경우가 잘 없다. 그래서 우리는 small fixed offset을 두었다. epipolar line은 depth value를 무한까지 확장시킬 수 있게 된다. 우리는 현실적으로 가능한 depth sensing range를 고려해서 이 범위를 clamp했다.(Figure 3의 우측 그림) 우리는 descriptor field에서 descriptor를 얻기 위해 bilinear sampling을 사용했다. 각 interest point의 descriptor는 각 image view point마다 epipolar line에 놓여져 묶이게 된다.

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210120-4.PNG)

위에서 D hat은 anchor image(target image)의 descriptor field이고, D ^ k 는 k th 보조 image의 descriptor field이다. epipolar line인 \epsilon에 clamp된 depth value는 x이다. 이러한 것은 효율적으로 descriptor field와 interest point descriptor 사이에서 cross-correlation map을 제공한다. 이 map에서 높은 value를 가진다는 것은, 보조 image의 key-point match가 anchor image로의 interest point로 잘 된다는 것을 말한다.

 여기서 우리는 3D point의 좌표를 얻기 위해, algebric triangulation을 따랐다. 우리는 서로 다른 interest point j 들에 대해서 이러한 과정을 실행했다. 각 cross correlation map의 peak value에서 2D position을 얻기 위해 2D interest point를 triangulating 하는 접근에서 시작했다. 2D position을 estimate하기 위해, 우리는 첫번째로 spatial axe 상에서 softmax를 취했다.

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210120-5.PNG)

 위 식에서 C _ j,k는 j번째 interest point와 k 번째 view에서의 cross-correlation map을 나타내고, W와 H는 spatial dimension을 의미한다. 그다음 우리는 cross correlation map에 상응하는 무게중심으로 2D position을 계산했다. (? 이걸 왜한거지?)

![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210120-6.PNG)

위 식에서는, matched point x _ j,k의 2D point와 cross-correlation map C _ j,k 사이의 differentiable한 것을 가능하게 한다. 우리는 2D matched point x _ j,k에서 3D point를 estimate하는 것으로 linear algebraic triangulation approach를 사용했다. 이 방법은 point z _ j의 3D coordinate을 찾는 것을 줄여준다.(이걸 줄여서 너무 굳어버린 system을 해결한다고 하는데 흠...)

![img7](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210120-7.PNG)

 위 식에서 A는 full projection matrix와 x _ j,k에서 온 component들로 구성이 되어 있다. 그런데 문제는 다른 view point에서 봤을 때에는 이러한 triangulation이 occlusion이나 motion artifact(blur같은 효과들)때문에 제대로 적용이 안 될 수도 있다. 그래서 우리는 weight를 더하게 된다.

![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210120-8.PNG)

 위 식에서 w는 각 cross-correlation map에서 max value를 가지게 된다. 이는 high confidence와 low confidence에 따라 값이 조절이 될 수 있도록 한다. 위 식에서 SVD를 적용시키면, B = UDV ^ T가 되고, z bar가 V의 마지막 column에 해당한다.

(위 내용은 전반적으로 다른 논문을 봐야 이해가 될 것 같다. 아래 논문을 참조해보자.

Iskakov, K., Burkov, E., Lempitsky, V., Malkov, Y.: Learnable triangulation of human pose. In: Proceedings of the IEEE International Conference on Computer Vision, pp. 7718–7727 (2019))

__3.3 Densification of Sparse Depth Points__

![img9](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210120-9.PNG)

 interest point detector는 image에서 2D position을 담당하고, z coordinate은 depth를 제공한다. 우리는 sparse depth image를 depth에 input으로 조금씩 주었다. 이러한 방법에서도 gradient가 propagate 될 수 있다는 것을 상기해보면, 위 과정은 SegNet에서 switch unpooling과 유사하다.

 전체적으로 우리는 U-Net style의 decoder를 사용하였다. 우리는 deep supervision을 4가지 단계로 제공한다. 서로 다른 receptive field size에서 feature를 섞기 위해 SPP block을 포함했다.

__3.4 Overall Training Objective__

 전체적인 network에서 얻게 되는 loss는 다음과 같다.

- Superpoint에서 얻어진 것을 GT로 두고, interest point들간의 cross entropy loss
- 2D point output과 GT 2D point output 사이의 smooth l1 loss
- SVD triangulation에서 나온 3D point와 GT 3D point의 smooth l1 loss
- dense depth map의 edge aware smoothness loss
- gt 3D depth map과 predicted dense depth map 사이의 multi scale smooth l1 loss

![img10](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210120-10.PNG)



### Experimental Results

__4.1 Implementation Details__

__Training__

DeMoN Dataset, ScanNet

__Evaluation__

 DPSNet, MVDepthNet, GPMVSNet 등과의 차이

__4.2 Detector and Descriptor Quality__

![img11](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210120-11.PNG)

__4.3 Depth Results__

![img12](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210120-12.PNG)

![img13](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210120-13.PNG)

 위 상황은 input을 여러장 줬을 때.

![img14](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210120-14.PNG)



### Conclusion



### References

Rabinovich, Andrew. "DELTAS: Depth Estimation by Learning Triangulation and Densification of Sparse Points."

Iskakov, K., Burkov, E., Lempitsky, V., Malkov, Y.: Learnable triangulation of human pose. In: Proceedings of the IEEE International Conference on Computer Vision, pp. 7718–7727 (2019)

