---
layout : post
title : "DVR"
date : 2021-04-02 +1500
description : Implicit한 방법으로 3D representation을 성공한 DVR 논문의 간단한 리뷰입니다.
tag : [PaperReview]
---

### Differentiable Volumetric Rendering: Learning Implicit 3D Representations without 3D Supervision, CVPR 2020



### Abstract

 이전의 방법들은 3D를 만드는 것에 있어 voxel 혹은 mesh based representation을 선택하게 되었다. 하지만 이러한 방식들은 결국 discretization과 low resolution에 고통을 겪어야 했다. 그래서 우리는, implicit한 방법으로 shape과 texture를 representation하는 방법을 제안한다.

 우리의 key는 depth gradient가 implicit differentiation의 개념을 사용해서 분석하면 얻을 수 있다는 것이다. 그리고 우리는 이러한 방법이 single view reconstruction 뿐 아니라 multiple view reconstruction에서도 성과를 보이고 있다는 것을 보여준다.



### Introduction

 대부분의 3D Reconstruction의 경우, synthetic data를 바탕으로 학습이 이루어지거나, full 3D representation을 기반으로 학습이 되어 왔다. 하지만 이러한 문제는, 역시 memory의 문제를 가져오게 된다.

__Contribution__

 이번 work에서 우리는 __Differentiable Volumetric Rendering(DVR)__ 을 소개한다. 여기서 우리는 implicit shape과 texture representation에 대한 정보를 담고 있는 network parameter에 대해서 predicted depth map의 gradient 분석을 할 수 있다는 것을 보여준다.

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210402-14.PNG)

 조금 추가적인 설명을 하면, network parameter가 differentiable 하게 volume에 어떻게 적용되는지를 연구한 논문이다. 방법이 좀 재밌긴 하다.



### Related Work

__3D Representations__

 voxel 등의 여러 3D representation의 방법들이 있는데, 상대적으로 작은 resolution에 국한되어 있다. 이러한 문제를 해결하기 위해 3D geometry와 texture implicitly를 설명해서 decision boundary와 binary classifier등으로 memory efficient한 3D representation을 많은 사람들이 연구하고 있다.

 우리가 이번에 한 work의 경우에는, 3D representation GT 없이, 2D supervision만으로 implicit하게 3D representation을 만들어낸 것에 의미가 있다.

__3D Reconstruction__

 classical한 방법 말고 최근 learning-based 로 3D recon을 하는 방법으로는, image feature matching, depth map fusion and refinement, classical MVS pipeline의 optimize 등이 있다. 하지만 이러한 방법들과 달리, 우리는 2D image를 supervise로 넣어서 consistent 3D representation을 output으로 내뱉게 된다.

__Differentiable Rendering__

 OpenDR등의 여러 implicit한 representation들이 있었다. 하지만 다른 방법들과는 다르게, 우리는 texture information을 visual hull과 concave shape을 극복하기 위해 사용했다.



### Method

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210402-15.PNG)

  network의 전체적인 overview는 위와 같다.

  그럼 위 그림에서 나오는 과정들을 자세히 살펴보자.

__3.1. Shape and Texture Representation__

__Shape:__

 implicit한 방법으로 만들 수 있는 occupancy network은 다음과 같다

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210402-16.PNG)

 위 식에서 $R^3$ 은 3D 공간상에서의 representation을 말하고, Z의 경우는 single-view reconstruction network을 지나고 나온 일종의 feature라고 볼 수 있다.

__Texture:__

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210402-17.PNG)

 texture의 경우 말 그대로 질감이라고 보면 된다. 위 식과의 차이로는, shape은 '공간에 물체의 surface가 있느냐'를 말하는 것이라면, texture는 '질감이 어떤가'를 말하는 것이다. 즉, sharp하냐 smooth 하냐 등, 우리가 봤을 때 '아 품질이 좋다'하는 것을 결정하게 되는 부분이라고 보면 된다.

__Supervision__

 3D supervision으로 학습하면 물론 좋긴한데, 이건 너무 expensive하다. 그래서 우리는 2D supervision으로도 3D representation을 할 수 있는 DVR을 소개한다.

__3.2. Differentiable Volumetric Rendering__

 우리의 목적은 $f_{\theta}$ 와 $t_{\theta}$ 를 학습하는 것이다. single image reconstruction을 고려해보자. 우리는 photometric reconstruction loss를 아래와 같이 두었다.

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210402-18.PNG)

 위 식에서 u는 pixel을 의미한다. 위와 같은 reconstruction loss를 network parameter $\theta$에 대해 줄이기 위해서, 저자는 다음 두 가지를 해야만 했다고 한다.

- $f_{\theta}$ 와 $t_{\theta}$ 가 주어졌을 때 $\hat{I}$를 render하는 것
- network parameter $\theta$에 대해 Loss의 gradient를 계산하는 것

 물론 이 두 가지를 아래와 같은 방법으로 다 해결했다고 한다.

__Rendering__

![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210402-19.PNG)

 저자가 하고 싶은 것은 어떤 image상의 point(u)가 어떤 depth(d)를 가지는지 아는 것이다. 위 그림에서 $r_0$는 camera의 위치이다.

 3D 공간 상에서의 점의 위치를 p라고 하였을 때, surface를 이루는 함수를 $f_{\theta}=\tau$ 를 만족하는 점 p를 $\hat{p}$ 라고 두면, 위 그림이 어느정도 보이게 된다. 여기서 f와 t의 차이는 위 그림에서 해당 공간에 점이 있느냐 없느냐를 f가 결정을 하는 것이고, 이러한 f들 중에서 어느 surface에 위치하는게 가장 아다리가 맞느냐를 texture함수인 t가 결정하는 것이라고 보면 된다. 그럼 이걸 어떻게 계산했는지를 보자.

__Gradients:__

![img7](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210402-20.PNG)

 Loss L을 network parameter $\theta$로 미분한 결과를 위와 같이 chain rule로 쪼갤 수 있다. 여기서 우측 부분만을 살펴보면, Jacobian matrix라고 볼 수 있다. $I_u = t_{\theta}(p)$ 로 둔다면, 우리는 결과적으로 아래와 같은 식으로 나타낼 수 있다.

![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210402-21.PNG)

 원래 편미분이란게 위와 같다. 아무튼, 위 식을 정리하기 위해 다음과 같은 생각을 할 수 있다.

 어떤 임의의 pixel u에 대해서, pixel u와 camera location r을 통과하는 ray가 있다고 하면, 이 ray의 식은 $r(d) = r_0 + dw$라고 둘 수 있고, 여기서 w는 $r_0$와 u를 잇는 vector connecting이라고 보면 된다. 여기서 $\hat{p}$는 무조건 ray 위에 있기 때문에, depth value $\hat{d}$의 경우는 $\hat{p}=r(\hat{d})$를 만족하게 된다. 여기서 $\hat{d}$를 surface depth라고 하면, 우리는 아래와 같은 식을 만들 수 있다.

![img9](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210402-22.PNG)

 이렇게 위 식을 조합해보면, 아래와 같다.

![img10](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210402-23.PNG)

 여기서 위 식을 정리해보면,

![img11](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210402-24.PNG)

 위와 같이 된다. 즉, network parameter와 surface point에 대해서 network의 결과물을 표현할 수 있고, 이 결과물을 조합해서 network parameter에 대한 surface depth를 찾을 수 있게 된다. 이를 이용한다면 2D supervised로도 3D representation을 구할 수 있게 된다.(surface를 알기 때문에)

__3.3 implementation__

 그래서 이걸 어떻게 학습을 시킬꺼냐 라는 것이 문제다. network parameter에서 surface depth를 결국은 구해내야 하기 때문이다. 그래서 저자는 아래와 같은 과정을 거쳤다고 한다.

__Forward Pass__

 occupancy network f는 ray에서 n개의 동일한 spaced sample을 평가한다.(어떤 surface에 point가 위치할 지를 결정하는 것) 이를 step을 이용해서, world-coordinate에서 point의 위치를 알 수 있다고 한다.

![img12](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210402-25.PNG)

 위 식에서 $s_0$는 surface point가 위치할 수 있는 surface 중에서 가장 높은 확률을 가진 surface를 의미하고, 이 surface를 기준으로 앞/뒤로 몇개의 surface를 몇개 더 그리게 된다.(확률이기 때문에 확률이 가장 높다고 100%는 아니라서, surface를 몇개 더 그리는듯)

 그리고 여기서 100%에 가깝게 만들기 위해, surface point가 존재하는 surface를 $\triangle s$로 찾는데, 이 수치를 j로 결정하게 된다. 이 때 j는 다음과 같은 process를 거치게 된다.

![img13](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210402-26.PNG)

 ray를 딱 쐈고, 어떤 surface에서 point가 딱 찾아질만한 곳을 object 밖의 빈 공간을 의미하는 free space와 object 안의 occupied space 사이에서 가장 surface에 적합한 곳을 만족하는 j를 찾게 된다. 그냥 TSDF에서 surface가 0으로 나오는데, 이 구간을 찾도록 학습시키는 것이라고 보면 될 듯.

__Backward pass__

 gradient를 back-propagation 어떻게 할 것이냐. 를 또 생각해야한다. 그 과정은 아래와 같다. 참고로, Loss의 gradient는 $\frac{L}{\theta}=\lambda \frac{\hat{d}}{\theta}$ 로 뒀을 때 이다. 이를 다시 써서 정리하면 아래와 같다.

![img14](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210402-27.PNG)

 위 식에서 left term은 normal backward operation에 상응하는 것을 찾았을 때를 말하고, 우측 trem은 단순한 scalar multiplication(batch 내에서)를 말한다. 이러한 방법을 통해서 back-propagation이 된다고 한다.

__3.4 Training__

 학습을 할 때에는 무려 5개의 Loss를 사용한다.

__RGB Loss__

![img15](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210402-28.PNG)

 RGB Loss의 경우는 색을 말하는 것으로, l1 loss를 썼다고 한다.

__Depth Loss__

![img16](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210402-29.PNG)

 depth의 경우 image의 pixel에서 해당하는 depth가 맞는지를 l1 loss로 계산하게 된다.

__Freespace Loss__

 추가 loss 중에서 Free-space loss이다. 이게 뭐냐면, mask 씌워서 비어있어야 하는 공간인데 depth prediction을 해버리는 것이다.(ray를 잘못 쐈다던가 등의 문제이기 때문에, ray가 object만 쏠 수 있도록 만들어 주는 것) 식은 아래와 같다.

![img17](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210402-30.PNG)

__Occupancy Loss__

![img18](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210402-31.PNG)

 물론 그 반대에 해당하는 경우도 있다. ray가 쏴서 surface를 감지해야하는데, 아무것도 없다고 하는 경우에 해당하는 loss이다. 이를 occupancy loss라고 하고, 위 식과 같이 쓰게 된다.

__Normal loss__

![img19](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210402-32.PNG)

 smoothness에 대한 loss도 추가하게 된다. 위 식에서 p는 surface point이고, q는 근처의 랜덤하게 sample된 surface point이다. 이러한 것들의 normal vector가 서로 비슷하도록 해서 smooth한 surface를 만들도록 한다.

__3.5. Implementation Details__

 다른 것들 여러가지가 있는데, 여하튼 NVIDIA V100을 썼다고 한다. ray sampling은 n=16~128까지 썼다고 한다.(Forward pass에서 j를 말함) 그리고 $\tau$의 값은 0.5를 두었다고 한다.



### Experiments

먼저 single RGB image에서 3D recon을 해보고, 그 다음으로 multi-view recon을 했다고 한다.

__4.1. Single-View Reconstruction__

__Datasets__

 ShapeNet dataset을 썼다고 한다.

__Baselines__

 다른 model들과의 비교에서, 3D-R2N2와 Pixel2Mesh, ONet 등과 비교했다고 한다.

__4.1.1. Multi-View Supervision__

 일단 multi-view supervision으로 학습을 시킨 경우의 결과는 다음과 같다

![img20](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210402-33.PNG)

![img21](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210402-34.PNG)

 좋은 성능을 보이는 것을 확인할 수 있다. 다른 것들보다도 나름 정확한 결과를 보이는 것을 볼 수 있고, 색깔도 은근 잘 맞다.

__4.1.2. Single-View Supervision__

 supervision을 할 때 한 개의 image만 input을 쓰는 것이다. 놀랍게도 효과가 있다!

__Result__

![img22](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210402-35.PNG)

 아마도 object level에서는 어느정도의 network parameter가 학습이 되어 어떤 것인지도 알아내는 것 같다. 아마도 어느정도의 overfitting이 만들어낸 결과이지않을까 싶다.

__4.2 Multi-View Reconstruction__

 Single view reconstruction에서는 두가지 조건을 두었다. 첫번째는 multi-view image와 mask가 주어진 경우이고, 두번째는 추가적인 sparse depth map이 주어진 경우이다.

__Dataset__

 DTU dataset을 사용했다고 한다.

__Baselines__

 3D representation을 하기 위해서 screened Poisson surface reconstruction(sPSR) 을 사용했다고 한다.

__Results__

![img23](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210402-36.PNG)

 그 결과이다. 그 어디에도 GT는 없다. 엄청난 결과물인 것을 확인할 수 있다.

![img24](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210402-37.PNG)

 위 Table에서 얼마나 수치적으로 훌륭한지 확인할 수 있다. 

 또한, 그 성능을 추가적으로 확인해보면,

![img25](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210402-38.PNG)

 어느정도 박살나있더라도 복원이 되고(이건 아무래도 image base로 학습시키다 보니, GT 3D representation이 박살나 있더라도 그거보다 더 잘 만드는 것 같다. 즉, GT를 만드는 것에 더 효과적인 방법이라고 볼 수 있는 듯 하다)

![img26](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210402-39.PNG)

 Colmap의 것이랑 비교한 것이라는데, 음... 뭐 아무튼 잘 됬다.



### Conclusions and Future Work

 DVR이 조금 더 많은 곳으로 확장이 되었으면 하는 것 같다. texture 뿐 아니라 조금 더 복잡한 material property등으로.



### Acknowledgements



### Reference

Ranftl, René, Alexey Bochkovskiy, and Vladlen Koltun. "Vision Transformers for Dense Prediction." *arXiv preprint arXiv:2103.13413* (2021).

