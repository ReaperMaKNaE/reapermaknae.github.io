---
layout : post
title : "GASDA"
date : 2021-05-01 +0900
description : Cycle-GAN을 이용하여 monocular depth estimation에서 성공적으로 domain adaptation을 이룬 GASDA 논문의 간단한 리뷰입니다.
tag : [PaperReview]
---

### Geometry-Aware Symmetric Domain Adaptation for Monocular Depth Estimation, CVPR 2019



### Abstract

 supervised depth estimation이 높은 성능을 보여주고 있지만, depth annotation을 얻는 것은 그 cost가 크다. 최근 이를 해결하기 위해 unsupervised 방식이 도입되긴 하였으나, 실제 label을 이용한 경우보단 그 효과가 덜하다. 이러한 현상을 해결하기 위해 domain adaptation이 나왔다. 풍부한 synthetic dataset을 이용하여 domain adaptation을 수행한다. 하지만 이러한 것도 target domain과의 gap이 존재한다는 것을 간과하면 안된다. 이러한 관찰에 따라, 우리는 geometry-aware symmetric domain adaptation framework인 GASDA를 제안한다. 두 image style translator와 depth estimator를 훈련시킴으로써 우리의 모델은 더 좋은 image style transfer를 만들 수 있었고 high-quality depth map을 받을 수 있었다. 

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210501-1.PNG)



### Introduction

 Monocular depth estimation이 computer vision 영역에서 활발한 연구 영역으로 자리잡았다. 하지만 그 depth annotation map을 얻는 것은 힘들기에, unsupervised monocular depth estimation이 많이 제안되고 있다. 하지만 이러한 방법은 빛의 변화, occlusion이나 blur등에 매우 취약한 모습을 보이고 있다. 따라서 이를 대응하기 위해 synthetic dataset을 이용해서 real-world data에 잘 적용시키기 위한 domain adaptation이 시작했다. 하지만 domain shift가 결코 만만하게 볼 상대는 아니다. 최근 연구들이 이를 줄이기 위해 계속 해왔으나 여러 한계에 부딪혔고, stereo image를 이용한 domain adaptation도 진행되어왔다.

 아무튼 이러한 연구들에 영감을 받아, 우리는 Geometry-Aware Symmetric Domain Adaptation Network(GASDA)를 제안한다. 이는 CycleGAN에 영감을 받았으며, synthetic to real, real to synthetic 에 해당하는, real-stereo image에서 epipolar geometry를 base로 geometry consistency를 이용한다.

 따라서, 우리의 contribution은 다음과 같다.

- end-to-end domain adaptation framework을 제안한다.
- 우리는 성능을 boost 시켜주는 real domain에서의 epipolar geometry를 이용한 constraint를 이용한다.
- KITTI dataset과 Make3D dataset에서 실험을 진행했다.



### Related Work

__Monocular Depth Estimation__

__Domain Adaptation__

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210501-2.PNG)



### Method

__3.1. Method Overview__

 주어진 synthetic image-depth pair가 N개가 있다고 하면, 우리의 목표는 이를 이용해 target domain의 depth를 예측하는 것이다. 다른 network들의 경우 대개 synthetic to real만 고려하거나, real to synthetic만 고려했는데, 우리는 둘 다 고려했다. 결과적으로 우리는 2개의 depth estimator $F_s$ 와 $F_t$ 를 학습시킬 수 있었다. 이 둘을 동시에 학습시키는 것은 쉽지 않은데, 왜냐하면 constraint을 잘 정해주지 않는다면 제대로 된 학습이 되지 않기 때문이다. 우리는 이러한 constraint를 epipolar geometry로 정했다. 여기서 notation은, real stereo pair를 다음과 같이 표기하고, l과 r은 각각 left, right를, i는 i 번째 image를 의미한다. => $x_{tl}^i, x_{tr}^i$ 

 추가적으로, 우리는 depth consistency loss를 추가했다. 이러한 모든 framework은 아래 그림에 나와있다.

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210501-3.PNG)

__3.2. GASDA__

__Bidirectional Style Transfer Loss__

 우리의 목표는 bidirectional translator인 $G_{s2t}$ 와 $G_{t2s}$ 사이의 gap을 좁히는 것이다. 구체적으로 target에서 일단 설명하자면, 우리는 discriminator $D_t$ 를 이용해서 $G_{s2t}, D_t$ 를 minimax game을 수행해서 학습시킨다. 이 때 adversarial loss는 아래와 같다.

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210501-4.PNG)

 불행하게도, 위 loss는 잘 되지 않았다. 그래서 우리는 cycle-consistency loss를 채용했다.

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210501-5.PNG)

 여기에 추가적으로 identity mapping loss를 추가했다.

![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210501-6.PNG)

 이를 모두 더한 총 loss는 아래와 같다.

![img7](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210501-7.PNG)

__Depth Estimation Loss__

 우리는 이제 synthetic image를 target domain의 style로 만든다. 우리는 depth estimation network $F_t$ 를 supervised manner로 학습되는 synthetic domain에서 학습시킨다. 아무튼 이 결과는 아래와 같다.

![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210501-8.PNG)

 위 식에서 $\tilde{y_{ts}}$ 는 target domain에서 예측한 source의 depth map이다.

![img9](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210501-9.PNG)

 뿐만 아니라, 상호적인 것을 쓰기 위해 위와 같은 loss도 있다. 위 식에서 $\tilde{y}_{ss}$ 는 source image input에 대한 $F_s$ 의 output이다. 따라서, full depth estimation loss는 아래와 같다.

![img10](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210501-10.PNG)

__Geometry Consistency Loss__

 이전에 우리는 이미 naive depth adversarial adaptation을 이용했지만, 이들은 완벽하지 않다. 다른 scene이나 dataset에서 오는 geometric structure를 무시했기 때문인데, 이에 우리는 epipolar geometry를 이용한 geometric constraint를 각 depth estimation network에 적용했다. 구체적으로, 우리는 오른쪽 이미지에서 얻은 predicted depth로 inverse warped image를 만들고, 이로 왼쪽 이미지를 만든다. 그리고 single scale SSIM term을 이용해 stereo image 사이의 geometry consistency loss를 만들게 된다.

![img11](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210501-11.PNG)

 위 식에서 $L_{gc}$ 는 full geometry consistency loss를, $L_{tgc} , L_{sgc}$ 는 각각 depth estimation network에서의 geometry consistency loss를 말한다. $x'_{tt} (x'_{st})$ 는 우측 이미지에서 inverse warp된 것이다.(이 뒤에 무슨 문장이 나오는데 해석이 잘 안됨. 이후 코드를 봐야할 듯)

__Depth Smoothness Loss__

![img12](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210501-12.PNG)

  위 식에서 역삼각형은 spatial direction의 첫 미분이다. 우리는 이러한 smoothness loss를 $X_t$ 와 $X_{t2s}$ 에만 적용했다. (왜냐면 source는 이미 full supervision이기 때문에)

__Depth Consistency Loss__

 $F_t(x_t)$ 와 $F_s(G_{t2s}(x_t))$ 사이의 inconsistency가 생각보다 커서, 이를 줄이기 위한 loss를 아래와 같이 설정했다고 한다.

![img13](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210501-13.PNG)

__Full Objective__

 그래서, 이걸 싹~다 합하면 아래와 같다.

![img14](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210501-14.PNG)

__3.3. Inference__

결국 마지막 predicted depth map은 아래와 같다.

![img15](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210501-15.PNG)

 위 식에서 두 term은 아래와 같다.

![img16](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210501-16.PNG)

![img17](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210501-17.PNG)

 그림으로 표현한 것.

![img18](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210501-18.PNG)



### Experiments

 KITTI와 Make3D dataset에서 진행.

__4.1. implementation Details__

__Network Architecture__

__Datasets__

__Training Details__

 high-quality image를 만들기 위해 fine tune을 했는데, 이는 아래 그림과 같다.

![img19](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210501-19.PNG)

__4.2. KITTI Dataset__

![img20](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210501-20.PNG)

![img21](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210501-21.PNG)

 위 표에서 [2] 보다 accuracy metric은 떨어지게 되는데, 이는 [2]가 GTA5 dataset에서 학습이 되었고, GTA5가 vKITTI 보다 더 나아서 GTA5 <-> KITTI 의 domain gap이 vKITTI <-> KITTI domain gap보다 작다고 한다. 근데 GTA5 dataset에서도 depth가 있나? 없는 것 같은데 뭐 어떻게 했나보다. 이는 [2]를 제대로 읽어봐야 알 듯 하다.

__4.3. Make3D Datset__

![img22](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210501-22.PNG)

__4.4. Ablation Study__

![img23](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210501-23.PNG)

![img24](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210501-24.PNG)

__Domain Adaptation__

__Geometric Consistency__

__Symmetric Domain Adaptation__

 위 3가지 내용들은 Table 3참조.



### Conclusion



### Reference

Zhao, Shanshan, et al. "Geometry-aware symmetric domain adaptation for monocular depth estimation." *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2019.

