---
layout : post
title : "DeepTAM"
date : 2021-01-14 +1507
description : DeepTAM, Deep Tracking and Mapping의 간단한 리뷰입니다.
tag : [PaperReview]
---

### DeepTAM: Deep Tracking and Mapping, ECCV 2018



 요약하면, pose hypotheses를 만들고, 여러 differentiable한 문제때문에 optical flow prediction을 도입, band module과 iterative한 refinement를 도입해서 성능향상을 도모했음.



### Abstract

 우리는 이번에 keyframe-based dense camera tracking과 depth map estimation을 한 번에 학습하는 system을 발표함. tracking에서 우리는 현재 camera image와 synthetic viewpoint 사이의 작은 pose 증가를 estimate함. 이것은 camera motion에서 data set의 bias와 학습 문제를 간단하게 만들어 줌. 추가로, 좀 더 정확한 예측을 위해 많은 양의 pose hypotheses를 만들어서 보여줌. mapping에서 우리는 정보를 현재의 depth estimate에 해당하는 cost volume에 축적함. mapping network는 cost volume과 depth prediction을 update하는 keyframe image의 조합임. 이러한 것은 depth 측정과 image based prior를 만드는 것에 효과적임. 우린 noisy camera pose에도 SOTA를 찍었음. RGB-D tracking algorithm으로 6DOF tracking의 성능을 보여줄거임. 이러한 것들을 다른 algorithm과도 비교해봤음.



### Introduction

 recognition에 반해서, camera tracking이나 3D mapping에 딥러닝을 적용하는 것엔 한계가 있었음. 왜냐하면 사실 이미 3D mapping은 굉장히 괜찮은 수준이기에 굳이 적용할 필요를 못느끼고 있었던 거였음. 그리고 이러한 learning은 너무나도 많은 camera tracking이나 3D mapping과 연관되어 있었음. 하지만 이렇게 제한된 곳에서는 deep learning이 잘 됬음. 예를 들면 disparity estimation같은 것들.

 우리는 full-scale SLAM system에 적용가능하도록 확장해봤음. 우리가 visual SLAM에서 중요하게 생각하는 두 가지 요소는 다음과 같음.

- camera pose tracking
- dense mapping

 그리고 이 논문의 main contribution은 다음과 같음.

- dataset bias problem을 해결할 tracking network architecture
- 좀 더 정확한 pose estimation을 위한 camera pose의 multiple hypothesis
- image based prior에서 depth 측정하여 정확하고 robust한 depth map을 얻는 mapping network architecture
- narrow band technique(?)을 이용한 효과적인 depth refinement strategy

 classic한 방법으로는 DTAM(Dense Tracking And Mapping)이 연관이 있다. 여기서 우린 learning problem만을 제외하고 유사하게 접근했고, 결과적으로 DeepTAM을 만들게 되었다.

 Tracking에 있어서 DeepTAM은 현재의 camera image를 color/depth image의 keyframe에 aligning하는 neural network를 사용함. 우린 coarse-to-fine network을 만들기 위해 작고 빠른 stack of network을 사용했음. 이러한 network는 camera pose estimation을 refine할 수 있음. each step마다 virtual keyframe(color, depth image)을 업데이트하고, 이런 것들 덕분에 camera pose prediction 성능을 향상시킬 수 있다. 또한 learning task를 간단하게 만들고 dataset bias를 줄인다. 추가로 우리는 pose accuracy를 향상시킬 많은 양의 hypotheses를 만드는 것을 보여준다.

 우리의 mapping network은 plane sweep stereo에서 시작했다. multiple image에서 information을 축정한 다음, DNN에서 depth map을 뽑아냈다. 이렇게 뽑아낸 cost volume을 refine하기 위해 previous surface estimate 근처에 narrow band를 선언했음. 이렇게 얻어딘 depth는 많은 vision task에서 가치있는 cue가 될 수 있다.

 학습의 관점에서 DeepTAM은 camera에 대해서 다양한 흔적과 learning implicit에 매우 적응을 잘한다.(? robust하단 뜻인가) 이런 것은 SIFT와 비교되는 점들이다. 딥러닝의 문제 중 하나로 overfitting이 있는데, 이마저도 우린 어떻게 해결을 했다.

 결과적으로, DeepTAM은 뭐여튼 아무튼 그렇게 SOTA임.



### Related Work

 가장 연관되어 있는 work는 DTAM이다. 같은 아이디어에서 시작했는데, 시간의 흐름에 따른 depth의 aggregation과 keyframe으로의 dense depth map을 추가한 drift-free camera pose tracking이다. 하지만, 우리는 이러한 개념을 완전히 다른 방법으로 접근했다. 특히, tracking과 mapping은 모두 deep network에 의해 실행이 된다.

 학습 방법의 관점에서 봤을 때 가장 유사한 것은 DeMoN이다. DeMoN과 비교했을 때 우리의 모델은, 두 장 초과의 image를 사용했다는 점이다. keyframe에서 오는 drift를 피해서 depth map을 조금 더 refine할 수 있었다.

 몇몇 deep learning에 저장된 work들이 있었다. PoseNet, DeepVO, SfMNet, UnDeepVO 등등. 거기에 추가로 DeMoN도 있었으나, 전부 제약이 조금씩 있었음.

 이러한 deep learning들은 KITTI dataset에 focus하였었음. 이 dataset의 가장 큰 문제는 camera translation과 rotation 사이의 부분이 존재하질 않아 애매한 점이 있었음.(자동차가 sideward로 움직일 수는 없었기 때문에) 반면에 우리가 진행하고자 하는 것은 6DOF(상하전후좌우)임.

 우린 classical한 tracking과 mapping technique을 모두 cover하지는 못했지만, 그래도 어느정도 커버는 친 상태임. LSD-SLAM이 loop closing을 포함해서 full SLAM pipeline을 보여주었음. 하지만 얘는 sparse한 depth estimation을 얻었음. 뭐 기타 여러개 더 있음.



### Tracking

 우린 주어진 현재 camera image I^C와, image I^K, inverse depth map D^K로 구성된 keyframe으로 4x4 transformation matrix T^KC를 estimate하고 시음. 여기서 keyframe pose T^K와 T^C는 아래와 같은 연관성이 있음.

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210114-12.PNG)

 T^KC를 구하려면 현재 image I^C와 keyframe (I^K, D^K)의 연관성을 찾아야했음. 이건 사실 image pair 사이에서 pixel displacement가 작다면 쉽게 해결할 수 있는 문제였음. 우린 이러한 camera pose를 iteractive rate하게 track하려 했기에, guess인 T^V를 T^C에 가깝게 가정했음. DTAM과 유사하게, virtual keyframe인 (I^V와 D^V)를 만들었음. 바로 T^KC를 예측하는 것 대신에 우리는 increment인 \delta T를 예측하기위해 학습시켰음. 

 ![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210114-13.PNG)

__3.1 Network Architecture__

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210114-14.PNG)

 우리가 설계한 encoder-decoder based architecture는 위와 같다.

 camera의 움직임이 현재 image의 keyframe과 연관되어있다면 예측이 되기 때문에, 우리는 보조 task로 optical flow를 사용했다. predicted optical flow는 두 frame 사이에서의 관계를 찾아내는 것을 도와주었다. 만들어진 각 pose hypothesis는 6D pose vector이다.

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210114-16.PNG)

 그래서 우리는 final pose estimation을 위와 같이 두었다. \delta\zeta_i = (r_i, t_i)^T 이다. r_i는 rotation, t는 translation이다. 간단하게 하기 위해 위 \delta\zeta는 small rigid body motion이라고 한다.

 coarse camera motion은 low resolution에서는 이미 가능하다. 그래서 우리는 coarse-to-fine strategy를 real time의 camera track에 사용한다. 우리는 3가지 구분되는 tracking network를 figure 2에 나타냈다. 이러한 network는 서로 다른 resolution에서도 pose estimation을 할 수 있고, previous resolution level에서도 refine이 가능하다.

 ![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210114-15.PNG)

 __3.2 Training__

 Learning based model은 자기에게 알맞는 dataset에 매우 의존적임. dataset은 중요한 mode를 전부 cover하지 않음. 자율 주행을 위한 KITTI의 경우는 motion이 "면(plane)"으로 제한되어있음. 결과적으로 이러한 learning based model은 이런 형태의 dataset에 overfit이 쉽게 됨. 인공적인 dataset은 이러한 문제를 완화시킬 수 있지만, 이걸 만드는건 또 다른 문제임.

 우린 이러한 문제에 두 가지 방법으로 태클을 걸었음. 식(2)를 사용해서 (식2는 \delta\zeta로 나왔던 부분) keyframe과 current camera imgae 사이의 absolute motion대신 small increment를 예측하는 거임. 이러한 것은 motion의 크기를 줄이고 task의 어려움을 낮추게 해 주었음. 두번째로는, real keyframe의 image와 depth map을 render해서 사용했음.(? 노가다했단 뜻인듯) GT의 pose를 중심으로 normal distribution을 가지는, virtual frame (I^V, D^V)에서의 pose T^V를 찾음. 이는 6DOF motion 모든 방향으로 가능했고, data augmentation이 가능해서 motion의 제한을 풀 수 있었음.

__Datasets__

 우린 SUN3D dataset과 SUNCG dataset을 사용하였음.

__Training Objective__

![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210114-17.PNG)

 tracking network의 Loss는 위와 같음. predicted optical flow __w__ 와 predicted pose \delta\zeta는 network의 output임. loss L_flow는 아래와 같이 정의됨.

![img7](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210114-18.PNG)

 나머지 두 종류의 loss는 다음과 같음.

![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210114-19.PNG)

 r과 t는 각각 rotation과 translation임.(근데 여기서 보면, PoseNet에서 생겼던 문제와 똑같은 방식을 선택한 것 같다. 해당 내용은 PoseNet을 Upgrade 했던 [https://reapermaknae.github.io/2021/01/Geometric_loss_functions/](https://reapermaknae.github.io/2021/01/Geometric_loss_functions/) 참조)

 alpha의 경우는 rotation과 translation 사이의 scale을 맞추기 위해 추가한 것. 식 (7)은 multivariate Laplace distribution의 negative log-likelihood로(이건 뭐지...), predicted average motion과 합쳤다. 여기서 \largeLatterSigma는,

![img9](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210114-20.PNG)

 위와 같다. 그리고 x는 gt와 predict value의 \delta\zeta 차이이다. 그런데 사실 이 값은 optimization 동안은 constant로 뒀다. K_v는 modified Bessel function의 두번째이다. 이런 형태가 loss base로 된 multivariate Laplace distribution이 multivariate normal distribution보다 훨씬 성능이 좋다는 것을 알게 되었다. uncertainty loss는 network에 예측된 pose인 \delta\zeta를 준다.

 optimizer로는 Adam을 썼다.



### Mapping

 우린 우리가 매 keyframe마다 계산한 depth map으로 geometry scene을 형성한다. high quality depth map을 만들기 위해 우리는 여러 image에서 온 information을 cost volume에다 축적했다. 그리고 depth map은 CNN에 의해 cost volume에서 추출이 된다.

 C가 Cost volume이고, C(x,d)가 pixel x, depth d에서의 photoconsistency라고 한다면, depth d와 C(x,d)는 아래와 같다.

![img10](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210114-21.PNG)

 \rho는 photoconsistency를 말한다.(pixel x, depth d에서 warped image I_i와 keyframe image I^K 사이의 3x3 patch들의 SAD값과 같음). 여기서 warp된 image는 warping function을 이용해서 얻는다.

 그리고 식 (9)에서 w는 weight로, 다음과 같다.

![img11](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210114-22.PNG)

 classic method에서는 cost volume은 data term에서 얻어지고, depth map은 minimum cost를 찾는 것에서 부터 얻어진다. 하지만, cost volume의 noise 때문에, 다양한 sophisticated regularization term과 optimization technique이 robust한 방법으로 depth를 추출해내도록 소개되었다. 대신에, 우리는 network을 cost volume에서 cost 정보를 matching시키고 동시에 cost 정보를 image based scene과 합쳐 조금 더 정확하고 robust한 depth estimate을 할 수 있도록 했다.

 cost-volume-based 방법에서 정확도는 depth labels N의 개수에 따라 제한이 된다. 그러므로, 우리는 narrow band strategy를 취해 labels의 숫자를 constant로 설정하면서 sampling density를 올렸다. 우리는 depth label의 narrow band를, 예측된 d_prev에서 다음과 같이 정의했다.

![img12](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210114-23.PNG)

 위 식에서 \sigma_nb는 narrow band width를 결정한다. narrow band는 조금 더 정확하게 depth map을 복구할 뿐 아니라, 정확한 장소에서 좋은 initialization과 regularization을 필요했다. 우리는 이러한 task를 multiple encoder-decoder type의 network를 사용해서 해결했다.

![img13](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210114-24.PNG)

 위 그림은 우리가 사용할 mapping network를 대략적으로 보여주는 것이다.

__4.1 Network Architecture__

 Network는 image와 camera set에서 계산된 cost volume C와 keyframe image I에서 온 keyframe inverse depth D를 예측하도록 훈련받았다. 여기서 depth는 inverse depth를 사용했다. (closer distance에서 좀 더 효과적임) 그리고 depth axis를 기준으로 coarse-to-fine strategy를 사용했다. 그러므로 mapping network는 fixed band module과 narrow band module 두 가지로 나뉜다. fixed band module은 전체적인 depth range에서 cost volume을 만들고, narrow band cost volume은 현재의 depth estimation과 축적된 정보들 중에서 small band에 해당하는 값들만 예측한다.

 fixed band module은 minimum과 maximum depth label 사이의 interpolation factor를 output으로 regress한다. 결과적으로 network는 절대적인 scale에 국한되지 않고 조금 더 유연하게 된다. fixed band와 달리 depth labels을 fronto-parallel plane의 set으로 가지고 있는 narrow band는 each pixel 각각에 해당한다. interpolation factor를 예측하는 것은 적합하지 않다.(왜냐하면 narrow band module안의 network는 band가 어떤 형태인지 모르기 때문에) 이렇게 band가 어떤 형태인지 모르게 한 후 학습시키는 것은 overfitting을 막ㄱ ㅔ해준다. 하지만 이러한 작업은 depth regularization을 어렵게 만들어준다.(아래 그림처럼)

![img14](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210114-25.PNG)

 그래서 우리는 depth regularization 문제를 해결 할 refine network를 추가했다. 두 network들 모두 data solving과 smoothness에서 함께할 수 있다.(??) 이러한 내용은 저 위에 있는 Figure 3에 network가 나와있다.

__4.2 Training__

 Tensorflow사용했고, Adam optimizer 썼다.

__Datasets__

SUN3D, SUNCG를 썼다. 추가적인 dataset은 MVS에 COLMAP을 써서 해결했다. 이걸 찍을 때엔 GoPro camera를 썼다.

__Training Objective__

 simple l1 loss를 사용했다.

![img15](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210114-26.PNG)

 여기서 g는 predict와 gt의 depth map에서 gradient를 나타낸 것이다.(discontinuity를 의미함)

![img16](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210114-27.PNG)



### Experiments

__5.1 Tracking evaluation__

__5.2 Mapping Evaluation__

![img17](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210114-28.PNG)

 mapping error metric은 위와 같음. sc-inv는 scale invariant metric임.

 그 외 성능 분석.

![img18](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210114-29.PNG)

![img19](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210114-30.PNG)

![img20](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210114-31.PNG)


### Conclusions



### Acknowledgements



### References

Zhou, Huizhong, Benjamin Ummenhofer, and Thomas Brox. "Deeptam: Deep tracking and mapping." *Proceedings of the European conference on computer vision (ECCV)*. 2018.
