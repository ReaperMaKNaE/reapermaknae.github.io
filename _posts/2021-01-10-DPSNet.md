---
layout : post
title : "DPSNet"
date : 2021-01-10 +1415
description : Plane Sweep을 이용하여 MVSNet과 다른 방향으로 3D Reconstruction을 DPSNet, End-to-End deep plane sweep stereo 논문의 간단한 리뷰입니다.
tag : [PaperReview]
---

### DPSNet: End-to-end Deep Plane Sweep Stereo, ICLR 2019



### Abstract

  Plane sweep algorithm을 이용해서 deep feature에서 cost volume을 만들었다.



### Introduction

 여러 분야에서 3D scene information은 유용했다.(semantic segmentation, human pose/action recognition 등) photo-consistency constraints가 struct엔 도움이 되었으나, textureless하거나 reflective region에는 별 효과가 없었음.

 최근 CNN을 이용해서 이를 해결하려는 움직임이 꽤나 있음.

 여튼 여기서 우리는 Deep Plane Sweep Network(DPSNet)을 소개함. 이 DPSNet은 spatial transformer networks에 영감을 받아 differentiable warping module을 사용할 수 있었음.

 그리고 local cost volume filtering에 기반한 cost aggregation을 소개함.



### Related Work

__Stereo Matching__

 stereo matching에서는 depth estimation을 해결하려는 움직임이 특히 많았음. 

__Depth from single images__

__Multi-view stereo__



 이 외에도 SurfaceNet과 같은 것들이 있었으나, GPU memory의 한계로 voxel representation이 제한된 케이스가 있었음.



 MVSNet이라고 concurrent한 연구가 있지만, 우리가 집중한 것은 다음과 같음.

- Full 3D recon이 아닌, dense depth estimation에 focus한 것
- two-view matching에도 accurate depth maps을 얻을 수 있는 것
- context features를 적용하여 모든 cost slice를 refine한 것.



### Approach

 DPSNet은 크게 네가지 파트로 나뉘어짐.

- Feature Extraction
- Cost Volume Generation
- Cost Aggregation
- Depth map Regression

 ![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210110-1.PNG)

자세한 부분들은 Code Review 파트에서 진행.

__3.1 Multi-scale feature extraction__

__3.2 Cost volume generation using unstructured Two-view images__

 ![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210110-2.PNG)

 cost volume을 만드는 과정에 있어서 inverse-depth space를 썼는데, inverse를 쓴 이유는 아마 코드를 더 짜기 쉽게 하기 위함으로 보임.

 ![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210110-3.PNG)

 위 식은 다른 곳에서도 많이 봤을 가능성이 높음. paired feature를 warp하는 방법.

__3.3 Cost Aggregation__

 ![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210110-4.PNG)

 edge-preserving filtering을 통해 noisy cost volume을 regularize하는 것이 key idea이다.

 마찬가지로 자세한 과정은 code reivew에서 살펴볼 예정.

__3.4 Depth regression__

 ![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210110-5.PNG)

 GC Net에서 사용했던 것과 동일.

__3.5 Training Loss__

![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210110-6.PNG)

위에서 \theta는 DPSNet에서의 set of all the learnable parameters를 나타낸다. H의 의미는 Huber norm이며, pytorch에서는 smoothL1Loss와 같다.



### Experiments

__4.1 Implementation Details__

SUN3D, RGBD, Scenes11 dataset 사용, 1200K iter, ADAM optimizer, batch size 16, lr은 2e-4

__4.2 Comparison with state-of-the-art methods__

단순한 비교가 아니라, completeness, geometry error, photometry error의 기준을 추가하였음.

__4.3 Ablation Study__

__Cost Volume Generation__

__Cost Aggregation__

__Depth Label Sampling__

__Number of Images__

__Rectified Stereo Pair__



### Discussion



### Code Review

 아래 github link에서 code를 확인할 수 있다.

[https://github.com/sunghoonim/DPSNet](https://github.com/sunghoonim/DPSNet)

train.py 내부의 def train을 살펴보자.

```python
def train(args, train_loader, dpsnet, optimizer, epoch_size, train_writer):
    global n_iter
    batch_time = AverageMeter()
    data_time = AverageMeter()
    losses = AverageMeter(precision=4)

    # switch to train mode
    dpsnet.train()

    end = time.time()

    for i, (tgt_img, ref_imgs, ref_poses, intrinsics, intrinsics_inv, tgt_depth) in enumerate(train_loader):
        # measure data loading time
        data_time.update(time.time() - end)
        tgt_img_var = Variable(tgt_img.cuda())
        ref_imgs_var = [Variable(img.cuda()) for img in ref_imgs]
        ref_poses_var = [Variable(pose.cuda()) for pose in ref_poses]
        intrinsics_var = Variable(intrinsics.cuda())
        intrinsics_inv_var = Variable(intrinsics_inv.cuda())
        tgt_depth_var = Variable(tgt_depth.cuda()).cuda()

        # compute output
        pose = torch.cat(ref_poses_var,1)

        # get mask
        mask = (tgt_depth_var <= args.nlabel*args.mindepth) & (tgt_depth_var >= args.mindepth) & (tgt_depth_var == tgt_depth_var)
        mask.detach_()

		# Train Loader is DataLoader from dataset

		# target image variable, reference image variable, pose, intrinsic variable, intrinsic_inv variable
		# Therefore, the values are : TargetImage, ReferImage, Pose, Intrinsic(Camera Parameter), IntrinsicInv(Camera Parameter)
		# Intrinsic_inv means inverse matrix for intrinsic matrix of camera parameter

		# target image variable		: Target Image
		# reference image varaible	: Reference Image(there are too many reference Image for each target Image)
		# pose						: Position of Camera
		# intrinsic variable		: Intrinsic Matrix of Camera(e.g., camera lens parameter)
		# intrinsic inv variable	: Inverse Matrix of Intrinsic Matrix of Camera

		# depths	: initial Cost Volume
		# disps		: refined Cost Volume
		# Something different with papers, where is disps?
		# I guesss the loss function doesn't carry it

        depths = dpsnet(tgt_img_var, ref_imgs_var, pose, intrinsics_var, intrinsics_inv_var)
		# _, depths = dpsnet(~~~)

        disps = [args.mindepth*args.nlabel/(depth) for depth in depths]

        loss = 0.
        for l, depth in enumerate(depths):
            output = torch.squeeze(depth,1)
            loss += F.smooth_l1_loss(output[mask], tgt_depth_var[mask], size_average=True) * pow(0.7, len(depths)-l-1)
			# loss += F.smooth_l1_loos(output[mask], tgt_depth_var[mask], size_average=True) * pow(0.7, len(depths)-l-1) + F.smooth_l1_loss(torch.squeeze(disps,1)[mask], tgt_depth......)

        if i > 0 and n_iter % args.print_freq == 0:
            train_writer.add_scalar('total_loss', loss.item(), n_iter)

        if args.training_output_freq > 0 and n_iter % args.training_output_freq == 0:

            train_writer.add_image('train Input', tensor2array(tgt_img[0]), n_iter)
            
            depth_to_show = tgt_depth_var.data[0].cpu()
            depth_to_show[depth_to_show > args.nlabel*args.mindepth] = args.nlabel*args.mindepth
            disp_to_show = (args.nlabel*args.mindepth/depth_to_show)
            disp_to_show[disp_to_show > args.nlabel] = 0
            train_writer.add_image('train Dispnet GT Normalized',
                                   tensor2array(disp_to_show, max_value=args.nlabel, colormap='bone'),
                                   n_iter)
            train_writer.add_image('train Depth GT Normalized',
                                   tensor2array(depth_to_show, max_value=args.nlabel*args.mindepth*0.3),
                                   n_iter)

            for k,scaled_depth in enumerate(depths):
                train_writer.add_image('train Dispnet Output Normalized {}'.format(k),
                                       tensor2array(disps[k].data[0].cpu(), max_value=args.nlabel, colormap='bone'),
                                       n_iter)
                train_writer.add_image('train Depth Output Normalized {}'.format(k),
                                       tensor2array(depths[k].data[0].cpu(), max_value=args.nlabel*args.mindepth*0.3),
                                       n_iter)

        # record loss and EPE
        losses.update(loss.item(), args.batch_size)

        # compute gradient and do Adam step
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # measure elapsed time
        batch_time.update(time.time() - end)
        end = time.time()

        with open(args.save_path/args.log_full, 'a') as csvfile:
            writer = csv.writer(csvfile, delimiter='\t')
            writer.writerow([loss.item()])
        if i % args.print_freq == 0:
            print('Train: Time {} Data {} Loss {}'.format(batch_time, data_time, losses))
        if i >= epoch_size - 1:
            break

        n_iter += 1

    return losses.avg[0]
```

 이 net을 공부할 당시만 해도 multi view geometry in computer vision에 대한 공부를 많이 하지 않은 상태였기에, 좀 지저분하지만 코드 내에 위와 같이 주석을 달아주었다.

 모델은 models 폴더 내의 PSNet.py에 들어가있다.

```python
class PSNet(nn.Module):
    def __init__(self, nlabel, mindepth):
        super(PSNet, self).__init__()
        self.nlabel = nlabel
        self.mindepth = mindepth

        self.feature_extraction = feature_extraction()

        self.convs = nn.Sequential(
            convtext(33, 128, 3, 1, 1),
            convtext(128, 128, 3, 1, 2),
            convtext(128, 128, 3, 1, 4),
            convtext(128, 96, 3, 1, 8),
            convtext(96, 64, 3, 1, 16),
            convtext(64, 32, 3, 1, 1),
            convtext(32, 1, 3, 1, 1)
        )

        self.dres0 = nn.Sequential(convbn_3d(64, 32, 3, 1, 1),
                                     nn.ReLU(inplace=True),
                                     convbn_3d(32, 32, 3, 1, 1),
                                     nn.ReLU(inplace=True))

        self.dres1 = nn.Sequential(convbn_3d(32, 32, 3, 1, 1),
                                   nn.ReLU(inplace=True),
                                   convbn_3d(32, 32, 3, 1, 1)) 

        self.dres2 = nn.Sequential(convbn_3d(32, 32, 3, 1, 1),
                                   nn.ReLU(inplace=True),
                                   convbn_3d(32, 32, 3, 1, 1))
 
        self.dres3 = nn.Sequential(convbn_3d(32, 32, 3, 1, 1),
                                   nn.ReLU(inplace=True),
                                   convbn_3d(32, 32, 3, 1, 1)) 

        self.dres4 = nn.Sequential(convbn_3d(32, 32, 3, 1, 1),
                                   nn.ReLU(inplace=True),
                                   convbn_3d(32, 32, 3, 1, 1)) 
 
        self.classify = nn.Sequential(convbn_3d(32, 32, 3, 1, 1),
                                      nn.ReLU(inplace=True),
                                      nn.Conv3d(32, 1, kernel_size=3, padding=1, stride=1,bias=False))

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2. / n))
            elif isinstance(m, nn.Conv3d):
                n = m.kernel_size[0] * m.kernel_size[1]*m.kernel_size[2] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2. / n))
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()
            elif isinstance(m, nn.BatchNorm3d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()
            elif isinstance(m, nn.Linear):
                m.bias.data.zero_()
```

 net을 만드는 코드는 위와 같은데, feature_extraction과 같은 내용은 같은 models 폴더 내의 submodule에 들어가있다.

```python
class feature_extraction(nn.Module):
    def __init__(self):
        super(feature_extraction, self).__init__()
        self.inplanes = 32
        self.firstconv = nn.Sequential(convbn(3, 32, 3, 2, 1, 1),
                                       nn.ReLU(inplace=True),
                                       convbn(32, 32, 3, 1, 1, 1),
                                       nn.ReLU(inplace=True),
                                       convbn(32, 32, 3, 1, 1, 1),
                                       nn.ReLU(inplace=True))

        self.layer1 = self._make_layer(BasicBlock, 32, 3, 1,1,1)
        self.layer2 = self._make_layer(BasicBlock, 64, 16, 2,1,1) 
        self.layer3 = self._make_layer(BasicBlock, 128, 3, 1,1,1)
        self.layer4 = self._make_layer(BasicBlock, 128, 3, 1,1,2)


        self.branch1 = nn.Sequential(nn.AvgPool2d((32, 32), stride=(32,32)),
                                     convbn(128, 32, 1, 1, 0, 1),
                                     nn.ReLU(inplace=True))

        self.branch2 = nn.Sequential(nn.AvgPool2d((16, 16), stride=(16,16)),
                                     convbn(128, 32, 1, 1, 0, 1),
                                     nn.ReLU(inplace=True))

        self.branch3 = nn.Sequential(nn.AvgPool2d((8, 8), stride=(8,8)),
                                     convbn(128, 32, 1, 1, 0, 1),
                                     nn.ReLU(inplace=True))

        self.branch4 = nn.Sequential(nn.AvgPool2d((4, 4), stride=(4,4)),
                                     convbn(128, 32, 1, 1, 0, 1),
                                     nn.ReLU(inplace=True))

        self.lastconv = nn.Sequential(convbn(320, 128, 3, 1, 1, 1),
                                      nn.ReLU(inplace=True),
                                      nn.Conv2d(128, 32, kernel_size=1, padding=0, stride = 1, bias=False))
    def _make_layer(self, block, planes, blocks, stride, pad, dilation):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
           downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes * block.expansion,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * block.expansion),)

        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample, pad, dilation))
        self.inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(self.inplanes, planes,1,None,pad,dilation))

        return nn.Sequential(*layers)
```

feature를 뽑아내기 위한 기본적인 과정은 위와 같다. _make_layer 함수는 block을 계속 추가하는데, 이 때 block은 BasicBlock이다. 코드는 아래와 같다.

```python
class BasicBlock(nn.Module):
    expansion = 1
    def __init__(self, inplanes, planes, stride, downsample, pad, dilation):
        super(BasicBlock, self).__init__()

        self.conv1 = nn.Sequential(convbn(inplanes, planes, 3, stride, pad, dilation),
                                   nn.ReLU(inplace=True))

        self.conv2 = convbn(planes, planes, 3, 1, pad, dilation)

        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)

        if self.downsample is not None:
            x = self.downsample(x)

        out += x

        return out
```

잘 살펴보면 Basicblock은 residual을 담당하는 부분.

 그렇게 결과적으로 보게 되는 feature는 아래와 같다.

```python
    def forward(self, x):
        output      = self.firstconv(x)
        output      = self.layer1(output)
        output_raw  = self.layer2(output)
        output      = self.layer3(output_raw)
        output_skip = self.layer4(output)


        output_branch1 = self.branch1(output_skip)
        output_branch1 = F.upsample(output_branch1, (output_skip.size()[2],output_skip.size()[3]),mode='bilinear')

        output_branch2 = self.branch2(output_skip)
        output_branch2 = F.upsample(output_branch2, (output_skip.size()[2],output_skip.size()[3]),mode='bilinear')

        output_branch3 = self.branch3(output_skip)
        output_branch3 = F.upsample(output_branch3, (output_skip.size()[2],output_skip.size()[3]),mode='bilinear')

        output_branch4 = self.branch4(output_skip)
        output_branch4 = F.upsample(output_branch4, (output_skip.size()[2],output_skip.size()[3]),mode='bilinear')

        output_feature = torch.cat((output_raw, output_skip, output_branch4, output_branch3, output_branch2, output_branch1), 1)
        output_feature = self.lastconv(output_feature)

        return output_feature
```

 자세히 보면 알겠지만, SPP Net이다. feature extraction에서는 SPPNet(16x16, 8x8, 4x4, 2x2)를 이용해 hierarchical contextual information을 추출한다. 그리고 같은 사이즈로 upsampling을 한 다음 해당 output을 모두 concatenate하게 된다.

```python
    def forward(self, ref, targets, pose, intrinsics, intrinsics_inv):

		# Get Camera Intrinsic Matrix and save them at quarter scale

        intrinsics4 = intrinsics.clone()
        intrinsics_inv4 = intrinsics_inv.clone()
		# why we should it divide by 4 ?
        intrinsics4[:,:2,:] = intrinsics4[:,:2,:] / 4
        intrinsics_inv4[:,:2,:2] = intrinsics_inv4[:,:2,:2] * 4

        refimg_fea     = self.feature_extraction(ref)

		# batch size and height, width
		# There are no channels because depth map doesn't need any channel.
        disp2depth = Variable(torch.ones(refimg_fea.size(0), refimg_fea.size(2), refimg_fea.size(3))).cuda() * self.mindepth * self.nlabel

        for j, target in enumerate(targets): # of targets : "N"
            cost = Variable(torch.FloatTensor(refimg_fea.size()[0], refimg_fea.size()[1]*2, self.nlabel,  refimg_fea.size()[2],  refimg_fea.size()[3]).zero_()).cuda()
            targetimg_fea  = self.feature_extraction(target)
			# Stack Cost Volume
            for i in range(self.nlabel): # of nlabel : "L"
				# why add 1e-16 ??
				# Make "L" warping image(which is space sweep, i.e., virtual image at each camera pose(j) and each depth(i))
                depth = torch.div(disp2depth, i+1e-16)
				# Get inverse_warp Image(== Projected Image to Target Image)
				# Target Image ---(Projection)---> Reference Image
                targetimg_fea_t = inverse_warp(targetimg_fea, depth, pose[:,j], intrinsics4, intrinsics_inv4)
                cost[:, :refimg_fea.size()[1], i, :,:] = refimg_fea
                cost[:, refimg_fea.size()[1]:, i, :,:] = targetimg_fea_t

            cost = cost.contiguous()
            cost0 = self.dres0(cost)
            cost0 = self.dres1(cost0) + cost0
            cost0 = self.dres2(cost0) + cost0 
            cost0 = self.dres3(cost0) + cost0 
            cost0 = self.dres4(cost0) + cost0
			# Cut down Channel to 1 from 32
            cost0 = self.classify(cost0)

            if j == 0:
                costs = cost0
            else:
                costs = costs + cost0

		# costs		= [batch size, 1(channel), N, H, W]
        costs = costs/len(targets)

		# costss	= [batch size, 1(channel), N, H, W]
        costss = Variable(torch.FloatTensor(refimg_fea.size()[0], 1, self.nlabel,  refimg_fea.size()[2],  refimg_fea.size()[3]).zero_()).cuda()

		# What is difference between costs and costss?? Just residual?
		#
		# costs		: Cost Volume for single channel without residual
		# costss	: Cost Volume for single channel with residual (which means residual twice)
		#
		# Cost with residual has 

		# Cost Aggregation
        for i in range(self.nlabel):
            costt = costs[:, :, i, :, :]
            costss[:, :, i, :, :] = self.convs(torch.cat([refimg_fea, costt],1)) + costt

		# Meaning of Upsample	: [B, CH, N, H, W] ===> [N, H, W] (Same as Resize?)

        costs = F.upsample(costs, [self.nlabel,ref.size()[2],ref.size()[3]], mode='trilinear')
        costs = torch.squeeze(costs,1)
        pred0 = F.softmax(costs,dim=1)
        pred0 = disparityregression(self.nlabel)(pred0)
        depth0 = self.mindepth*self.nlabel/(pred0.unsqueeze(1)+1e-16)

        costss = F.upsample(costss, [self.nlabel,ref.size()[2],ref.size()[3]], mode='trilinear')
        costss = torch.squeeze(costss,1)
        pred = F.softmax(costss,dim=1)
        pred = disparityregression(self.nlabel)(pred)
        depth = self.mindepth*self.nlabel/(pred.unsqueeze(1)+1e-16)

        if self.training:
            return depth0, depth
        else:
            return depth
```

 

### References

Im, Sunghoon, et al. "DPSNet: End-to-end deep plane sweep stereo." *arXiv preprint arXiv:1905.00538* (2019).

DPSNet github, [https://github.com/sunghoonim/DPSNet](https://github.com/sunghoonim/DPSNet)