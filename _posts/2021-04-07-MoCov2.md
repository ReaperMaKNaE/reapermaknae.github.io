---
layout : post
title : "MoCo V2"
date : 2021-04-07 +0900
description : SimCLR의 방법을 MoCo에 적용하여 더욱 성능을 향상시킨 MoCo v2를 다룬 논문의 간단한 리뷰입니다.
tag : [PaperReview]
---

### Improved Baselines with Momentum Contrastive learning



 참고로 Technical report이다. 내용은 2 pages에, reference 한 쪽 해서 총 3쪽이 끝이다. 그만큼 내용이 간단하다. 간략하게만 소개하면, SimCLR에서 썼던 기법을 MoCo에도 적용시킨 것이다.



### Abstract

 Conrastive learning이 semi-supervised learning에서 효과적인 결과를 보여주고 있다. 최근은 SimCLR등이 발표되며 supervised와 그 격차를 점점 줄이고 있다. SimCLR에서 사용한 기법인 MLP와 새로운 data augmentation 방법, learning rate 등을 통해 그 결과를 더 잘 뽑아낼 수 있었다.



### Introduction

 최근 self-supervised learning의 기법이 발전하면서, supervised-learning의 수준을 거의 따라잡고 있다. 최근 움직임 중에서, SimCLR이라는 것이 있는데, 여기서 적용한 MLP projection head와 stronger data augmentation 등을 MoCo에 적용했더니 그 성능이 훨씬 좋아진 것을 발견할 수 있었다. 그 결과, SimCLR에 비해 훨씬 효율적인 batch size로 더 좋은 결과를 낼 수 있었다.



### Background

__Contrastive learning.__

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210407-1.PNG)

 위 그림에서 좌측은 End-to-End 기법을 의미하고(SimCLR version) 우측은 MoCo이다. MoCo의 경우 queue와 batch의 개념을 효율적으로 융합해서 batch size를 줄이는 것에 효과적이었다.

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210407-2.PNG)

 위 그림은 InfoNCE라 불리는 loss이다. 자세한 내용은 MoCo 참조.

__Improved designs.__

 SimCLR이 좋은 성능을 냈던 것은 아래 이유와 같다.

- 엄청난 크기의 batch size로 negative sample을 더욱 잘 준 것
- output FC projection head를 MLP head로 변경하는 것
- stronger data augmentation

 여기서 MoCo framework에 적용할 수 있는 것들은 다 적용해서 성능 향상을 불러왔는데, 그 결과는 아래와 같다.



### Experiments

__Settings__

 evaluation의 경우 2가지 protocol을 따랐다. 첫번째는 ImageNet linear classification이고, 두번째는 VOC object detection에 transferring을 하는 것이다. 모든 실험은 ResNet-50을 backbone으로 사용했다.

__MLP Head.__

 SimCLR을 따라서, fc head를 2-layer MLP head로 변경했다. 이는 오로지 unsupervised learning stage에서만 사용이 된 것이고, evaluation에는 전혀 사용하지 않았다. temperature hyperparameter를 MLP Head에 따라 조사한 결과는 아래와 같다.

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210407-3.PNG)

 temperature hyperparameter가 0.07인 경우에는 MLP의 적용에 따른 accuracy 상승이 2.3%이고, 0.2인 경우에는 그 상승이 59%에서 66.2%로 무려 7.2%의 상승이 있었다.

__Augmentation.__

 SimCLR에서 적용한 몇몇 augmentation 중에서, stronger color distortion의 경우 diminishing gain이 생기는 것을 확인하였다. 하지만 이러한 data augmentation을 적용한 결과 그 성능의 향상이 있었다.(그래서 추가한 data augmentation은 basic data augmentation에 blur augmentation이라고 함.)

MLP와 함께 이를 정리한 table은 아래와 같다. VOC detection의 경우, 5번의 run에서 mean을 구한 것이다. cos의 경우 cosine learning rate schedule로, SimCLR에서 적용한 learning rate이다.

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210407-4.PNG)

__Comparison with SimCLR.__

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210407-5.PNG)

 SimCLR과 비교한 결과이다. 훨씬 적은 수의 batch, epoch에도 불구하고 ImageNet Acc.가 더 높은 것을 확인할 수 있다.

__Computational cost.__

![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210407-6.PNG)

 8대의 V100 16G GPUs로 test 한 결과라고 한다. ~~(8대라고..?)~~  end-to-end(SimCLR)에 비해 훨씬 효율적인 것을 확인할 수 있다.



### Conclusion

 (따로 discussion이나 conclusion 항목이 없다. 그냥 technical report라서 그런듯. 그래서 임의로 Conclusion이라는 것을 붙임)

 위 table에서 살펴보면 굳이 큰 size의 batch가 필요없다는 것을 알 수 있다.(성능을 위해서) 추가한 것들이 크게 없기 때문에, MoCo v1에서 조금의 변경으로도 구현이 가능하다. 이러한 improvement들이 future work들에 잘 적용이 되길 빈다.



### Reference

Chen, Xinlei, et al. "Improved baselines with momentum contrastive learning." *arXiv preprint arXiv:2003.04297* (2020).