---
layout : post
title : "GCNet"
date : 2021-01-09 +1154
description : 최초로 3D Cost Volume을 도입한 GCNet을 propose한 End-to-End Learning of Geometry and Context for Deep Stereo Regression의 간단한 리뷰입니다.
tag : [PaperReivew]
---

### End-to-End Learning of Geometry and Context for Deep Stereo Regression, ICCV 2017



 사실 이 친구를 먼저 review했어야 했는데, 순서가 조금 뒤죽박죽입니다.

아무튼 최초로 3D Cost volume이란 것을 제안한 GCNet의 간단한 리뷰입니다.



 추가로 해당 논문의 network를 구현한 코드(pytorch ver)는 아래 github에서 확인이 가능합니다.

[https://github.com/dl19940602/GCnet-pytorch](https://github.com/dl19940602/GCnet-pytorch)



### Abstract

rectified pair of stereo image에서 disparity를 regressing하는 매우 novel한 architecture를 소개할 것이다.



### Introduction

 자율주행이나 UAV등 stereo image에서 geometry를 얻어오는 것은 굉장히 핫한 상태.(2017년 기준)

기존의 여러 방식이 있었는데, 여하튼 우리는 아래 두 가지 키 아이디어가 있음.

첫번째는 3d cost volume을 이용했음.

두번쨰는 argmin function을 썼음. 매우 differentiable하지.

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210109-6.png)

 위 사진이 네트워크의 전체적인 구조인데, 맨 마지막에 Soft ArgMax라고 써져있는 게 아니라 argmin이라고 써져있어야하는 것을 잘못 적은 것 같다.



### Related Work

SGM, Graph Cut 같은 과거에 사용하던 알고리즘들이 나옴. 추후 시간이 나면 리뷰는 해볼 예정.



### Learning End-to-End Disparity Regression

GC-Net(Geometry and Context Network)의 요약은 다음과 같음.

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210109-7.png)

 이후 코드 리뷰에서 자세히 살펴볼 예정.

__3.1 Unary Features__

이건 약간 CRF에서 사용되는 개념같은데, pixel wise, pair wise, unary 등으로 나뉘고, 이 중에서 pair wise는 인접한 픽셀은 비슷한 클래스를 가진다는 개념, unary는 확률 정보를 가지고 있는 term... 등의 표현.

 그냥 feature map 뽑는거라고 이해하면 편하듯 하다.

__3.2 Cost Volume__

 위에서 뽑힌 확률 기반 feature로 cost volume을 만들게 됨.

__3.3 Learning Context__

 대충 network 학습 시킬 때에 어떤 필터로 어떤 방식을 왜 썼는지에 대한 내용.

__3.4 Differentiable ArgMin__

기존에 사용하던 argmin의 문제점은 다음과 같음.

첫번째로, discrete하고 subpixel disparity추정을 만들 수 없음.

두번째로, differentiable하질 않아서 back-propagation을 못함.

이러한 한계를 방지하기 위해, 아래와 같은 soft argmin을 propose함.

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210109-8.png)

여기서 d는 disparity이고, c_d는 해당 disparity에 대한 predicted cost, sigma는 softmax operation. 이걸 d에 대해 weight를 줘서 곱한 것이 위와 같음.

비슷한 형태가 아래 논문에서 소개된 적이 있다고 하나,(soft attention mechanism으로) 많이 다른 듯 하다.

Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. "Neural machine translation by jointly learning to align and translate." *arXiv preprint arXiv:1409.0473* (2014).

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210109-10.png)

 그런데 한 가지 문제가 있다. 여러가지 모드를 고려하였을 때, 가끔 에러를 나타내는 것. 여기서 modal이라는 것이 나오는데, 이는 통계용어로, detect하는 것의 종류를 의미. 말이 이상한 것 같다.

 PN님 께서 포스팅하신 [[기초통계] Unimodality란?(Unimodal Probability Distribution 개념)](https://m.blog.naver.com/PostView.nhn?blogId=sw4r&logNo=221423830022&proxyReferer=https:%2F%2Fwww.google.com%2F)를 한번 보는 것이 좋은 것 같다.

 대충 target하는 mode가 여러개인 경우 soft argmin이 제대로 된 detection을 못한다는 뜻인데, 이 때에는 cost curve를 pre scale하게 되면 해당 문제를 해결할 수 있다고 한다. 이러한 현상의 원인으로는 softmax probability가 uni-modal result를 조금 더 극적으로? 만드는 경향이 있기 때문이라고 함.



__3.5 Loss__

regression loss는 다음과 같음.

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210109-9.png)



### Experimental Evaluation

여러 dataset에 대해서 실험을 해 봤음.

__4.1 Model Design Analysis__

__4.2 KITTI Becnchmark__

__4.3 Model Saliency__

Saliency check도 진행했는데, 얼마나 model이 감각적으로? 잘 맞게 한지 보여주는 방법.



### Conclusions



### Code Review

 pytorch로 쓰여져있으며, 해당 코드는 아래 github에서 확인 가능하다.

 [https://github.com/dl19940602/GCnet-pytorch](https://github.com/dl19940602/GCnet-pytorch)

모델 자체는 크게 복잡하지 않다.

 ResNet50 network에다 cost volume만 집어넣은 형태이다.

 아래는 network를 만드는 pytorch code.

```python
class ResNet(nn.Module):
	# in Resnet 50, input is : ResNet(BottleNeck, ContextBlock2d, [3, 4, 6, 3], gc=True)
	
	# block		: BottleNeck
	# block1	: ContectBlock2d
	# num_block	: [3, 4, 6, 3]

	# Visualized Architecture for ResNet50
	# https://www.researchgate.net/figure/The-architecture-of-ResNet50-and-deep-learning-model-flowchart-a-b-Architecture-of_fig1_334767096

    def __init__(self, block, block1, num_block, num_classes=100, gc=False):
        super().__init__()

		# the plane means channel.
		# inplanes	: # of input channel
		# planes	: # of output channel

        self.inplanes = 64

        self.conv1 = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True))
        #we use a different inputsize than the original paper
        #so conv2_x's stride is 1
		"""
		the input channel would have 4 times size
		initial value is 64.
		after that, 64 * 4 = 256
		conv2_x		: (input_channel, output_channel, # of blocks, stride) = (256, 64, 3, 1)
		conv3_x		: (input_channel, output_channel, # of blocks, stride) = (256, 128, 4, 1)
		conv4_x		: (input_channel, input_channel2, output_channel, # of blocks, stride) = (512, 256, 6, 1)
		conv5_x		: (input_channel, output_channel, # of blocks, stride) = (1024, 512, 3, 1)
		"""
        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)
        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)
        self.conv4_x = self._make_layer1(block, block1, 256, num_block[2], 2, gc=gc)
        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)
        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512 * block.expansion, num_classes)

    def _make_layer(self, block, planes, blocks, stride=1):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes * block.expansion,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * block.expansion),
            )

        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample))
        self.inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(self.inplanes, planes))
        
        return nn.Sequential(*layers)


    def _make_layer1(self, block, block1, planes, blocks, stride=1, gc=False):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes * block.expansion,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * block.expansion),
            )

        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample))
        self.inplanes = planes * block.expansion
        for i in range(1, blocks-1):
            layers.append(block(self.inplanes, planes))
        layers.append(block1(self.inplanes, self.inplanes))
        layers.append(block(self.inplanes, planes))

        return nn.Sequential(*layers)


    def forward(self, x):
        output = self.conv1(x)
        output = self.conv2_x(output)
        output = self.conv3_x(output)
        output = self.conv4_x(output)
        output = self.conv5_x(output)
        output = self.avg_pool(output)
        output = output.view(output.size(0), -1)
        output = self.fc(output)

        return output 
```





### References

Kendall, Alex, et al. "End-to-end learning of geometry and context for deep stereo regression." *Proceedings of the IEEE International Conference on Computer Vision*. 2017.

GCNet Github, [https://github.com/dl19940602/GCnet-pytorch](https://github.com/dl19940602/GCnet-pytorch)

Modal, [기초통계] Unimodality란?(Unimodal Probability Distribution 개념), [[기초통계] Unimodality란?(Unimodal Probability Distribution 개념)](https://m.blog.naver.com/PostView.nhn?blogId=sw4r&logNo=221423830022&proxyReferer=https:%2F%2Fwww.google.com%2F)