---
layout : post
title : "3D-R2N2"
date : 2021-01-13 +1722
description : 3D-R2N2, A unified Approach for Single and Multi-view 3D Object Reconstruction 논문의 간단한 리뷰입니다.
tag : [PaperReview]
---

### 3D-R2N2: A unified Approach for Single and Multi-view 3D Object Reconstruction, ECCV 2016



 summary: 3D LSTM/GRU를 적용하였고, the number of input(single~MVS)에 상관없는 network를 설계하여 3D recon을 이룬 논문.

 3D LSTM/GRU를 network에 적용한 것이 key라고 할 수 있다.



### Abstract

 최근 3D recon 방법들에 대한 영감을 받아, 3D Recurrent Reconstruction Neural Network를 propose함. 한 가지 물체를 다양한 곳에서 촬영한 large collection으로 학습한 network. 임의의 각도에서 촬영된 한 장 혹은 여러 장의 image를 input으로 받아 3D occupancy grid로 만들어진 output을 내뱉음. 다른 네트워크와 달리 annotation이나 class label이 필요없음. traditional SfM이나 SLAM이 실패한 곳에서 3D recon을 하는 것에 성공함.



### Introduction

 자동으로 3D object를 만드는 것은 여러 분야에서 쓰여왔는데, 3D printing이 생기고 나서 boost되었음. 최근 3D object recon에 여러 방법들이 시도되었지만, 한계가 있었음.

- object가 정말 많은 각도에서 촬영되었어야 함. 그리고 이 각도가 크면 안 됨.
- object의 겉면에 반사가 잘 없어야하고, texture가 풍부해야함.

 large baseline과 반사가 거의 없는 표면과 관련된 문제를 피하기 위해 space carving이나 probabilistic extension과 같은 3D volumetric recon이 인기를 끌었다. 하지만 이런 방법들은 물체가 배경과 확실하게 독립되어있거나 camera가 calibrate 되어 있어야 했다.

 또 다른 관점에서는 기존에 알고 있던 object에 대한 지식이나 형태가 사용가능하다는 점이다. 이러한 것은 여러 view에서 연관성을 찾는 것에 대한 의존성을 줄였다.

 우리가 여기서 언급할 방법도 위와 비슷하지만, 포인트는 다르다. 가능한, 적합한 3D shape을 matching하려고 하는 시도 대신, deep convolutional neural network를 이용해서 이런 방대한 내용을 학습시키는 것을 선택했다. 

 이러한 방법으로 최소한 적은 image로부터 3D object recon을 할 수 있도록 했다.

 최근 진행하고 있는 single view 3D recon과 LSTM network의 성공에 영감을 받아, 우리는 이번에 3D-R2N2를 제안한다. 이 network는 한장 혹은 그 이상의 image를 input으로 받고 3D occupancy grid로 찬 형태의 output을 낸다.

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210113-9.PNG)

 3D-R2N2의 key는 input gate와 forget gate의 조절임. 학습 중에는 이러한 방법이 3D recon에서 적합하고 연속성있는 3D representation을 선택할 수 있도록 도와줌.

 우리의 main contribution은 다음과 같음.

- standard LSTM framework을 확장한 3D R2N2를 제안함.
- 한 framework에 single 혹은 multi-view 3D recon을 통합했음.
- 다 필요없고, minimal supervision만 training과 testing 과정에 필요함.
- single-view recon에서 SOTA
- traditional SfM/SLAM 방식이 실패한 3D recon을 가능하게 했음.

section 2에서는 LSTM과 GRU를 간단하게 소개, section 3에서는 3D R2N2 architecture 소개, section 4에서는 어떻게 training/test 했는지, 마지막은 PASCAL 3D와 ShapeNet dataset에서의 결과를 보여줄 계획임.



### Recurrent Neural Network

 우린 여기서 LSTM과 GRU에 대해 간단히 소개할 예정.

__Long Short-Term Memory Unit__

__Gated Recurrent Unit__

(둘 다 Computer Vision with Machine Learning에서 쓰이는 용어들 포스팅 참조)



### 3D Recurrent Reconstruction Neural Network

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210113-10.PNG)

 main idea는 이전의 observation을 유지하고, observation이 더욱 좋아지도록 output recon을 refine하는 LSTM를 저울질 하는 것임.

 네트워크는 크게 3가지 성분으로 구성되어 있음.

- 2D CNN
- LSTM(3D-LSTM)
- 3D DCNN

  2D CNN이 feature를 함축하고, LSTM이 가능성있는 feature들을 골라낸 다음, 3D DCNN이 LSTM의 output을 해석하고 3D probabilistic voxel recon을 생성함.

 (여기서 LSTM이 하는 역할은 occlude된 부분은 제끼는 것과 유사.)

__3.1 Encoder: 2D-CNN__

 위 Figure 2에서 보면 알겠지만, Encoder가 2개임. standard feed-forward CNN과 deep residual variation임.

__3.2 Recurrence: 3D Convolutional LSTM__

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210113-11.PNG)

 우리 network에서 core part는 위 이미지에 해당하는 부분임. 위와 같이 큰 형태의 input을 그 어떤 regularization 없이 쓰는 것은 어려운 task라고 할 수 있음. 그래서 우리는 새로운 architecture인 3D-Convolutional SLTM을 제안함. 이 network는 기존 LSTM unit들로 구성이 되어 있음. 3D LSTM에서의 notation과 식은 다음과 같음.

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210113-12.PNG)

 3D LSTM unit은 i,j,k로 index되고, T는 tensor이다. 4D tensor(NxNxN)에서 N은 4를 썼다. 기존의 LSTM과 다르게 output gate가 없는데, 마지막에 한번만 빼내기 때문이다. 이 덕분에 parameter도 좀 줄일 수 있었다.

  평범한 LSTM과 3D LSTM의 차이로는, 평범한 LSTM의 경우 hidden layer에 있는 모든 element들(h_(t-1)의 모든 elements)들이 현재 hidden state인 h_t에 영향을 끼치지만, 3D convolutional 3D-LSTM에서 hidden state의 h_t(i,j,k)는 그 이전 3D LSTM (i,j,k) unit의 neighboring 3D-LSTM unit에만 영향을 받는다. 조금 더 구체적으로, neighboring의 크기는 kernel의 크기와 같다. 따라서 unit들은 서로 weight를 share할 수 있고 network는 좀 더 regularize될 수 있다.

 이것 말고도 GRU base로도 만들었다. 이건 다음과 같다.

  ![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210113-13.PNG)

__3.3 Decoder: 3D Deconvolutional Neural Network__

input image sequence를 받은 다음, 3D-LSTM은 hidden state h_T를 decoder에 전달해준다.(image sequence에 image가 T개 만큼 있다면, hidden state는 h_T) decoder는 여기서 3D convolution, non linearities(??), 3D unpooling을 통해 target output resolution까지 resolution을 끌어올린다.

 encoder와 마찬가지로, decoder도 간단한 형태와 복잡한 형태 두 가지를 제공한다. 마지막 layer에서는 final activation을 voxel-wise softmax를 이용해서 voxel cell의 occupancy probability로 변환한다.

__3.4 Loss: 3D Voxel-wise Softmax__

 Loss function은 voxel-wise cross-entropy의 합으로 정의된다.

  ![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210113-14.PNG)

 위에서 y(i,j,k)는 GT occupancy이고, each voxel (i,j,k)에서의 final output은 bernoulli distributions인 [1-p(i,j,k), p(i,j,k)]를 따른다. 위에서 large gamma(흐물흐물한 X)는 input을 말한다.



### Implementation

__Data augmentation__

 학습에 있어서 우리는 3D CAD model을 썼다.

__Training__

 input의 수는 상관이 없다. 이를 어떻게 구현했는지 구체적으로 보면, mini-batch의 종류에 따라 training input length는 다르다.

__Network__

 input image는 127x127이고, output은 32x32x32임.



### Experiments

__5.1 Dataset__

__ShapeNet__

__PASCAL 3D__

__Online Products__

__MVS CAD Models__

__5.2 Network Structures Comparison__

![img7](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210113-15.PNG)
 결과는 encoder decoder 모두 residual에 recurrent network는 GRU를 쓴게 성능이 가장 좋았음. 이 모델을 이제 Res3D-GRU-3 이라고 칭함.

__5.3 Single Real-World Image Reconstruction__

![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210113-16.PNG)

__Training__

__Results__

__5.4 Multi-View Reconstruction Evaluation__

__Experiment setup__

 Res3D-GRU-3 모델을 써서 참여.

__Overall results__

__Per-category results__

__Qualitative results__

![img9](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210113-17.PNG)

__5.5 Reconstructing Real World Images__

__5.6 Multi View Stereo(MVS) vs 3D-R2N2__

__Dataset__

__Experiment setup__

__Results__



### Conclusions



### Acknowledgement



### References

Choy, Christopher B., et al. "3d-r2n2: A unified approach for single and multi-view 3d object reconstruction." *European conference on computer vision*. Springer, Cham, 2016.

