---
layout : post
title : "DETR"
date : 2021-03-24 +1100
description : Object detection에 Transformer을 통째로 넣어 좋은 성능을 낸 DETR 논문의 간단한 리뷰입니다.
tag : [PaperReview]
---

### End-to-end object detection with Transformers, ECCV 2020



### Abstract

 Object detection에 있어서 Full transformer를 pure하게 사용한 첫 논문이다. backbone network, FFN을 추가로 사용하였으며, object detection뿐 아니라 panoptic segmentation에서도 그 성능을 뽐냈다.



### Introduction

 Faster RCNN에 비해서 small object보다는 성능이 떨어지지만, large object에 대해서는 그 성능이 더 좋았다. 

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-46.PNG)



### Related Work

__2.1 Set Prediction__

 prediction 과정에서 문제가 생기게 된 것은, 바로 matching이다. 기존의 경우 class에 FC layer를 달아서 결과를 뽑아냈는데, 마지막으로 뽑아낸 output에 regress를 추가하지 않으려면 새로운 방법이 필요했다. 그래서 DETR에서 생각한 방법은 bipartite matching으로, 이를 적용하기 위해 hungarian algorithm을 적용했다. 이는 permutation-invariance(순서에 상관없이, 아무리 섞어도 결과가 같다는 뜻)하고, target이 unique하게 match되는 것을 장려하여 그 결과가 자연스레 좋아지게 된다.

__2.2 Transformers and Parallel Decoding__

__2.3 Object Detection__

 내용 생략.



### The DETR model

 직접적으로 object detection을 수행하기 위해선 두 가지가 필요하다.

- Set prediction loss
- Architecture

__3.1 Object detection set prediction loss__

 DETR은 pure transformer를 사용한 모델로, decoder를 통해 나가는 길이 single path이다. 따라서, loss를 최적화 하는 것이 좀 중요한데, 저자의 경우는 optimal bipartite matching을 위해 hungarian algorithm을 썼다고 한다. 해당 algorithm은 konig's theorem에서 유래하였으며, 1:1 matching에 있어서 효과적인 algorithm이다. 이 알고리즘을 이용해서, 가장 loss가 작은 matching을 찾아 $\sigma$ 에 저장한다.

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-47.PNG)

 이렇게 찾아낸 matching에서, class에 대해서는 negative log-likelihood 를 취했고, bbox에 대해서는 일반적인 $l_1$ loss와 GIoU loss를 더해서 사용했다고 한다.

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-48.PNG)

 아래는 bbox loss.

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-49.PNG)

__3.2 DETR architecutre__

구조는 간단하다고 한다. 아무 CNN backbone에, encoder-decoder transformer를 붙이고, FFN을 마지막에 덧붙이면 끝이라고 한다. CNN을 거치고 나온 feature map은 positional encoding을 통해서 transformer encoder의 input으로 보내버리고, 나온 output을 object query와 함께 transformer decoder로 넘겨준다. 마지막에 나온 결과를 FFN 통과시켜서 최종 object detection을 마치게 된다.

 각 내용에 대한 것들을 위로 함축시킨 것으로, 아래 image 한장으로 대체함.

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-50.PNG)

 object query는 뭔데?? 하면, 각 object를 잡을 수 있도록 만들어주는... 것이다. 이건 말로 뭔가 표현하기가 좀 힘든데, 아무튼 추후 appendix에서 자세하게 다루게 된다.

__Auxiliary decoding losses.__

 마지막에 Auxiliary decoding loss를 추가했다고 한다. 뭐 어디에 나와있진 않음. 코드를 봐야 할 듯.



### Experiments

__Dataset__

 dataset으로는 COCO 2017 detection and panoptic segmentation dataset을 사용했다고 한다.

__Technical details.__

 AdamW setting, lr은 $10^{-4}$ , weight decay는 lr과 동일, 모든 weight는 Xavier init(그냥 랜덤 init), backbone으로는 ImageNet-pretrained ResNet model을 사용했다고 한다. ResNet은 기본적으로 ResNet 50을 말하는데, 여기서 다른 backbone들과의 비교를 위해 ResNet-101을 사용한 모델을 DETR-R101이라고 variation을 추가했다. 뿐만 아니라, receptive field를 키울 목적인지 dilated convolution을 ResNet의 c5에 추가한 케이스를 더했는데, 각 model을 DETR-DC5, DETR-DC5-R101 이라고 한다.

__4.1. Comparison with Faster R-CNN__

 그 결과는 아래와 같다.

![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-51.PNG)

 그 결과, 작은 object에 대해서는 그 성능이 좋지 않았다.(아무래도 이전 BoTNet에서는 small object에 대해서 강하다고 말했는데, DETR이 나오면서 transformer가 small object에는 약하다는 인식이 어느정도 박혀있는 것 같다. ~~물론 아닐수도. 모든 것은 저의 뇌피셜~~ )

__4.2 Ablations__

__Number of encoder layers.__

![img7](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-52.PNG)

 위는 encoder에 있는 layer의 개수에 따른 성능표. 저자는 왜 저렇게 성능차이가 많이 나는가 했더니, 알고보니 encoder가 distinguish하는 역할을 한다고 생각했다. 그 증거로 아래 그림을 투척했다.

![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-53.PNG)

 정말 놀라운 그림이다. 각 object들마다 잘 골라낸 것을 확인할 수 있다.

__Number of decoder layers.__

 그럼 decoder는 뭐하냐? 했는데, 알고보니 NMS의 역할을 수행하고 있었다. 그래서 decoder layer마다 NMS를 쓴 경우와 쓰지 않은 경우를 비교해봤더니, 아래와 같다.

![img9](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-54.PNG)

 경우에 따라 다르긴 한데, 전체적인 AP를 봤을 땐 오히려 NMS를 쓸 때 성능이 더 떨어지는 것이 나왔다.(!!) 그래서 해당 과정이 필수인 것. 그럼 encoder처럼 visualization을 한 것은 없을까?

![img10](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-55.PNG)

 물론 있다. encoder는 feature map에서 object를 detection하는 것을 잘 한다면, decoder는 object의 가장자리를 잘 잡는 것을 확인할 수 있다.

__Importance of FFN__

 FFN을 쓴 덕에 성능이 향상되었다고 한다. FFN을 빼면 2.3AP가 떨어진단다.

__Importance of positional encodings.__

 DETR에서 사용한 positional encoding은 두 종류가 있다고 한다. 하나는 spatial positional encoding이고, 다른 하나는 output positional encoding(object query)라고 한다. 이에 대해 비교한 것은 아래 table에 나와있다.

![img11](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-56.PNG)

 sin, cos에 대해서 알았는데 까먹음. 고로 생략.(추후 기억이 난다면 수정계획)

__Loss ablations__

 loss는 총 3개를 합했다고 한다. 하나는 classification loss, bbox distance loss, GIoU loss이다. 이 loss들에 대한 ablation을 수행한 결과는 다음과 같다. (class는 무조건 사용한다는 가정하에)

![img12](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-57.PNG)

__4.3 Analysis__

__Decoder output slot analysis__

![img13](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-58.PNG)

 위 그림은 decoder의 output을 골라내서 살펴본 것이다. 즉, object query에 대한 내용이 들어가 있는데, 해당 그림을 살펴보면 녹색은 작은 bbox, 붉은 색은 bbox의 상하, blue는 bbox의 좌우를 잡는 것을 확인할 수 있다. visualization을 해서 그렇지, 100개 중에서 무작위로 뽑아낸 20개가 서로 어떤 것을 특정해서 찾는 것을 보면, object detection에서 object query가 무언가를 하고 있다는 것을 알 수 있다.

__Generalization to unseen numbers of instances.__

![img14](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-59.PNG)

 위 그림은 class의 형평성을 위해서 giraffe를 여러개 붙인 건데, 관찰할 수 있듯 같은 물체를 같은 물체라고 인식을 할 수는 없다. 이는 attention의 한계로, decoder에서 나오는 head의 개수만큼 detection을 하는데 서로 다른 head에서 나온 것이 같다고는 분류하기 힘들다. 아마도 이는 추가적인 model이 필요할 듯 하다. 여하튼, 이런 점을 비추어 봤을 때 그 어떤 object query도 한 쪽으로 편향되어 있지 않다는 것을 저자들이 깨달았다고 한다.(overfitting이 일어나지 않는다 라는 뜻 같음)

__4.4 DETR for panoptic segmentation__

 위와 같은 이유로 semantic segmentation이 불가능하기 때문에, panoptic segmentation을 했다.

![img15](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-60.PNG)

 결과를 보면 위와 같다. 나온 결과에 FPN style의 CNN을 씌워서 logits에 mask를 씌우고, visualization을 하는 것.

![img16](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-61.PNG)

 그 결과를 visualization한 것은 위와 같다.

 training을 할 때에는 DICE/F-1 loss와 Focal loss를 사용했다고 한다.

__Training details.__

 DETR, DETR-DC5, DETR-R101 model을 COCO dataset에서 사용했다고 한다.

__Main results__

![img17](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-62.PNG)

 panoptic quaility를 지표로 사용했다. 참고로 PQ가 panoptic quaility이고, 이는 SQ와 RQ의 곱이라고 한다. 이는 식을 봐야하는데, 검색해보면 금방 나오므로 일단 여기서는 pass. 표에서도 확인할 수 있듯이 그 성능은 상당히 준수함을 확인할 수 있다.



### Conclusion



### Acknowledgements



### Appendix

__A.1. Preliminaries: Multi-head attention layers__

multi head self attention에 대한 내용이다. transformer에 대한 공부를 하였으면 쉽게 이해할 수 있는 항목이므로 pass.

__A.2. Losses__

 여기서 batch마다 normalization을 하지 않고 layer normalization을 한 이유가 나온다. 그래서 sub-batch 마다 normalization을 진행하게 되는데, 이는 GPU에 조금 더 맞는 방식이라고 한다.

__Box loss__

__DICE/F-1 loss__

 자세한 내용은 논문의 appendix 참조.

__A.3. Detailed architecture__

 자세한 architecture라고 하나, 사실상 transformer 복습이다.

![img18](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-63.PNG)

 위와 같이 만들었는데, 그냥 transformer다. 이후 complexity와 FLOPs에 대한 computation이 나오는데, 해당 내용은 생략.

__A.4. Training hyperparamters__

 해당 내용도 생략. training details에 들어갈 내용이 있다고 보면 된다.

__A.5. Additional results__

![img19](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-64.PNG)

 panoptic segmentation에서 실패한 케이스도 있다고 한다. 물론 위 그림에 보이는 것 처럼 잘 되는 case도 있다. 위 그림의 경우, 좌측에서 부터, GT, PanopticFPN with ResNet-101, DETR with ResNet이라고 한다.

__Increasing the number of instances__

![img20](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-65.PNG)

 instance의 개수를 한 화면에 무지막지하게 때려 넣어봤다고 한다. 결과는 AP와 비슷한 개수의 instance가 detect가 되고, 결국 나중에 가면 갈수록 그 에러가 커지는 것을 볼 수 있다. 이는 아무래도 Head의 개수가 정혀져있기 때문이 아닐까 싶은데, 그 숫자를 늘리면 더 성능이 좋아질 것으로 보인다.

__A.6. Pytorch inference code__

 transformer를 그대로 땡겨와서 썼기 때문에 코드도 간단하다. 코드는 직접 참조하는 것을 권장.



### Reference

Carion, Nicolas, et al. "End-to-end object detection with transformers." *European Conference on Computer Vision*. Springer, Cham, 2020.

