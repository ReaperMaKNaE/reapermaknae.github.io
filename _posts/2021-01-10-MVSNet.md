---
layout : post
title : "MVSNet"
date : 2021-01-10 +2000
description : MVSNet, Depth Inference for Unstructured Multi-view Stereo에 대한 간단한 리뷰입니다.
tag : [PaperReview]
---

### MVSNet: Depth Inferences for Unstructured Multi-view Stereo, ECCV 2018



### Abstract

 처음엔 deep visual image feature들을 뽑고, differentiable homography warping으로 reference camera frustum에 기반한 3D cost volume을 만든 다음, 3D convolution을 진행해서 initial depth map을 regularize/regress해서 output을 만들어 냈다.



### Introduction

 Overlapping image에서 dense representation을 뽑아내는 것은 지속되어 왔다. 전통적인 방법들도 있지만, 최근 CNN에서도 성공을 보이고 있다.

 여러 연구들이 있었지만, 2 view- stereo에서 multi-view scenarios로 학습을 확장시키는 것은 사소한 문제가 아니다. 2 view-stereo에서 matching점을 찾아 연결하고 이걸로 depth map을 찾아 나서는 것을 그대로 multi view stereo에 적용시켰다간 less accurate result에 도달하게 된다. 이를 감안하고 CNN에 도입한 것으로는 SurfaceNet이나 LSM(Learned Stereo Machine)이 있는데, 둘 다 volumetric representation을 exploit한다. 이건 엄청난 memory consumption을 유발하게 된다.

 이걸 끝내기 위해 우린 whole 3D scene을 한꺼번에 rendering 하는 것 보다 depth map을 한 단계씩 계산하는 것을 바탕으로 둔 MVSNet을 propose한다.

 differentiable homography warping의 key는 2D image feature에서 3D cost volume을 이끌어 낼 수 있는 camera geometry들과 end-to-end training이 가능하다는 점이다.

 SurfaceNet과 LSM과는 다음과 같은 다른 점이 있다.

- regular Euclidean space가 아닌 camera frustum 기반의 3D cost volume
- large scale recon이 가능한 per-view depth map estimation



### Related Work

__MVS Reconstruction__

__Learned Stereo__

__Learned MVS__



### MVSNet

 ![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210110-29.PNG)

 전체적인 구조는 위와 같이 생겼다.

__3.1 Image Features__

 첫번째 step은 N개의 input에 대해서 deep feature들을 뽑는 것이다.(dense matching을 위해)

__3.2 Cost Volume__

 SurfaceNet과 LSM에서 regular grid를 사용한 것에 반해, 우리는 reference camera frustum에 의한 cost volume을 만들게 되었다.

__Differentiable Homography__

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210110-30.PNG)

  warped feature map인 V_i(d)에서 reference feature map F_i는 위와 같은 Homography로 나타낼 수 있다.  warping process는 traditional 한 plane sweep 방법(DPSNet에서 썼던 방법)과 유사하나, differentiable bilinear interpolation은 feature map에서 쓰였지 Image에서 쓰이진 않았다.

__Cost Metric__

 multiple feature volume을 한개의 cost volume으로 만드는 aggregate은 아래와 같다.

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210110-31.PNG)

 여기서 V_bar는 all feature volume, all operations에 대해서 average volume을 의미한다.

__Cost Volume Regularization__

 raw cost volume은 noise에 오염될 수 있고, depth map을 만들기 위해서는 smoothness constraint가 필요하다. 우린 cost volume regularization에 multi-scale 3D CNN을 적용했는데, 이러한 방법은 3D version UNet과 좀 비슷하다.(neighboring information을 aggregate하기 위해 상대적으로 low memory, computation cost임에도 large receptive field를 얻어오는 구조)

 이렇게 만들어진 cost volume은 per-pixel depth estimation뿐 아니라 estimation confidence 측정도 가능하다.

__3.3 Depth Map__

__Initial Estimation__

 probability volume P에서 depth map D를 찾는 가장 간단한 방법은 winner-take-all이다. 하지만, argmax operation은 sub-pixel estimation이 불가능하고, back propagation이 되지 않기 떄문에 probability weighted sum을 쓰기로 했다.

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210110-32.PNG)

 위 식에서 P(d)는 depth d에 대해서 모든 pixel들의 probability estimation을 의미한다.

 위 식을 어디서 봤을텐데, GCNet에서 썼던 그 식이 맞다. 

__Probability Map__

 ![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210110-33.PNG)

 GCNet에서 soft argmin을 사용할 때 unimodal이 아니면 제대로 된 값을 못 찾아갔던 것 처럼, 여기에서도 그런 현상이 일어나게 되었는데, 해당 논문에서는 가장 가까운 4개의 depth hypotheses를 모두 더해서 확률로 취했더니, (d)처럼 매우 좋은 결과를 얻을 수 있었다고 한다. 이 방법 말고 standard deviation이나 entropy같은 것들도 있었으나, 모두 significant improvement를 보여주진 못했다. 이런걸 떠나서 outlier filtering에서도 훨씬 좋은 결과를 얻었다.

__Depth Map Refinement__

 probability volume에서 depth map이 나올 때, semseg이나 image matting에서 발생하는 것과 유사한 문제인, large receptive field때문에 생기는 oversmoothing이 reconstruction boundary에서 발생했다. 최근 image matting algorithm에 영감을 받아, MVSNet의 끝에 depth residual learning network을 추가했다.

__3.4 Loss__

initial depth map과 refined depth map의 loss 두 가지 모두가 고려되었다.

 ![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210110-34.PNG)

 위 식에서 람다는 1로 결정했다.



### Implementations

__4.1 Training__

__Data Preparation__

DTU dataset 사용. mesh surface를 만들기 위해 SPSR(screened Poisson surface reconstruction) 사용

__View Selection__

image pair에서 어떤 view를 선택하는가는 아래 식을 이용했다.

 ![img7](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210110-35.PNG)

s(i,j)가 score이고, 해당 score를 구하기 위해서 gaussian function을 사용했다. 우리가 실험에서 사용한 value 설정은 (\theta0, sigma1, sigam2) == (5, 1, 10) 이다.

Gauss Function: [https://en.wikipedia.org/wiki/Gaussian_function](https://en.wikipedia.org/wiki/Gaussian_function)

__4.2 Post Processing__

__Depth Map Filter__

 dense point clouds로 바꾸기 전에, outlier를 filter로 쳐내는 과정이 필요했다. 우리는 filter에 있어서 두가지 기준을 잡았다.

- photometric consistency
- geometric consistency 

 첫번째로 photometric consistency의 경우, matching quaility를 측정한다. probability가 0.8 아래인 경우는 outlier로 쳤다.

 두번째로 geometric consistency(constraint)의 경우, multiple view에서의 depth consistency를 측정했다. reprojected된 coordinate과 reprojected된 depth가 해당 조건을 만족하도록 constraint을 뒀다.

 ![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210110-36.PNG)

 위 두가지 simple한 방법이 outlier를 골라내는 것에서 strong robustness를 보여주었다.

__Depth Map Fusion__

 다른 view에서 온 depth map을 unified point cloud로 표현하기 위해 depth map fusion을 진행했다. 여러 논문들을 참조했다고 한다.

Merrell, P., Akbarzadeh, A., Wang, L., Mordohai, P., Frahm, J.M., Yang, R., Nist´er, D., Pollefeys, M.: Real-time visibility-based fusion of depth maps. International Conference on Computer Vision (ICCV) (2007)

Galliani, S., Lasinger, K., Schindler, K.: Massively parallel multiview stereopsis by surface normal diffusion. International Conference on Computer Vision (ICCV) (2015)

Sch¨onberger, J.L., Zheng, E., Frahm, J.M., Pollefeys, M.: Pixelwise view selection for unstructured multi-view stereo. European Conference on Computer Vision (ECCV) (2016)

 reconstruction noise를 최소화 하기 위해, filtering step을 each pixel마다 씌우고, reproject된 depth들의 평균을 이용해서 3D point cloud를 만들게 되었다.

 ![img9](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210110-37.PNG)

 그 과정이 위에 나와있다.



### Experiments

__5.1 Benchmarking on DTU dataset__

__5.2 Generalization on Tanks and Temples dataset__

__5.3 Ablations__

__View Number__

![img10](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210110-38.PNG)

 view가 적을 수록 loss가 커지는 것을 확인할 수 있음.

__Image Features__

__Cost Metric__

__Depth Refinement__

__5.4 Discussions__

__Running Time__

__GPU Memory__

__Training Data__

 이 외에 몇가지 문제가 있는데, GT가 100% 믿을만 하지 않고, 모든 view에서 occluded된 pixel은 아예 학습이 되지 않는다는 점이다. 미래에는 이를 해결할수 수 있도록 제대로 된 MVS dataset이 나오길 빈다.



### Conclusion







### References

Yao, Yao, et al. "Mvsnet: Depth inference for unstructured multi-view stereo." *Proceedings of the European Conference on Computer Vision (ECCV)*. 2018.

Gauss function, 위키피디아, [https://en.wikipedia.org/wiki/Gaussian_function](https://en.wikipedia.org/wiki/Gaussian_function)

Merrell, P., Akbarzadeh, A., Wang, L., Mordohai, P., Frahm, J.M., Yang, R., Nist´er, D., Pollefeys, M.: Real-time visibility-based fusion of depth maps. International Conference on Computer Vision (ICCV) (2007)

Galliani, S., Lasinger, K., Schindler, K.: Massively parallel multiview stereopsis by surface normal diffusion. International Conference on Computer Vision (ICCV) (2015)

Sch¨onberger, J.L., Zheng, E., Frahm, J.M., Pollefeys, M.: Pixelwise view selection for unstructured multi-view stereo. European Conference on Computer Vision (ECCV) (2016)