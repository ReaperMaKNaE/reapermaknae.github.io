---
layout : post
title : "SETR"
date : 2021-03-25 +2000
description : DETR의 한계였던 semantic segmentation을 transformer로 해낸 SETR 논문의 간단한 리뷰입니다.
tag : [PaperReview]
---

### Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers



### Abstract

 해당 논문에서는 semantic segmentation을 sequence-to-sequence prediction task로 두었다고 한다. 이러한 과정은 pure transformer를 통해 이루어졌고, image를 patch 단위로 잘라서 보내는 접근 방식을 사용했다고 한다. 이름이 SETR이라 DETR과 비슷하다고 느낄 수 있겠으나, 실제 방식은 ViT와 유사하다.



### Introduction

 encoder를 이용하여 접근하는 방식들이 많아졌는데, 이러한 방식들은 보통 두 가지의 메리트가 있다. 첫번째는 translation equivariance이고, 두번째는 locality이다. 하지만 이러한 방법에서도 receptive fields의 약점이 존재한다.

 이러한 문제를 해결하기 위해, 우리는 large kernel sizes, atrous convolutions, image/feature pyramid등을 한꺼번에 조합하는 방식을 선택했다. 그렇게 탄생한 것을 우리는 SETR(Segmentation Transformer)라고 부르기로 했다. fixed sized patch로 image를 자르고, patch sequence를 만든다.

 해당 논문의 contribution은 다음과 같다.

- sequence-to-sequence 방식으로 semantic segmentation 문제를 다시 푼 것
- transformer framework으로 segmentation을 진행한 것
- complexity를 해결할 수 있는 다른 decoder design



### Related Work

__Semantic segmentation__

__Transformer__

 DETR, STTR, LSTR, ViT등에 대해 설명. ViT에 대해서 상당히 감명을 받았는지, 해당 방식을 이용해 sem-seg을 진행했다고 한다.



### Method

__3.1 FCN-based semantic segmentation__

 뭔가 내용을 보면 related work에 들어가야할 것 같은데 여기에 빠져있다. 고로 생략.

__3.2 Segmentation transformers (SETR)__

__image to sequence__

 2D image와 1D sequence의 경우 match가 잘 되지 않는다. 그러므로, image를 patch로 잘라서 집어넣는걸 하게 된다. 전체적인 부분이 다 ViT를 따라해서, 해당 논문을 읽었다면 넘겨도 좋아보인다. 포스팅은 아마 안 할 예정인데, 전반적으로 ViT보다 더 잘 설명이 된 것 같아 ViT를 안 본 사람이라면 바로 이쪽으로 넘어와도 될 것 같다. 아무튼 16x16으로 image를 자르고, 그래서 1D sequence로 나타내었을 때 H*W가 256배로 줄어드는 효과를 얻는다. 이후 specific embedding을 추가해서 최종 sequence input을 구한다.

__Transformer__

 transformer의 encoder의 MSA와 MLP는 거의 그대로라고 보면 된다. 이는 아래 overview 그림의 (a)를 참조하면 확인 가능하다. 이후 다시 self-attention에 대한 내용이 나온다. transformer에 대한 공부를 했으면 쉽게 이해가 갈 듯 하다.

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210325-16.PNG)

__3.3 Decoder designs__

decoder를 design하는 것에는 3가지의 다른 방식을 썼다고 한다.

__(1) Naive upsampling (Naive)__

 1x1 conv + sync batch norm + 1x1 conv를 쓰고, 이후 bilinearly upsample로 full image resolution을 얻는데, 이런 decoder를 가진 SETR을 SETR-Naive라 한다.

__(2) Progressive UPsampling (PUP)__

 naive와 같은 경우 noise의 문제가 생길 수 있어, progressive upsampling strategy를 취했다고 한다. 이러한 내용은 위 overview 그림의 (b)에 해당한다고 한다. 이렇게 만들어진 decoder를 가지는 형태를 SETR-PUP 라고 한다.

__(3) Multi-Level feature Aggregation(MLA)__

 overview 그림에서 (c)에 해당하는 내용이다. feature pyramid network을 이용해서 만들었다고 한다. 이렇게 만들어진 network을 SETR-MLA라고 한다. 몇몇 자세한 내용이 논문에 나와있는데, 이는 해당 논문을 참조하는게 좋아보인다.



### Experiments

__4.1 Experimental setup__

 3가지의 많이 사용되는 dataset에 대해서 실험을 했다고 한다.

__Cityscapes__

__ADE20K__

__PASCAL Context__

__Implementation details__

__Auxiliary loss__

 아래 논문을 참조하여 auxiliary loss를 추가했다고 한다.

Zhao, Hengshuang, et al. "Pyramid scene parsing network." *Proceedings of the IEEE conference on computer vision and pattern recognition*. 2017.

__Multiscale test__

 mmsegmentation의 default setting을 했다고 하고, synchronized BN 을 decoder와 auxiliary loss head에서 사용했다고 한다.

__Baselines__

__SETR variatns__

 encoder의 layer가 12인가 24인가에 따라서 T-small, T-large model로 나누고, T-small의 경우는 SETR-Naive의 경우에만 적용하여, 이 경우는 SETR-Naive-S로 표시했다고 한다. 추가적으로 SETR-Hybrid model이 있는데, 이 model은 ResNet-50과 SETR-Naive-S의 조합이라고 한다. 즉, encoder만 ResNet인 셈.

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210325-17.PNG)

__Pre-training__

__Evaluation metric__

 mIoU를 사용했다고 한다.

__4.2 Ablation study__

 몇몇 내용들이 나온다. 요약하자면 ADE20K val set에서는 SETR-MLA가 best였는데, 그 외의 모든 경우엔 SETR-PUP가 best performance를 찍었다고 한다.

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210325-18.PNG)

 위에서 다룬 내용의 전체적인 실험 결과는 위와 같다. 몇몇 요약이있는데, pre-training의 경우는 필수적으로 해야한다라고 한다.

__4.3 Comparison to state-of-the-art__

__Results on ADE20K__

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210325-19.PNG)

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210325-20.PNG)

__Results on Pascal Context__

![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210325-21.PNG)

![img7](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210325-22.PNG)

__Results on Cityscapes__

![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210325-23.PNG)

![img9](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210325-24.PNG)

 cityscape의 경우에는 full resolution이 너무 커서(crop에 비해), epoch을 더 돌렸다고 한다.



### Conclusion



### Appendix

__A. Visualizations__

__Features__

![img10](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210325-25.PNG)

__Attention maps__

__Position embedding__

 위와 같은 것들이 나와있지만, 여기선 생략.



### Reference

Zheng, Sixiao, et al. "Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers." *arXiv preprint arXiv:2012.15840* (2020).

