---
layout : post
title : "BoTNet"
date : 2021-03-24 +1100
description : ResNet의 c5 block에서 3x3Conv를 MHSA로 바꾸어 큰 성능 향상을 불러온 BoTNet 논문의 간단한 리뷰입니다.
tag : [PaperReview]
---

### Bottleneck Transformers for Visual Recognition, CVPR 2021



### Abstract

 ResNet의 bottelneck에서 3x3 convolution layer를 MHSA(Multi head self-attention) module로 변경하면서 COCO dataset에서 Faster RCNN framework을 기반으로 Mask AP 44.4%와 Box AP 49.7%를 기록했다. ImageNet benchmark에서는 84.7%로 우수한 성능을 확보했고, efficientNet model보다 TPU-v3 hardware에서 2.33배 더 빠른 inference speed를 얻었다. 아래는 bottleneck transformer의 간단한 visualization.

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-17.PNG)



### Introduction

 최근 여러곳에서 transformer를 접목시키는 일이 많아지고 있다. 아래는 최근의 trend를 조사하여 나타낸 taxonomy.

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-18.PNG)

 위 taxonomy에서, pure attention model의 SASA, LRNet, SANet, Axail의 경우 Self attention을 사용한 것이고, ViT는 Transformer를 사용했다. 모든 것을 통째로 transformer block으로 만든 것들도 있는데, 이는 곧 포스팅할 DETR 같은 것들이다. 여기서 BoTNet은 위에 설명했듯이, ResNet의 일부 블럭만을 바꾼 것이다. 이에 대한 자세한 이유는 추후 ablation study에서 나온다.

 self-attention에서 큰 해상도의 이미지를 받아오는 것과 memory consumption을 해결하는 문제는 중요하다. 논문의 저자는 이 문제를 추상적이고 low resolution의 feature map에서 self-attention을 진행하는 것으로, global self-attention을 사용하는 것으로 해결했다고 한다. 다른 추가적인 작업 없이 성능을 끌어올린 것이 가장 중요한 포인트라고 말하는 것 같다.(보통 이런 걸 논문에선 대개 bells and whistles 라고 표현하는 것 같다. 뭐 cascade RCNN이니 뭐니 하는...) 아무튼 performance gain이 상당히 좋고, 작은 object에 대해서도 그 정확도가 높으며, non-local layer에서도 성능이 좋았다고 한다.



### Related Work

 다른 논문들과는 달리, 재미있게도 최근에 나온 Work들과의 비교를 중심으로 이루고 있다.

- Transformer vs BoTNet
- DETR vs BoTNet
- Non-Local vs BoTNet

__Transformer vs BoTNet__

 이번 논문에서 한가지 키는 바로 ResNet의 bottleneck block을 MHSA로 변경한 것이다. 이것에 대한 설명이 아래 그림에 나와있다.

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-19.PNG)

 위 그림에서 맨 우측 부분이 BoTNet에 해당한다. Transformer와의 차이점으로는, Normalization의 차이가 있다. Transformer의 경우 layer normalization을 사용한다면, BoTNet은 일반적인 Network로 batch normalization을 사용하였고, Non-linearity와 output projections가 있다. 그리고 Adam optimizer를 주로 사용하는 transformer와는 달리 SGD를 사용하였다. ~~그런데 사실 그냥 MHSA를 쓴거지, 다른 transformer를 base로 만들었다는 모델들과는 달리 그냥 MHSA로 바꾼 것 밖에 없어서, 모델이 자체적으로 당연하게 다른 거를 이렇게 써놓은 건데, 좀 뭔가 그렇긴 하다.~~ 

__DETR vs BoTNet__

 DETR은 아직 포스팅하진 않았지만, 바로 다음에 할 예정이므로 미리 적자면, DETR의 경우 object detection에 있어서 region proposal만 없앴을 뿐 아니라 NMS마저도 없애버린 아주 획기적인 object detector이다. 그런데 BoTNet의 경우에는 그곳에 집어넣기엔 좀 다른 개념이라서 비교대상을 DETR로 잡는게 위 Transformer로 잡은 거랑 비슷할 정도로 큰 의미를 찾을 수 있나 싶다.

__Non-Local Nets vs BotNet__

 사실 Non-Local Nets에 대해 잘 모른다. 논문을 전체적으로 한 번 봤는데, 사실 그렇게 알아야하나 싶기도 하다. 이 친구들은 Transformer와 Non-Local-Means algorithm을 서로 이었다는데, 음... Non local means algorithm을 모르므로 pass. 아무튼 3가지 차이점으로, BoTNet의 경우는 multiple head를 썼고, value projection이며, position encoding을 썼다고 한다. 그리고 NL block과 BoT block의 경우 channel factor reduction이 서로 다르고, NL Block은 추가를 해야하는 반면 BoTNet block은 replacing이 차이라고 말하고 있다.



### Method

 참 별거없게도, 아래가 다다.

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-20.PNG)

 그럼 여기서 positional embedding을 어떻게 수행하느냐, 가 문제인데, 그 형태는 아래와 같다.

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-21.PNG)

 위와 같이 진행을 하게 된다. 그런데 왜 하필 c5에서만 적용을 했을까? 를 생각해봤을 때, input image의 size가 1024x1024라면, 아무리 patch로 나누어도 그 크기가 엄청나다. ViT나 image에 transformer를 다 때려넣는 걸 생각해보면, torch.view 같은 걸로 쭉 풀어서 쓰게 되는데, 이러한 방법은 memory와 computation에 치명적이기 때문이다. 그래서 introduction에서도 말했듯이, low resolution에서 feature를 뽑는 것을 생각해냈는데, 이런 low resolution에서 feature를 뽑으려면 자연스럽게 channel이 큰 block에서 진행을 해야 했고, 그 결과 c5 block을 선택하게 된 것이다.

__Relative Position Encodings__

 position encoding에 있어서 absolute와 relative가 있는데, 해당 내용은 다른 논문에서 나온 내용이다. 이는 MHSA를 쓸거면, 아니 Transformer를 조금이라도 보려면 한 번은 봐야할 것 같다. 일단 나도 추후 리뷰 예정.



### Experiments

 Detectron에서 진행했다고 한다. 사실 크게 바꿀게 없으니 그럴만 한 것 같다.

__4.1 BoTNet improves over ResNet on COCO instance segmentation with Mask R-CNN__

![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-22.PNG)

 위 그림이 결과이다. AP에 붙어있는 bb는 bbox를 말하고, mk는 mask를 말한다. 일단 어떤 형태든 간에 BoTNet이 붙는 순간 성능이 올라가는 것을 확인할 수 있다. 그런데 저자는 epoch가 올라갈수록 차이가 별로 없어지는게 마음에 들지 않았던지, scale jitter를 썼다.

__4.2 Scale Jitter helps BoTNet more than ResNet__

 그냥 image를 augment할 때 그 크기를 여러가지로 수정했더니 잘 됬다. 의 내용이다. 

![img7](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-23.PNG)

 위와 같이 scale jitter를 설정한 결과, 그 성능이 더 좋아지는 것을 확인할 수 있다.(72epoch을 기준으로 한 것)

__4.3 Relative Position Encodings Boost Performance__
 이건 ablation study와 비슷한 거긴 한데, position embedding을 무엇으로 했느냐에 따라 성능차이가 어떻게 달라지는지를 보여준다.

![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-24.PNG)

__4.4 Why replace all three c5 spatial convolutions?__

 그럼, 그렇게 좋으면 왜 c5 block의 전부를 MHSA로 바꾸지 않고 가운데것만 쏙 바꿨어요? 라는 질문이 생길 수 있다. 그래서 얘들도 해봤단다.

![img9](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-25.PNG)

 basic한 block은 위와 같이 생겼다. 위에 없는 것으로 [0,1,0]이 있는데, 이게 논문에서 제안하는 baseline model이므로 이를 빼고 올려둔 것 같다. (반대로 [0,0,0]은 그냥 ResNet임)

![img10](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-26.PNG)

 여러 종류의 BoTNet을 돌려본 결과, 위 표와 같은 결과가 나왔다고 한다. 전반적으로 성능이 다 올라가서, 어? 더 좋네? 할 수 도 있지만, 테이블끼리 비교해서 자세히 보면 결국 논문에서 제안한 BoTNet-[0,1,0]이 가장 성능이 좋은 것을 확인할 수 있다.(평균 내보면 그게 제일 성능 증가폭이 큼)

__4.5 BoTNet improves backbones in ResNet Family__

 ResNet이 50만 있는게 아니다. ResNet-101, ResNet-152 등 여러 버전이 있다. 그래서 다 실험을 해봤다고 한다.

![img11](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-27.PNG)

 결과는 당연히 모든 분야에서 accuracy 증가. 그리고 한가지 좀 놀라운 것은, R101보다 BoT50이 성능이 더 좋다. 이는 이러한 scale에서만 해당하는 것이아니라 R152와 BoT101을 비교해도 BoT101이 성능이 더 좋고, 이는 mask의 경우에도 똑같은 결과를 낳았다.

__4.6 BoTNet scales weel with larger images__

![img12](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-28.PNG)

 이번에는 input image를 키워봤다. input image의 size를 키우더라도 그 성능이 증가하는 것을 확인할 수 있다. 

 논문에서는 아래 table도 같이 실어놨다고 했는데, 좀 애매한 영역이긴하다. 아무즌 72 epoch을 돌리고, scale jitter를 적용한 상태라고 한다.

![img13](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-29.PNG)

__4.7 Comparison with Non-Local Neural Networks__

![img14](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-30.PNG)

 NL Net과는 어떻게 비교하더라도 그 성능이 더 좋아진 것을 확인할 수 있다.

__4.8 Image Classification on ImageNet__

__4.8.1 BoTNet-S1 architecture__

![img15](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-31.PNG)

 S1의 경우는 stride를 1로 만들어서 64x64의 image를 그대로 사용하는 것이다. 그 결과는 아래와 같다.

![img16](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-32.PNG)

 __4.8.2 Evaluation in the standard training setting__

 위 그림의 table 참조.

__4.8.3 Effect of data augmentation and longer training__

  위 table에서 학습량만 늘린 table이다. 전체적으로 증가하는 것을 확인할 수 있다.

![img17](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-33.PNG)

__4.8.4 Effect of SE blocks, SiLU and lower weight decay__

![img18](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-34.PNG)

 SE Net과의 비교.

__4.8.5 Comparison to EfficientNets__

 여러 내용이 있지만, 하나만 보면 EfficientNet이 FLOPs에서는 유리하나, 그 외의 것들을 고려하였을 땐 생각보다 compound scaling이 별로네? 라는 내용이다. 추후 계속해서 내용이 나온다.

__4.8.6 ResNets and SENets are strong baselines until 83% top-1 accuracy__

 논문의 저자가 ResNet과 SENet을 정말 좋아하는 것을 여기서 느낄 수 있다. 일단 해당 Net의 구조를 살펴보자.

![img19](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-35.PNG)

 아마도 자세한 테이블은 곧 나올텐데, 아래 plot을 참조. 보면 ViT나 이를 개량한 DeiT같은 것들도 여전히 기존의 baseline을 이길 수 없다는 것을 보여준다.

![img20](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-36.PNG)

__4.8.7 BoTNets are the best performing models beyond 83% top-1 accuracy__

![img21](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-37.PNG)

 위 표는 BoTNeT을 자랑하는 plot. T가 BotNet을 의미한다. plot에서 B7-RA는 EfficientNet-B7 trained with RandAugment이다. 아무튼 이러한 것들을 table로 쭉 보면 아래와 같다.

![img22](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-38.PNG)

 table을 보면 어느 순간부터는 BoTNet이 우세하긴 하지만, 그래도 의외로 ResNet이나 SENet이 강한 것을 볼 수 있다. 이러한 점들 때문에 저자가 ResNet과 SENet을 좋아하는 것 같다.

![img23](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-39.PNG)

 이러한 내용을 모두 합친 model의 모습.

__4.9. Discussion__

뭐 여하튼 생각보다 compound scaling이 별로란 것이랑(EfficientNet을 은근 깐다), SENet과 ResNet을 찬양하는 것. ViT, DETR과 같은 모델들은 사실상 pure transformer network인데, 저자의 경우는 이러한 것들 보단 자기처럼 hybrid로 가는 것이 더 좋지 않겠냐! 라는 물음을 던진다. 




### Conclusion



### Acknowledgements



### Appendix

![img24](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-40.PNG)

 위 그림과 같은 결과에서 차이를 비교할 수 있다. 사실 내눈엔 잘 안보이는데, BoTNet이 더 잘 잡는것 같아보인다.

__A.1. Code for the BoT block__

__A.2. Implementation: COCO instance segmentation and object detection__

__A.3. BoTNet improves over ResNet for Object detection__

 작은 object도 잘 잡아낸다는 내용이 있다.

__A.4. BoT block with stride__

 다음 논문을 위해서 아껴놓겠다고 말한다. 대충 얘내들이 쓸 다음 논문의 아이디어는 이럴 가능성이 있단다.(물론 이런 내용은 아닌데, 이런 아이디어를 서슴없이 적어놓은 거 보면 ~~다 끝내 놓고 남들 약올리려고~~ 아마도 미리 해 놓지 않았을 까.)

![img25](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-41.PNG)

__A.5. Non-Local Comparison__

![img26](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-42.PNG)

 논문 냈더니 괜히 애들이 시비를 많이 걸었나보다. 위와 같이 detail한 사항을 포함해서 NL Net과 어떻게 다른지 비교를 한 것 같다. 이와 관련해서는 계속해서 나온다.

__A.6. Comparison to Squeeze-Excite__

![img27](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-43.PNG)

 일단 ResNet에서 BoT block이 아닌 SE Block을 적용한 경우의 비교라고 한다. 떨어지는건 좀 신기했다.

__A.7. ImageNet Test-Set Accuracy__

![img28](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-44.PNG)

 SE와의 비교한 경우의 정확도 차이.

__A.8. Resolution Dependency in BoTNet__

![img29](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210324-45.PNG)

 encoding에 대해 다시 진행한 결과. 이 쯤이면 누가 보던간에 이 친구들이 appendix가 아닌 본 논문에서 소개한 내용들 중에서 최적화된 것들을 잘 골라내서 만들었다.

__A.9. M.Adds and Params__

 좀 대단한건, parameter의 수와 FLOPs 계산이 좀 이상해서, 다른 곳에서 따와서 알아냈다고 한다. 자세한 내용은 해당 논문 참조.



### Reference

Srinivas, Aravind, et al. "Bottleneck Transformers for Visual Recognition." *arXiv preprint arXiv:2101.11605* (2021).

