---
layout : post
title : "Depth estimation via gradient field and contrastive loss"
date : 2021-03-31 +1200
description : Contrastive learning에서 depth estimation을 위해 pretext task로 gradient field를 연구한 논문의 간단한 리뷰입니다.
tag : [PaperReview]
---

### Learning a Geometric Representation for Data-Efficient Depth Estimation via Gradient field and Contrastive loss, ICRA 2021



### Abstract

 single-view depth estimation이 최근 연구가 되고 있는데, 이와 같은 경우 annotated label이 매우 많이 필요하다. 해당 논문에서는 현재 존재하는 self-supervised method가 depth estimation에 적합하지 않고, gradient-based self-supervised learning algorithm이 unlabeled image에서 geometric information을 얻을 수 있다는 연구 결과를 담고 있다. 이러한 방법을 2가지의 다른 monocular depth estimation algorithm(DenseNet-161과 ResNet-50)에서 실험을 진행했다고 한다.

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210331-19.PNG)

 위 figure는 논문에서 제안하는 방식. Contrastive learning의 baseline은 MoCo를 이용했다고 한다.



### Introduction

 depth estimation network를 학습시키는 것에는 상당히 많은 양의 pixel-wise depth annotated label이 붙은 image pair가 필요하다. 이를 해겨하기 위해 여러 self-supervised learning method가 나왔지만, 그 중에서 depth estimation에 효과적인 방법은 없었다.

 depth estimation의 성능을 올리기 위해, 새로운 방법인 Sobel kernel과 Canny edge binary mask를 이용한 gradient field를 제안한다. 우리의 실험 결과에 의하면, random initialization보다 3배나 적은 data의 양에도 성능이 비슷하게 나왔다. 이 논문의 의미로는, unsupervised 방식으로 depth estimation에 적합한 encoder를 pre-train하는 첫 논문이라고 한다.

 우리의 contribution은 다음과 같다.

- 기존의 self-supervised learning 방식들은 depth estimation task에 적합하지 않다는 것을 보여준다.
- 우리가 제안한 gradient-based momentum contrastive learning은 geometric representation을 잡는 것에 적합하다.
- 우리가 제안한 방식으로 학습한 pre-train 된 monocular depth estimation model은 그 성능이 매우 우수하다.(random initialization 방식에 비해 data-efficiency는 3배나 된다.)



### Related Work

__A. Monocular Depth Estimation__

 depth estimation network들은 대부분 encoder-decoder structure를 사용하고 있다.

__B. Self-supervised visual representation Learning__

 몇몇 pretext task에 대해 말을 해주고 있다. 대표적으로 사용되는 pretext task인 jigsaw puzzle, rotation, exemplar 등을 말하고 있다.

__C. Contrastive Learning__

 최근 Contrastive learning의 동향에 대해 말을 하고 있는데, SimCLR과 MoCo에 대한 이야기를 나누고 있다.



### Method

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210331-20.PNG)

 전체적인 구조는 위와 같다. encoder를 gradient map을 이용하여 pre-train을 하고, depth-estimation network의 encoder에 집어넣는 방식이다. 이 때 depth-estimation network으로는 DenseNet-161과 ResNet-50을 사용하였다.

__A. Momentum Contrastive learning__

 여러 방식의 Contrastive learning method가 있는데, 해당 논문에서는 momentum contrastive learning을 사용하였다. MoCo에 대한 자세한 내용은 이전 포스팅 참조.

 MoCo와 해당 논문의 차이점으로는, MoCo에서의 Query data는 이 논문의 RGB image에 대응하고, key data는 gradient field에 대응하게 된다. 이렇게 한다면, query data(RGB Image)는 key data(gradient field)에 맞도록 학습이 되고, 이로 인해 geometric representation을 배울 수 있다는 것이 저자의 주장이다. 이렇게 만들어진 encoder를 직접 사용해보기 위해, 현재 존재하는 depth estimation network인 DenseNet-161과 ResNet-50의 encoder에 넣어서 실험을 했다고 한다.

__B. Gradient Field__

 image I의 gradient field G를 만들어 내기 위해, Canny edge detector를 사용했다고 한다. 기존의 Canny edge detector의 경우 주변의 pixel들과 sobel operator로 gradient를 비교해서 일정 이하의 gradient면 걸러버리는 binary mask를 씌우는데, 저자는 이를 고쳐서 dominant gradient와 location을 추출해 edge dominance에 따라 gradient field가 다른 강도의 값을 가지도록 했다. 이렇게 gradient field를 만드는 식은 아래와 같다.

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210331-21.PNG)

 위 식에서 $E$ 와 $B_{Canny}$ 는 각각 Sobel operator에서 온 gradient의 크기와 canny algorithm에서 온 binary result를 의미한다. 이 두 개를 element-wise multiplication을 통해 최종 gradient field인 G를 얻는다고 한다.

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210331-22.PNG)

 이렇게 나온 결과를 위 그림에서 확인할 수 있다.



### Experiments

__A. Dataset__

 NYU Depth v2를 썼다고 한다. 그런데 training speed와 computational efficiency 때문에, input RGB resolution은 640x480이지만 output depth resolution은 320x240으로 줄였다고 한다.

__B. Implementation Details__

 다른 방식은 대부분 MoCo와 같은데, batch-size를 64로 했다고 한다. (아무래도 이 쪽에서 학습을 시키려면 막대한 크기의 RAM을 가진 GPU가 필요해보이긴 한다.)

 이후 fine-tuning의 작업 과정에선(depth estimation network에서 depth를 뽑아낼 때엔) training 과정 중엔 batch-size를 4로 두고, eval 에선 1로 두었다고 한다.

__C. Performance Evaluations__

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210331-23.PNG)

 결과는 위와 같다. ImageNet의 경우에는 GT label로 pre-training을 했다고 한다. contrastive learning을 통해 얻은 encoder가 아닌, 그냥 따로 빼서 feature vector를 뽑아낸 것이라고 한다.(아마도 classification을 말하는 것 같다) 아무튼, 성능이 떨어지는 이유로는 dataset의 양 차이라고 한다. ImageNet이 14M image를 가지고 있고, NYU Depth v2는 120K image를 가지고 있다. 다른 pretext task보다 성능이 우수하다고 한다. 그리고 표에 나와있는 것 처럼, Rotation이나 Exemplar와 같은 방법들은 depth estimation에서는 그 성능을 발휘하지 못한 것을 확인할 수 있다.

__D. Impact of encoder pre-training__

 한 문장으로 요약하면, pre-training이 굉장히 중요하다고 한다.

__E. labeled Data-Efficiency__

![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210331-24.PNG)

  위 그림에서 보면, Random initialization에 비해 data를 2~3배 적게 쓰더라도 그 성능이 더 뛰어난 것을 확인할 수 있다. 위 그림에서 1%, 3%, 5%, 10%는 그 만큼 labeled data를 썼다는 뜻이다.

__F. Evaluation on Domain Generalization__

 Domain Generalization을 위해 indoor에서 학습시킨 model을 outdoor에서도 써봤다고 한다.

![img7](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210331-25.PNG)

 그 결과는 위와 같다. N은 NYU-Depth v2를 말하는 것이고, Make3D로 domain adaptation을 한 다음 Make3D에서 측정한 결과가 N->M 이라고 한다. 의외로 나쁘지 않은 outdoor 성능을 확인할 수 있다.

![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210331-26.PNG)

 이를 table로 확인한 결과는 위와 같다.

 이게 왜 잘 되었느냐에 대한 것으로 저자는 gradient가 edge 부근에서 따와지면서 network가 물체의 edge를 잘 파악할 수 있도록 train이 된 것이라고 생각하고 있다.



### Conclusions



### Reference

He, Kaiming, et al. "Momentum contrast for unsupervised visual representation learning." *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2020.

