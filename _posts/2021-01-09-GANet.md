---
layout : post
title : "GANet"
date : 2021-01-09 +1443
description : Cost volume의 연산을 줄인 SGA를 도입한 GA-Net, Guided Aggregation Net for End-to-end Stereo Matching 논문의 간단한 Review입니다.
tag : [PaperReview]
---

### GA-Net: Guided Aggregation Net for End-to-end Stereo Matching, CVPR 2019



 Guided 개념을 추가하여 stereo matching에서 SOTA 성능을 보인 GANet의 간단한 리뷰입니다.



 아래 github에서 code를 확인할 수 있습니다. 해당 network에 대한 코드리뷰는 하지 않을 계획.

[https://github.com/feihuzhang/GANet](https://github.com/feihuzhang/GANet)



### Abstract

 stereo matching task에서 cost volume이 차지하는 부피는 정말 컸음. 그래서 우리는 이러한 형태를 새로 만든 GA(Guided aggregation) block으로 해결한 것을 보여주려고 함.



### Introduction

 전통적으로, stereo reconstruction에는 3가지 스텝이 존재했음.

- Feature Extraction
- Matching Cost Aggregation
- Disparity Prediction

 그런데 이러한 과정에서 연산이 너무 많이 소요가 됨.(SGM이라던지 등등의 사용에서)

 우리가 제안하는 guided aggregation의 방법은 3D convolution에서 GC Net에 비해 1/100의 연산 복잡도를 가지게 됨.(FLOPs에 한해) 이 덕분에 realtime model이 어느정도 구현이 됨.



### Related Work

__2.1 Deep Neural Networks for Stereo Matching__

여러 논문에서 SGM을 이용해 이러쿵 저러쿵 했음.

__2.2 Cost Aggregation__

 전통적인 stereo matching algorithm에서는 neighboring disparities의 변화에 패널티를 줘서 smooth하게 만들도록 제약을 걸었음. 그게 local이랑 semi global인데, 아래에 나와있음.

__2.2.1 Local Cost Aggregation__

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210109-15.png)

 Cost volume C의 한 지점에 대해서 w(guided filter weight)를 넣고 이를 국한 시킨 것이 위 식에 보이는 local cost aggregation. C^A(p,d)는 aggregated matching cost를 의미. 이 cost는 local region N_p에 대해서만 aggregate하기 때문에 속도가 좀 빠르고 real time performance가 나옴.

__2.2.2 Semi-Global Matching__

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210109-16.png)

 semi-global aggregation을 할 때, matching cost와 smoothness contraints는 한 개의 energy function E(D)로 줄여지게 되는데, best disparity map은 energy E(D)를 minimize 시키는 것이라고 볼 수 있음. 그 Energy function이 위 식임. 인접한 점의 disparity discontinuity가 작을 때(==1), 클 때(>1)에 따라 P1, P2의 coefficient를 앞에 붙여서 해당 energy function이 최소화 될 수 있도록 만드는 disparity map이 best.

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210109-17.png)

 그리고 잘 알려진 방법인 SGM이 있음. SGM은 위와 같이 인접한 점에서의 cost를 모두 더하고, 이를 가장 작게 만드는 것임. 이러한 방법은 다양한 곳에서 응용이 되어 왔는데, 아무튼 이젠 우리껄 봐야함.



### Guided Aggregation Net

__3.1 Guided Aggregation Layers__

 여태 다른 사람들이 해온 것과는 다르지만, SGM과 local matching cost aggregation method의 방법에 영감을 받아, 우리는 SGA(Semi global Guided Aggregation)과 LGA(Local Guided Aggregation) Layer를 propose함.

__3.1.1. Semi-Global Aggregation__

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210109-18.png)

 우리가 제안하는 aggregation step은 위와 같음. SGM과 뭐가 다르냐면,

- 우리는 학습가능한 user-defined parameters를 썼고, 이를 penalty weight로 넣었음.(w_1~w_4) 이러한 것들은 서로 다른 상황/위치에서 좀 더 adaptive하고 flexible함.
- minimum section을 한개는 sum으로 바꿨음. 이러한 효과는 아래 논문에서 증명이 되었음. (Springenberg, Jost Tobias, et al. "Striving for simplicity: The all convolutional net." *arXiv preprint arXiv:1412.6806* (2014).)
- 두번째 minimum section(internal)은 maximum으로 바꿈. 이건 learning target이 GT에 가까워질 확률을 최대화 시키기 위한 것임.

 근데 위 두 식 모두 경로에 따라서 값이 커지기 때문에, normalize해 줄 필요가 있음. 그 식이 아래에 있음. 그 결과가 아래에 있는 semi-global aggregation임.

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210109-19.png)

 결국 마지막에 얻게 되는 값은, 4개의 방향중에서 가장 큰 값을 골라냄. 우리는 이 value로 학습을 시도함.

![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210109-20.png)

__3.1.2 Local Aggregation__

![img7](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210109-21.png)

 우리가 만든 local aggregation은 위와 같은 형태임.

 서로 다르게 썰린 cost volume은 LGA의 같은 filtering(aggregating) weights를 공유함. 그래서 cost volume이 잘리더라도 매우 differentiable하게 됨.

__3.1.3 Efficient Implementation__

  어떻게 저떻게 다른 논문에서 한 것과 유사한 방식으로 진행했고 위에 적힌 aggregation은 어떻게 적용을 했다는 내용.

__3.2 Network Architecture__

![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210109-22.png)

 해당 Network를 보면 SGA와 LGA를 어디서 사용했는지 확인할 수 있다.

__3.3 Loss Function__

 smooth L1 loss function을 사용. 다른 network에서도 많이 사용했지만, 다시 한번 올리면 아래와 같음.

![img9](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210109-23.png)

 disparity regression은 GCNet에서 사용했던 soft argmin을 채용함.



### Experiments

 Scene Flow, KITTI를 사용했음.

__4.1 Ablation Study__

__4.2 Effects of Guided Aggregations__

__4.3 Comparisons with SGMs and 3D Convolutions__

__4.4 Complexity and Real-time Models__

__4.5 Evaluations on Benchmarks__

__4.5.1 Scene Flow dataset__

__4.5.2 KITTI 2012 and 2015 Datasets__



### Conclusion



### Appendix

__A. Backpropagation of SGA__

 Equation 5와 Equation 6을 응용해서 어떻게 backpropagation을 했는지에 대한 설명이 나와있음.

![img10](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210109-24.png)

![img11](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210109-25.png)

__B. Details of the Architecture__

 Table로 Architecture를 나타내어주고 있음. 추후 Code review 예정.



### References

Zhang, Feihu, et al. "Ga-net: Guided aggregation net for end-to-end stereo matching." *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*. 2019.

GANet github, [https://github.com/feihuzhang/GANet](https://github.com/feihuzhang/GANet)

Springenberg, Jost Tobias, et al. "Striving for simplicity: The all convolutional net." *arXiv preprint arXiv:1412.6806* (2014).