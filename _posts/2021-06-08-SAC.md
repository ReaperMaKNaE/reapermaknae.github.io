---
layout : post
title : "SAC"
date : 2021-06-02 +0900
description : Depth estimation에 surface normal을 응용한 논문인 Normal Assisted Stereo Depth Estimation의 리뷰입니다.
tag : [PaperReview]
---

### Self-supervised Augmentation Consistency for Adapting Semantic Segmentation



### SUMMARY

 알겠지만 domain이 달라지면 class prior가 지맘대로 움직임. 이걸 momentum encoder로 해결함.



### Abstract

 우리는 practical하고 높은 정확도를 가지는 semantic segmentation을 위한 domain adaptation에 대한 접근법을 제안한다. 이전의 work들과는 달리, 우리는 adversarial training, network ensemble, style transfer를 사용하지 않는다. 대신에, 우리는 기본적인 data augmentation technique인 photometric noise, flipping and scaling을 이용하여 이러한 image transformation 사이에서 semantic prediction의 consistency를 확신한다. 우리는 이러한 원리를 가벼운 self-supervised framework으로 발전시켜 추가적인 training round 없이 pseudo label을 얻을 수 있도록 한다. practitioner의 관점에서 간단하게 학습하여 우리의 접근법은 눈에 띄게 효과적이다. 우리는 다른 backbone network의 선택과 adaptation scenario에서 상당한 SOTA accuracy improvement를 이루었다.



### Introduction

 UDA는 annotated dataset보다 더욱 다르게 분산된 unlabeled data를 사용할 수 있는 semi-supervised learning의 변형이다. 이에 맞는 사례로는 synthetic data를 찾는 것으로, real-world image의 라벨링에 비해 조금 더 accessible한 annotation을 말한다. semantic segmentation에서 UDA를 이용한 몇몇 성공 사례들은 점점 더 정확한 방법이 되도록 성장하는데, style transfer network나 adversarial training, network ensemble등을 사용한다. 이렇게 model complexity를 증가시키는 것은 reproducibility를 저하하고 진행을 느리게 만든다.

 이번 work에서 우리는 추가적인 training effort 없이 semantic segmentation accuracy에서 SOTA를 찍은 UDA framework을 제안한다. 이 목표를 달성하기 위해, 우리는 simple한 semi-supervised approach인 self-training을 이용하였다. 최근 work들은 adversarial training과 network ensemble을 합친 것을 사용하나, 우리는 대조적으로, self-training standalone이다. 이전의 self-training method와는 비교되게(추가적인 round 사이에 전문적인 간섭이 필요한 것들을 의미함) 우리의 접근방식은 multiple training round의 불편함을 피한다. 우리는 전문적인 간섭 없이 end-to-end co-evolving pseudo label model을 학습시킨다.

 우리의 방식은 fully supervised learning에서 아주 흔하게 쓰이는 data augmentation을 사용한다.(photometric jitter, flipping and multi-scale cropping) 우리는 이러한 image perturbation들 사이에서 모델에 의해 만들어진 semantic map의 consistency를 강요한다. 다음과 같은 assumption이 key premise를 공식화한다.

__Assumption 1.__

$f: I \rightarrow M$ 이 image $I$ 에서 semantic output $M$ 으로의 pixelwise mapping이라고 하자. 여기서 $\rho_\epsilon : I \rightarrow I$ 는 photometric image transform을 말한다.

 유사하게, $\tau_{\epsilon'} : I \rightarrow I$ 는 spatial similarity transformation을 말하고, 여기서 $\epsilon, \epsilon' \sim p(\cdot)$ 은 predefined density를 따르는 control variable이다. (예를 들면 $p = N(0,1)$ ) 그리고, 임의의 image $I \in I$ 에 대해서 $f$ 는 photometric image transform에 대해서는 invariant하고 spatial similarity transformation에 대해서는 equivariant하다. 즉,

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210608-1.PNG)

 위 식이 성립한다.

 다음으로, 우리는 momentum network을 사용한 training framework을 소개한다. momentum network은 fixed supervision in model distillation과 반대로 model update를 안전하게 제공한다. 우리는 self-supervision에서 pseudo-label을 만드는 것에 대한 long-tail recognition의 문제를 다시 한번 생각해보기도 할 것이다.

 특히, 우리는 기하급수적으로 움직이는 class prior를 few sample의 class의 confidence threshold를 삭감하기 위해 유지하고 그들의 relative contribution을 training loss에서 증가시킨다. 우리의 framework은 train하기 쉽고 supervised setup에 비해 추가적인 연산이 붙지만, benchmark에서의 성능은 확실함을 보여준다.

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210608-2.PNG)



### Related Work

__Learning domain-invariant representations__

__Self-training on pseudo labels.__

__Spatial priors.__

__Relation to our approach.__

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210608-3.PNG)

 위 테이블에서 볼 수 있듯, 우리가 한 work의 training process에서 streamline을 알 수 있다. 첫째로, 우리는 adversarial training을 사용하지 않았다. 이는 feature invariance만으로는 label invariance를 보장할 수 없기 때문이다. 둘째로, 우리는 우리의 model을 한방에(여러 step에 나누지 않고) co-evolving pseudo label을 할 수 있도록 했다.(pseudo-label이 점점 진화하는 것을 말하는 듯 하다.) 우리의 framework은 noisy mean teacher와 닮은 경향이 있고 self-ensembling과 consistency regularisation을 융합한다. 이와 비슷한 접근법이 medical imaging이나 UDA work에서도 진행이 되고 있지만, 인정받을 만한 augmentation의 범위 내에 있어서 제한이 있다. 우리는 photometric invariance, scale과 flip equivariance를 이용하여, 복잡한 방법 에 비해 high-fidelity(높은 충실함?) pseudo supervision을 뽑아낸다. 우리는 scaling이 label quality를 예측하기에는 좀 아닌것 같아서 multiple scale과 flip을 이용했다. 이는 test-time augmentation과 training time에서 uncertainty estimation을 병행한다.



### Self-Supervised Augmentation Consistency

__3.1. Framework overview__

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210608-4.PNG)

 Figure 2a에 나와있는 것 처럼, 우리의 framework은 target domain에 adapt하려고 하는 segmentation network을 포함하고 있고, momentum network을 사용하였다. self-supervised scene adaptation을 수행하기 위해, 우리는 처음에 두 network 모두에게 target domain의 sample image 한장에서 온 random crop과 horizontal flip을 넣게 된다. 각 pixel에 대해서 우리는 적절한 inverse spatial transformation 이후 momentum network에서 오는 prediction의 평균을 이용한다. 그리고 우리는 individual sample에 적응을 할 수 있는 running statistics에 기반한 threshold를 사용하여, averaged map에서 confidence pixel을 선택함으로써 pseudo GT를 만들게 된다. 최종적으로, segmentation network은 pseudo label에 맞게 그들의 parameter들을 update하기 위해서 stochastic gradient descent를 사용한다.

 우리의 접근은 mean teacher framework, temporal ensembling과 유사하다. 하지만 우리가 점차 보여주겠지만, ensembling property는 보조적인 역할만 수행하게 된다. 더욱 중요하게는, reinforcement learning과 unsupervised learning에서 사용되는 momentum encoder와 유사하게 우리의 momentum network은 segmentation network의 self-supervised training을 위해서 안전한 target을 제공하게 된다. 이러한 시선은 우리가 다음에 서술하겠지만, target-generating process에 더욱 집중할 수 있도록 한다.

__3.2. Batch construction__

 각 sample된 target image에서 우리는 random scale, flip, location을 가지면서 aspect ratio는 유지하는 N crops를 만든다. 우리는 기존의 input resolution은 유지할 뿐 아니라 crop을 해서 network에 input으로 주게 된다. Figure 2b의 설명이 이를 설명한다. 그 다음은 image classification에서 noisy student model을 이용하는데, segmentation network의 input은 추가적으로 photometric augmentation을 적용받게 된다. 이는 random colour jitter, smooth with Gaussian filter등이다. 반면에 momentum network에서는 clean한 image만 받게 된다. 이는 photometric perturbation에 상관없도록 model을 만들게 된다.

__3.3. Self-supervision__

__Multi-scale fusion.__

 우리는 Figure 2c에 나와있는 것 처럼 momentum network에서의 output mask를 original image canvas에 re-project한다. 각 pixel에 대해서 overlapping이 된 부분은 prediction의 평균값을 사용한다. 몇몇 pixel은 crop의 밖에 놓이게 되는데, 이로 인해 original image의 single forward pass의 결과를 포함하게 된다. 우리는 이러한 prediction을 온전하게 유지한다. 융합된 map은 self-supervision에서 pseudo mask를 추출하는 것에 사용된다.

__A short long-tail interlude.__

 희귀한 class들을 다루는 것은 recognition에서 악명높은 문제이다. semantic segmentation에서 우리는 class들 사이를 구별할 때 image-level과 pixel-level의 빈도로 구분한다. self-supervision을 만들면서 우리는 이러한 case를 특별히 주의하고,

- pseudo label을 선택할 때 낮은 threshold value를 사용하도록,
- focal loss와 gradient의 contribution을 높이도록,
- importance sampling

을 사용하였다. 우리는 다음에 이에 대한 자세한 내용을 서술할 것이다.

__Sample-based moving threshold__

 multi-round training과 self-training을 사용한 대부분의 work들은 training process와 pseudo label을 다시 만드는 과정이 필요하다. 그 이유 중 하나는 supervision을 위해 pseudo label을 filtering하기 위한 threshold value를 다시 계산해야 하기 때문인데, 이는, model parameter가 고정이 된 전체 target dataset에 대한 예측을 통과해야하기 때문이다.

 중간에 간섭 없이 end-to-end training을 가능하게 하기 위한 우리의 목적을 맞추기 위해, 우리는 threshold를 on-the-go로 계산하는 다른 방식을 취했다. 이를 위한 main 재료로는, 우리는 게속해서 움직이는 class prior를 유지시키는 것으로 했다.(domain마다 class prior가 엄청나게 움직이게 되는데 - feature단에서 - 이를 어떻게 유지시켰다 라는 것을 말함) 자세하게, momentum network의 각 softmax prediction에 대해서 우리는 처음에 sample n에 들어있는 pixel이 class 개수 c에 대해서 다음과 같은 piror estimate 확률을 구한다.

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210608-5.PNG)

 위 식에서 $m_{c,n,:,:}$ 은 class c에 대한 mask prediction을 말한다. 우리는 momentum을 이용해서 exponentially 움직이는 average를 다음과 같이 유지했다.

![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210608-6.PNG)

 우리의 sample-based moving threshold인 $\theta_{c,n}$은 moving prior $\chi$ 보다 작은 값을 취했다. 이 때 moving prior는 굉장히 적은 수의 class를 말하며, long-tail class라고도 한다. 이 값은 최대 1까지만 묶여있게 된다. 이를,

![img7](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210608-7.PNG)

 위 식에서 $\beta$ 와 $\zeta$ 는 hyperparameter이고, $m$ 은 class c에 대한 peak confidence를 의미한다. 즉,

![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210608-8.PNG)

 Figure 3는 equation 3를 plot한 것이다. 

![img9](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210608-9.PNG)

 위 과정은 moving class prior에 대해서 적절한 $\beta$ 를 selection하기 위함이다. 예를들어서 정말 많은 class인, 이미지마다 찍히는 road 같은 경우에는 exponential term이 거의 의미가 없다. 하지만, long-tail class와 같은 경우에는 이러한 upper bound보다 threshold가 낮기 때문에, 이러한 class들의 pixel들은 더욱 supervision으로 선택된다. pseudo label을 얻기 위해서 우리는 threshold $\theta$ 를 momentum network의 peak prediction에 적용했다.

![img10](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210608-10.PNG)

 위 식에서 $c^{*} = argmax_c m_{c,n,i,j}$ 는 해당 pixel에서 dominant class를 의미한다. 이러한 pixel의 confidence가 threshold보다 작으면, non dominant prediction도 self-supervised loss에서 무시될 것이다.

__Focal loss with confidence regularisation__

 우리의 loss function은 long-tail class를 gradient signal에서 contribution을 올리기 ㅜ이해 focal multiplier를 포함한다. 이전의 work들과는 다르게, 우리의 moving class prior $\chi$ 는 focal term과 관련되어있다.

![img11](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210608-11.PNG)

 $\bar{m}$ 은 parameters $\phi$ 와 segmentation network의 예측이고, pseudo label $c^*$ 는 equation (5)에서 오며, $\lambda$ 는 focal term의 hyperparameter를 의미한다. $\chi$ 가 작다는 뜻은 low value를 의미하고 이는 높은 weight를 가져야함을 알 수 있다. 1보다 높은 $\lambda$ 는 long-tail  class에 대해서 상대적인 weight를 증가시키고, 0으로 setting이 된다면 focal term을 없애게 된다. 우리는 추가적으로 momentum network의 confidence value로 우리의 loss를 regularize한다는 것을 기억해야한다.(equation (4) 참조) 부정확한 pseudo label의 경우를 대비하여, 우리는 이러한 confidence가 작아야 된다고 예측하고 있고 multi-scale fusion과 calibration 때문에 training을 regularize 해야 한다고 예측한다. 우리는 $\phi$ 에 대해서 각 pixel에 해당하는 equation (6)의 loss가 작아지도록 했다.

__3.4. Training__

__Pre-training with source-only loss.__

 우리는 이전의 work을 따라 source data only인 경우 cross-entropy loss를 줄임으로써 segmentation task에 시동을 걸기 위해 Adaptive Batch normalization을 선택했다. 우리의 실험에서 mean과 standard deviation을 training이 끝날 때 다시 계산하는 것이 불필요하다는 것을 확인했다. 대신에, 우리는 training 이전에 source와 target image의 batch를 번갈아가면서 사용하지만, 후자의 경우(target의 경우)에는 loss를 무시했다. target batch에 대해서, 이는 BN layer의 mean과 standard deviation을 update하면서도 model parameter는 건드리지 않도록 남겨둔다.

(음.. source에 대해서만 학습을 하는건데, batch normalization의 경우에는 target에도 적용을 시켜야 하므로, target의 경우에는 loss를 제껴서 batch normalization에 쓸 mean과 deviation만 들고 오는 학습 방법을 말하는 듯 하다.)

__Importance sampling.__

 equation (6)의 loss function은 높은 image frequency인 long-tail class(신호등, 기둥) 등을 세지만 bus, train과 같은 별로 없는 sample에 대해서는 효과적이지 못했다. 이러한 점을 완화시키기 위해, 우리는 importance sampling을 수행했다. 이는 long-tail class들의 sample frequency를 증가시킨다. 우리는 density $p$ 를 이용하여 target image를 다시 sampling함으로써 expected target loss를 최소화한다.

![img12](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210608-12.PNG)

 $p$ 를 얻기 위해, 우리는 pre-trained segmentation network을 사용하고 equation 1에서 사용하는 각 image n에 대해서 class prior estimate인 $\chi$ 를 계산한다. training 동안, 우리는

- semantic class $c$ 를 균일하게 sample하고,
- target sample $l$ 을 다음과 같은 확률로 얻는다.

![img13](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210608-13.PNG)

 이러한 2 step sampling process는 모든 image가 모든 $l$ 에 대해서 일반적인 class 때문에 non-zero sample probability를 가지도록 한다.

__Joint target-source training.__

 우리는 segmentation network을 equation (6)과 (7)에 선언된 $p$ 에서 sample이 된 target data의 focal loss와 source에서 오는 CEloss를 사용하여 SGD를 이용해 학습시킨다. 

![img14](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210608-14.PNG)

 위 그림은 pseudo label을 보여준다. 우리는 주기적으로 momentum network의 parameter $\psi$ 를 다음과 같이 update한다.

![img15](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210608-15.PNG)

 위 식에서 $\phi$ 는 network parameter이다. $\gamma$ 는 update를 regulate 하는 것인데, 숫자가 작을수록 빠르게 update하면서 unstable training이 된다. 반면에 높은 숫자는 시기상조의 suboptimal convergence를 나타냈다. 우리는 $\gamma$ 를 최대한 유지하면서 매 iteration마다 momentum network을 update한다.



### Experiments

__Datasets.__

 우리의 실험은 3개의 dataset에서 진행이 되었다. cityscapes, GTA5, synthia이다.

__Setup.__

 우리는 이전 work들과 동일한 evaluation protocol을 사용했다. metric으로는 segmentation accuracy와 per-class IoU, mIoU를 썼다.

__4.1. Implementation details.__

__4.2. Comparison to state of the art__

__GTA5 $\rightarrow$ Cityscapes__

![img16](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210608-16.PNG)

__SYNTHIA $\rightarrow$ Cityscapes__ 

![img17](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210608-17.PNG)

놀라운 점은 더 약한 backbone인 VGG를 썼음에도 다른 network들을 다 압도한 것이다.

__4.3. Ablation Study__

![img18](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210608-18.PNG)

 augmentation consistency와 momentum network가 매우 중요한 역할을 수행하고 있다.

__4.4. Qualitative assessment__

![img19](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210608-19.PNG)

baseline은 source-only loss with ABN이다. 따로 spatial prior에 대한 explicit encoding이 없음에도 boundary가 잘 잡히는 것을 볼 수 있다.



### Conclusion



### Appendix





### Reference

Araslanov, Nikita, and Stefan Roth. "Self-supervised Augmentation Consistency for Adapting Semantic Segmentation." *arXiv preprint arXiv:2105.00097* (2021).
