---
layout : post
title : "FACA"
date : 2021-04-27 +0900
description : Pixel-level에서 domain adaptation을 진행하여 여러 domain 사이 adaptation을 진행한 FCNs in the Wild 논문의 간단한 리뷰입니다.
tag : [PaperReview]
---

### FCNs in the Wild: Pixel-Level Adversarial and Constraint-based Adaptation



### Abstract

 FCN이 여러 dense prediction에서는 성공적인 결과를 보여주고 있지만, 정작 비슷한 다른 데이터셋에 적용시켜보면 성능이 다 박살나있다. 이건 어찌 생각해보면 당연한데, domain마다 촬영한 카메라, 환경 등등 다양한 요소가 다르기 때문에 그 결과가 당연히 다를 수 밖에 없다. 우리는 이를 pixel-level distribution shift에서 분석하여, 처음으로 domain adaptive semantic segmentation method를 소개한다. 그것도 unsupervised adversarial 접근을 통해서. 그 결과는 아래 사진에서 확인할 수 있다.

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210427-1.PNG)



### Introduction

 Semantic segmentation은 visual recognition task에서 꽤나 중요한 요소 중 하나이다. 꽤나 엄청난 성장을 보이고 있음에도 불구하고, 아직 그 한계는 존재하고 있다. 예를 들자면, visual domain에서의 그 적용은 아직 힘든 상황이다. 따라서 아직도 많은 사람들이 해당 과제에 대해 도전중이다. 현재의 경우엔, 많은 self-driving application에서는 city나 driving route에 따른 결과로 해석을 진행하고 있고, person recognition task에서는 indoor scene에서 진행을 하고 있다. 추가적으로, 이를 수행하기 위해선 많은 dataset이 필요한데, pixel-wise annotation은 매우 큰 cost를 소요하기 때문에, 둘 사이의 정보를 전달하는 것은 많은 비용이 들 수 밖에 없다.

 그래서 우리는 unsupervised method를 이용한 domain adaptation을 제안한다. 이 구조는 FCN으로 이루어져있다. 우리의 두번째 contribution은 global/local alignment 방법이다. 이는 global/category specific adaptation을 통해 이루어진다.

 이러한 내용들을 우리는 SYNTHIA, GTA5, CityScapes 등의 dataset을 이용하였고, 추가적으로 BDDS(Berkeley Deep Driving Segmentation)을 만들어서 test/eval을 진행했다.



### Related Work

__Semantic Segmentation__

__Domain Adaptation__



### Fully Convolutional Adaptation Models

 이번 section에서는 common label space를 가지는 두 domain 사이를 FCN으로 semantic segmentation을 하기 위한 adaptation algorithm을 소개한다. 우리가 제안하는 모델은 generality를 잃지 않고 다른 semantic segmentation에 적용이 가능하다.

source domain $S$ , source images $I_S$ , labels $L_S$ 를 고려하고, 이들을 이용하여(only source만으로), pixel-wise per-category score map인 $\phi_S(I_S)$ 를 만든다.

 우리의 목표는 이렇게 학습한 모델이 unlabeled target domain $T$ 와 $I_T$ 에서도 적용이 되도록 하는 것이다.(annotation 없이) 우리는 이러한 network의 parameter를 $\phi_T(\cdot)$ 으로 표현한다. 만약 source와 target의 차이가 없다면, target에 source를 그대로 쓰는 것도 방법이 될 수 있다. 하지만, 대~~부분은 어떤 일이 있든간에 다르다.

 그러므로, 우리는 unsupervised adaptation 접근 방법을 사용한다. 우리는 2가지 경우에서 domain adaptation을 생각했다. 첫번째는 두 domain의 feature space에 상응하는 distribution shift에 대한 global change이고, 두번째는 category specific parameter를 의미한다.

 여기서 우리는 global과 category specific shift를 둘 다 줄이는 방법을 이용한다.

 우리의 model에서, 우리는 새로운 semantic segmentation loss objectives를 소개한다. 하나는 global distribution distance를 의미하는 $L_{da}(I_S, I_T)$ 이고, 다른 하나는 category specific parameter를 의미하는 $P_{Ls}, L_{mi}(I_T, P_{Ls})$ 를 말한다. 마지막으로, 우리는 semantic segmentation 결과의 차이를 source domain에서 다시 구하게 된다. ( $L_{seg}(I_S, L_S)$ ) 이를 우리는 아래와 같이 모을 수 있다.

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210427-2.PNG)

 우리의 framework은 다음과 같이 visualization이 가능하다.

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210427-3.PNG)

 위 network은 16 lyaers의 VGGNet으로 이루어져있는데, 16개의 layer 중 마지막 3개의 layer는 fully connected layer로 구성이 되어있고, 각각 $fc_6, fc_7, fc_8$ 로 notation을 하기로 하자. 이들은 input resolution과 크기가 같아지도록 bilinear up-sample layer를 따른다.

__3.1. Global Domain Alignment__

 $L_{da}(I_S, I_T)$ 는  global domain alignment adaptation이다. 최근 연구들이 domain discrepancy distance가 adversarial learning procedure에 의해 최소화 될 수 있다고 한다. 하지만 이는 지금까지는 image에만 상응했을 뿐이다. 그래서 우리는  semantic segmentation model을 위해 domain invariant한 표현을 학습하는 것을 도우는 pixel wise가 적용된 adversarial learning을 소개한다.  여기서, 우리가 대답해야할 첫번째 질문은 dense prediction framework에 어떤 것을 우리가 병합해야하는지이다. recognition은 image의 full scale에서 pixel level alignment를 진행하기 때문에, adversarial learning에 도달하기 위해서는 너무 큰 제한이 걸리게 된다.

 대신에, 우리는 final representation layer( $fc_7$ )에서 각 spatial unit의 receptive filed에 상응하는 영역을 각자의 instance로 고려한다. 이를 하기 위해 우리는 우리의 마지막 pixel prediction에 사용되는, 같은 정보의 adversarial training 과정을 보급한다. 그러므로, 이러한 제공은 source와 target에서 조금 더 minimize를 하는 것에 의미가 있다.

 $\phi_{l-1}(\theta, I)$ 를 network parameter $\theta$ 에 대한 pixel prediction 전 마지막 layer의 output이라고 하자. 그러면, 우리의 domain adversarial loss $L_{da}(I_S, I_T)$ 는 대체적으로 minimization 이 되는 objective로 구성이 되어 있다. 한가지 우려되는, 우리가 minimizing 해야 하는 source와 target의 distance인 $d(\phi_{l-1}(\theta, I_S), \phi_{l-1}(\theta, I_T))$ 아래 존재하는 representation space의 parameter인  $\theta$ 이다. 두번째로 우려되는 점은 source와 target domain의 instance를 구별하기 위한 domain classifier의 훈련을 통한 distance function의 예측이다. domain classifier parameter를 $\theta_D$ 라 하자. 그러면 우리는 source와 target region에서 서로 다른 domain classifier를 인식하고 그러한 classifier를 이용해서 source와 target의 distance minimization을 guide하도록 해야한다.

 $\sigma$ 를 softmax function이라 하고, domain classifier prediction을 $p_{\theta_D}(x) = \sigma(\phi(\theta_D, x))$ 라 하자. layer $l-1$ 의 output이 $H\times W$ 의 spatial unit을 가진다고 가정하면, 우리는 domain classifier loss를 아래와 같이 변경할 수 있다.

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210427-4.PNG)

$L_{Dinv}$ 를 inverse domain loss라고 한다면, 이는 다음과 같다.

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210427-5.PNG)

 따라서 이러한 정의로 인해, 우리는 minimization procedure의 대체를 다음과 같이 할 수 있다. 

![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210427-6.PNG)

 위와 같은 두 목적을 optimizing하는 것은 image region에 대해 학습이 가능한 domain classifier를 이루고(domain classifier가 잘 학습되도록 만들고... 같음), domain classifier를 이용하여 source와 target domain 사이의 distance를 줄이는 것에 사용한다.

__3.2. Category Specific Adaptation__

 domain classifier를 이용해 global domain distribution distance를 줄였다면, 다음은 category specific network parameter를 수정하여 우리의 source model을 적응시키는 것이다. 이를 하기 위해, 우리는 최근 multiple instance learning을 목표로하는 FC를 소개하는 논문들을 몇 개 읽었다. 이러한 work들은 다음 training에 사용할 수 있는 target labeling을 예측하도록 만들기 위해 크기와 그 존재(existence)에 constraint를 걸었다. 우리는 이러한 방법을 unlabeled setting에도 적용이 가능하고 generalize가 되도록 한 새로운 적용기법을 제안한다.

 첫번째로, 우리는 pixel-wise unsupervised adaptation problem에 쓸모있는 새로운 constraint을 고려한다. 통상적으로, source domain $P_{L_S}$ 의 image당 labeling statistics를 계산하는 것에서 시작했다. 특히, class c를 가지고 있는 각 source image에서 우리는 GT label에 이러한 class가 존재하는 확률을 계산했다. 그리고 이를 histogram으로 나타내어, histogram에서 10% 미만의 분포를 나타내는 class들은 $\alpha_c$ , 평균 값(아마도 하위10%~상위 10% 사이)은 $\delta_c$ , 그리고 상위 10%는 $\gamma_c$ 로 계산했다. 우리는 이러한 distribution을 우리의 target domain의 size constraint로 사용하였고, explicit하게 source에서 target domain으로 scene layout information을 전달할 수 있었다. 에를 들면, driving scenario에서, '길'은 많은 범위를 차지하는 반면, street sign은 조그만 부분만을 차지하고 있다. 이러한 정보는 여러 instance learning을 제한하는 것에 중요하다. 그에 반대로, 이전의 work들은 이미지에서 알고 있는 class들에 대해 single size threshold를 사용했었다.

 우리는 image-level labels가 안다는 경우에서 constrained multiple instance loss를 보여주는 걸로 시작한다. 그러므로, 특정한 class c에 대한 주어진 target image에서, 우리는 아래와 같은 prediction map $p=argmax\phi(\theta, I_T)$ 의 output에 다음과 같은 constrain을 걸었다.

![img7](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210427-7.PNG)

(잠깐 멈춰서 해석해보자면, image 한 장의 각 pixel에 대해서, 어떤 class인지를 확인한다. 이 class가 위와 같은 범위 내에 존재한다 하면 아, 얘가 target domain에도 있는 친구구나. 하는 것이다. 즉, domain이 달라서 class를 새로 예측을 해야하는데, unsupervised learning에 응용이 가능하도록, source에서 한 image에 class가 몇% 있느냐를 기억하고, 이를 target에도 똑같이 적용시켰을 때 대충 비슷한 놈들끼리 위와 같은 constrain을 만족한다~ 가 된다.)

 그러므로, 우리의 constrain은 class c에 대해 pixel단위로 파악할 수 있도록 격려한다. 위에서 우리가 maximum 값도 걸었는데, 가끔 너무 많이 잡히는 것들이 있어 그런 것들은 제외했다.(sky, people 등등)

 다른 곳에서 사용하는 optimization detail을 사용하긴했는데, 한가지 중요한 수정이 있었다. 이게 가끔 overfitting이 일어나게 되는데, 이를 해결하기 위해 우리는 lower 10% of source class distribution인 $\alpha_c$ 가 0.1보다 크게 되면, 우리는 이 값을 0.1로 fix하도록 하였다. 이러한 re-weighting은 class를 다시 re-sampling해서 밸런스를 맞춰주는 걸 도와주었다.(상대적으로 적은 수의 class들에 대해서)

 이렇게 하는 것 만으로 image level label을 하는 것엔 무리가 있다. 그러므로, 우리의 접근은 image level label을 먼저 진행한 다음 class size constraint를 수행하는 것으로 최적화했다.

 약하게 supervised setting을 하는 것에 반해, 우리는 아예 known image-level annotation을 사용하지 않았다. 대신에 우리는 source domain에 대해서는 fully supervised를 진행했고, 이를 unsupervised target domain에 넘겨줄 수 있도록 constrain을 거는 곳에 사용했다. 그러므로 우리는 source domain에서는 fully supervised를 사용하게 된다.

 정리하면, 주어진 target image $I_T$ 에 대해서 우리는 최근 output class prediction map인 $p=argmax\phi(\theta, I_T)$ 를 계산한다. 각 class에 대해서 우리가 계산한 값은 $d_c = \frac{1}{H \cdot W} \sum_{h\in H} \sum_{w\in W} (p_{hw}=c)$ 가 된다. 마지막으로 우리는 $d_c > 0.1 \alpha_c$ 를 만족한다면 class c에 대한 image-level label을 assign한다. 



### Experiments

 우리는 3가지 실험을 진행했는데, 각각은 cities -> cities, season -> season, synthetic -> real에 해당한다. 이들 각각은 서로 다른 dataset에서 진행이 되었고, Caffe로 쓰여졌다.

__4.1. Datasets__

__Cityscapes__

__SYNTHIA__

__GTA5__

__BDDS__

__4.2. Quantitative and Qualitative Results__

GA만 사용한 경우, GA와 CA를 둘 다 사용한 경우를 비교하였음.

__4.2.1. Large Shift: Synthetic to Real Adaptation__

 GTA5에서 Cityscape과 SYNTHIA에서 Cityscape

![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210427-8.PNG)

__4.2.2. Medium Shift: Cross Seasons Adaptation__

 season->season의 비교인데, SYNTHIA에서만 비교를했다. 해당 데이터셋에서 계절별로 제공을 하는듯.

![img9](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210427-9.PNG)

__4.2.3. Small Shift: Cross City Adaptation__

![img10](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210427-10.PNG)

__4.3. BDDS Adaptation__

cities->cities adaptation에서만 분석을 진행하는데 사용한 dataset.

![img11](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210427-11.PNG)



### Conclusion



### Reference

Hoffman, Judy, et al. "Fcns in the wild: Pixel-level adversarial and constraint-based adaptation." *arXiv preprint arXiv:1612.02649* (2016).

