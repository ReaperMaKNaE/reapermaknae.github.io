---
layout : post
title : "DeepV2D"
date : 2021-01-08 +0930
description : DeepV2D, Video to Depth with differentiable structure from motion, ICLR 2020의 간단한 리뷰입니다.
tag : [PaperReview]
---

### DeepV2D: Video to Depth with differentiable structure from motion, ICLR 2020



### Abstract



classical geometric algorithm들을 trainable module로 바꾸고, end to end differentiable architecture로 합쳤음. motion estimation과 depth estimation은 서로 보완하며 accurate depth를 얻을 수 있게 됨.



### Introduction



![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210108-7.png)

SfM을 이용해서 해결하려는 접근. Figure 1에 어느정도 잘 나와있음.

motion module이 depth를 input으로 받아들이면, camera motion을 update해서 다음 depth module에 input으로 줌. 그럼 depth module은 camera motion을 받아서 stereo reconstruction을 이룸.(recon이라기 보단 그냥 depth estimation인데...) 이걸 반복하다보면 accurate depth를 얻을 수 있다.

Cemera motion을 estimate하는 것으로 Flow-SE3를 썼다. 이 모델은 depth를 input으로 받은 다음, frame 쌍에서 2D correspondence를 추정.

geometric reprojection error를 줄이기 위해 PnP를 unroll해서 Gauss-Newton을 실행했다...

BA Net과 비교하면, BA Net은 one joing nonlinear optimization over all variables를 했다면, 우리는 좀 더 다루기 쉬운 sub problems와 block coordinate descent를 optimization했다.



### Related Work



__Structure from Motion__



low tecture regions, occlusions, lighting changes(white balance) 등에서 성능문제가 심하게 나왔음.

여기서 SLAM이 치고들어오는데, 우리의 motion net이 LSD-SLAM이랑 유사함.

걔들은 intensity gradients를 사용했는데, 우린 대신 misalignments를 예측하도록 했음.



__Geometry and Deep Learning__



__Depth__

뭐 여러 네트워크들이 있었음. PSMNet이라던지, MVSNet이라던지. 근데 다 camera pose가 known임.



__Motion__

Video frame에서 camera motion을 estimate하는 방법들이 많이 제안되어왔음.

보통 frame 쌍들 사이에서의 상대적인 motion에 대한 예측에 focus해왔다면, 우리는 pose를 각 frame마다 update하는 방식을 도입.



__Depth and Motion__

다른 Net 까는 중...

SFMNet은 multiple frames를 depth prediction에 사용하지 않았음.

DeMoN은 2 frame 사이에서 depth와 motion을 찾아내었고, DeepTAM은 여러 frame에서 찾아내었음.

우리 방식과 classical한 SLAM들 처럼, DeepTAM은 depth와 motion estimation을 나눴음.

근데 우리는 이 2개의 모듈이 differentiablity함. DeepTAM에서 중요한 것은 incremental update에서 camera motion estimation을 식으로 푼 것.

각 학습 과정에서 DeepTAM은 keyframe을 따고, target과 rendered viewpoint에서 residual motion을 예측함.
depth와 motion 추측은 non-linear least squares problem으로 모여짐.

이에 따라 우리가 만든 Flow-SE3 module도 2D correspondence에서 6 dof camera motion update로 이루어짐.

DeMoN이나 DeepTAM에서 impose하지 못한 motion에서의 geometric constraint와 generic layer를 사용하지 못했다는 것에서 우리 모델과 차이가 있음.(먼말이고?)

BANet처럼 geometric constraint를 도입했지만, geometric reporjection error를 최소화했다는 것을 보여줌.

BA Net과 가장 다른 점으로는 우린 motion과 depth estimation을 분리했다는 것임. 이거 덕분에 depth basis가 필요없음.



### Approach
DeepV2D는 calibrated video sequence에서 depth를 예측하는 것.

__Notation and Camera Geometry__

camera projection operator와, backprojection operator를 다음과 같이 정했음. 그냥 projection하고 backprojection하는 식임.

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210108-8.png)

3.1 Depth Module

여기서 포커스하는건 2D feature extractor, cost volume backprojection, 3D stereo matching임.

__2D feature extraction__

2 stacked hourglass module로 이루어진 2D encoder는 아무튼 feature를 뽑아냄.

__Cost Volume Backprojection__

이미지 상의 pixel좌표 + depth에서 differentiable bilinear sampleing operator를 썼음.

자세한것은 아래 논문 참조.

Jaderberg, Max, Karen Simonyan, and Andrew Zisserman. "Spatial transformer networks." Advances in neural information processing systems 28 (2015): 2017-2025.

여튼 이렇게 나온 cost volume을 keyframe feature에서 뽑힌 cost volume과 합쳐서 크기를 키웠음.

이렇게 되면서 feature dimension을 decimate하지 않고 두 개의 장점을 모두 취득함.

__3D Matching Network__

만들어진 3D cost volume을 hourglass로 보내서 이러쿵 저러쿵.

3.2 Motion Module

input pose를 받아서 다음 pose를 update할 perturbations set을 내놓음

__Initialization__

keyframe이 될 frame을 일단 뽑고, initialization시킨 다음, 다른 frame들을 relative motion으로 estimation.

__Feature Extraction__

feature를 배운다네. 그럴 수 있지.

__Error Term__

Given Depth에서 equation 2를 이용해, camera i에서 이미지 하나를 warp시켜(F_j) feature map을 구함.

여기서 relative pose를 떄려넣었는데 어? 맞네? 하면 align시키는거고, noisy가 많으면 align하지 않고 error term으로 감.

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210108-9.png)

식(4)에서 X는 coordinate임. perturbation된 pose를 적용시켰을 때와 initializaion된 pose와의 차이가 error가 됨.

__Optimization Objective__

error term은 아래 식을 이용하여 weight가 축적되고, 이는 총 error term으로 적용이 됨.

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210108-10.png)

optimization은 2가지가 있음, Global Pose Optimization, Keyframe Pose Optimization.

__Global Pose Optimization__

모든 frame pair가 comparison 대상이기 때문에, global한 optimization이 필요함.

__Keyframe Pose Optimization__

우린 given frame에서 keyframe을 뽑고, 서로 다른 keyframe과의 error terms을 계산함. 음... 뭐 아무튼 그렇대.

__LS-Opitmization Layer__



3.3 Full system

__Initialization__

두 가지 전략을 취했음.

첫번째는 self initialization with constant depth map

두번째는 single image depth network에서 나온 output을 initialization.

근데 둘 다 잘 됬음.

3.4 Supervision

__Depth Supervision__

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210108-11.png)

GT와 predicted depth 사이의 L1 distance를 이용했음.

여기다 추가로 작은 L1 smoothness penalty를 추가함.

__Motion Supervision__

![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210108-12.png)

여기는 그냥 순수히 reprojection error만 비교했음.

__Total Loss__

두 개를 더했고, motion앞에 constant value를 곱하긴 하는데, 그냥 1로 뒀음.

### Experiments
실험은 NYU, ScanNet, SUN3D, KITTI에서 진행.

4.1 Depth experiments

4.2 Tracking Experiments

DeepV2D는 기본적인 SLAM System으로도 바뀔 수 있음.



### Conclusion



### Appendix

A. LS-Optimization Layer

B. Training Details

Tensorflow로 쓰여짐. 2 stages로 tarining이 됨. data augmentation은 포토샵적인 요소를 더함.

C. Timing and Memory Usage

D. Additional Tracking Information

E. Camera pose ablations

F. Additional Results

G. Network Architectures



### Reference

Teed, Zachary, and Jia Deng. "Deepv2d: Video to depth with differentiable structure from motion." *arXiv preprint arXiv:1812.04605* (2018).

Jaderberg, Max, Karen Simonyan, and Andrew Zisserman. "Spatial transformer networks." Advances in neural information processing systems 28 (2015): 2017-2025.