---
layout : post
title : "PSMNet"
date : 2021-01-09 +1406
description : 3D cost volume에 SPP의 개념을 추가한 Network인, Pyramid Stereo Matching Network 논문의 간단한 리뷰입니다.
tag : [PaperReivew]
---

### Pyramid Stereo Matching Network, CVPR 2018



  최초로 SPP를 Cost Volume에 도입한 논문. 근데 자세히 살펴보면 이 친구들이 SPP Part라고 해 놓은 것은 정확히 SPP랑 1:1 대응은 아니다. 이 부분은 나중에 Code Review 파트에서 자세히 살펴볼 계획.

 아무튼 Code가 굉장히 깔끔하게 잘 써져 있다.



### Abstract

최근 나오는 architecture들은 patch-based Siamese networks에 좀 의존하고 있음.

 ill-posed region에서 나오는 context information에 대한 것들이 굉장히 부족한데, 우리는 이를

SPP와 3D CNN으로 해결했음.



### Introduction

 여러 분야에서 Depth estimation은 필수적임.

 근데 CNN based stereo matching 방법들에겐 문제가 있음. "얼마나 결과(context information)가 effective한가?"

 몇몇 관련 연구들이 이를 해결하려고 다양한 방법을 사용했음.(Displets, ResMatchNet, GC-Net)

 우린 여기서 global context information을 exploit하기 위해 PSM이란 것을 propose함.

우리가 메인으로 생각하는 것은 다음 4가지 요소임.

- 그 어떤 post-processing없이 end-to-end learning framework를 propose함.

- global context information을 뽑아내기 위한 SPP module을 소개

- stacked hourglass 3D CNN
- KITTI dataset에서의 SOTA accuracy



### Related Work

 depth estimation에서는 굉장히 다양한 방법들이 나왔음(matching cost computation이나 cost volume optimization)

 SGM을 이용하기도 했고, Detecting incorrect labels, Replacing incorrect labels with new one, Refining the removed labels(DRR)을 쓴다던가, CRL(Casacde Residual Learning)에 대해서도 연구가 되었음. 뭐 다른 network들도 있음.

 우리는 이걸 cost volume에 적용했지만, optical flow에 적용한 사례도 있음. SPyNet이란 친구들과 PWCNet이란 친구들임.

 

### Pyramid Stereo Matching Network

__3.1 Network Architecture__

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210109-11.png)

 위와 같이 생겼음.

 H* W * C 등의 자세한 내용이 Table로 정리되어 있으나, 여기서는 생략. 논문을 직접 본다면, 위 네트워크를 어떻게 구현했는지 잘 알 수 있음.

 이후 3.4까지 계속 네트워크에 대한 설명이나, 논문을 직접 참고하여 읽는 것을 추천하므로 해당 내용은 포스팅 X

__3.2 Spatial Pyramid Pooling Module__

__3.3 Cost Volume__

__3.4 3D CNN__

__3.5 Disparity Regression__

 해당 내용은 GC Net에서 제안했던 soft argmin을 사용하였음.

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210109-12.png)

__3.6 Loss__

 Loss의 경우 smooth L1 loss function을 적용.

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210109-13.png)



### Experiments

 데이터셋은 Scene Flow, KITTI 2015, KITTI 2012 를 사용하였음.

__4.1 Experiment Details__

__4.2 KITTI 2015__

__4.3 Scene Flow__

__4.4 KITTI 2012__



### Conclusions



### Code Review

 PSM Net은 코드가 정말 깔끔하게 잘 되어 있다. 아래 github를 참조.

[https://github.com/JiaRenChang/PSMNet](https://github.com/JiaRenChang/PSMNet)

```python
def main():

	start_full_time = time.time()
	for epoch in range(0, args.epochs):
	   print('This is %d-th epoch' %(epoch))
	   total_train_loss = 0
	   adjust_learning_rate(optimizer,epoch)

	   ## training ##
	   for batch_idx, (imgL_crop, imgR_crop, disp_crop_L) in enumerate(TrainImgLoader):
	     start_time = time.time()

	     loss = train(imgL_crop,imgR_crop, disp_crop_L)
	     print('Iter %d training loss = %.3f , time = %.2f' %(batch_idx, loss, time.time() - start_time))
	     total_train_loss += loss
	   print('epoch %d total training loss = %.3f' %(epoch, total_train_loss/len(TrainImgLoader)))

	   #SAVE
	   savefilename = args.savemodel+'/checkpoint_'+str(epoch)+'.tar'
	   torch.save({
		    'epoch': epoch,
		    'state_dict': model.state_dict(),
                    'train_loss': total_train_loss/len(TrainImgLoader),
		}, savefilename)

	print('full training time = %.2f HR' %((time.time() - start_full_time)/3600))

	#------------- TEST ------------------------------------------------------------
	total_test_loss = 0
	for batch_idx, (imgL, imgR, disp_L) in enumerate(TestImgLoader):
	       test_loss = test(imgL,imgR, disp_L)
	       print('Iter %d test loss = %.3f' %(batch_idx, test_loss))
	       total_test_loss += test_loss

	print('total test loss = %.3f' %(total_test_loss/len(TestImgLoader)))
	#----------------------------------------------------------------------------------
	#SAVE test information
	savefilename = args.savemodel+'testinformation.tar'
	torch.save({
		    'test_loss': total_test_loss/len(TestImgLoader),
		}, savefilename)
```

 main은 위와 같고, default model은 stackhourglass.py이다.

```python
def train(imgL,imgR, disp_L):
        model.train()

        if args.cuda:
            imgL, imgR, disp_true = imgL.cuda(), imgR.cuda(), disp_L.cuda()

        #---------
        mask = disp_true < args.maxdisp
        mask.detach_()
        #----
        optimizer.zero_grad()
        
        if args.model == 'stackhourglass':
            output1, output2, output3 = model(imgL,imgR)
            output1 = torch.squeeze(output1,1)
            output2 = torch.squeeze(output2,1)
            output3 = torch.squeeze(output3,1)
            loss = 0.5*F.smooth_l1_loss(output1[mask], disp_true[mask], size_average=True) + 0.7*F.smooth_l1_loss(output2[mask], disp_true[mask], size_average=True) + F.smooth_l1_loss(output3[mask], disp_true[mask], size_average=True) 
        elif args.model == 'basic':
            output = model(imgL,imgR)
            output = torch.squeeze(output,1)
            loss = F.smooth_l1_loss(output[mask], disp_true[mask], size_average=True)

        loss.backward()
        optimizer.step()

        return loss.data
```

 위 코드는 train 코드. 이 위에 optimizer는 Adam으로 설정했고, learning rate는 0.001이다.

 그럼 stackhourglass.py를 살펴보자.

일단 PSMNet class에서 init은,

```python
class PSMNet(nn.Module):
    def __init__(self, maxdisp):
        super(PSMNet, self).__init__()
        self.maxdisp = maxdisp

        self.feature_extraction = feature_extraction()

        self.dres0 = nn.Sequential(convbn_3d(64, 32, 3, 1, 1),
                                     nn.ReLU(inplace=True),
                                     convbn_3d(32, 32, 3, 1, 1),
                                     nn.ReLU(inplace=True))

        self.dres1 = nn.Sequential(convbn_3d(32, 32, 3, 1, 1),
                                   nn.ReLU(inplace=True),
                                   convbn_3d(32, 32, 3, 1, 1)) 

        self.dres2 = hourglass(32)

        self.dres3 = hourglass(32)

        self.dres4 = hourglass(32)

        self.classif1 = nn.Sequential(convbn_3d(32, 32, 3, 1, 1),
                                      nn.ReLU(inplace=True),
                                      nn.Conv3d(32, 1, kernel_size=3, padding=1, stride=1,bias=False))

        self.classif2 = nn.Sequential(convbn_3d(32, 32, 3, 1, 1),
                                      nn.ReLU(inplace=True),
                                      nn.Conv3d(32, 1, kernel_size=3, padding=1, stride=1,bias=False))

        self.classif3 = nn.Sequential(convbn_3d(32, 32, 3, 1, 1),
                                      nn.ReLU(inplace=True),
                                      nn.Conv3d(32, 1, kernel_size=3, padding=1, stride=1,bias=False))

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2. / n))
            elif isinstance(m, nn.Conv3d):
                n = m.kernel_size[0] * m.kernel_size[1]*m.kernel_size[2] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2. / n))
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()
            elif isinstance(m, nn.BatchNorm3d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()
            elif isinstance(m, nn.Linear):
                m.bias.data.zero_()
```

 위와 같이 되어있다. 기본적인 network와 비슷하게 짜여져있는 것을 확인할 수 있다. 기본적인 network를 만드는 것은 horuglass라는, 같은 .py 파일 내에 존재하는 다른 클래스인데, 해당 내용은 다음과 같다.

```python
class hourglass(nn.Module):
    def __init__(self, inplanes):
        super(hourglass, self).__init__()

        self.conv1 = nn.Sequential(convbn_3d(inplanes, inplanes*2, kernel_size=3, stride=2, pad=1),
                                   nn.ReLU(inplace=True))

        self.conv2 = convbn_3d(inplanes*2, inplanes*2, kernel_size=3, stride=1, pad=1)

        self.conv3 = nn.Sequential(convbn_3d(inplanes*2, inplanes*2, kernel_size=3, stride=2, pad=1),
                                   nn.ReLU(inplace=True))

        self.conv4 = nn.Sequential(convbn_3d(inplanes*2, inplanes*2, kernel_size=3, stride=1, pad=1),
                                   nn.ReLU(inplace=True))

        # conv5 and conv6 is deconv
        self.conv5 = nn.Sequential(nn.ConvTranspose3d(inplanes*2, inplanes*2, kernel_size=3, padding=1, output_padding=1, stride=2,bias=False),
                                   nn.BatchNorm3d(inplanes*2)) #+conv2

        self.conv6 = nn.Sequential(nn.ConvTranspose3d(inplanes*2, inplanes, kernel_size=3, padding=1, output_padding=1, stride=2,bias=False),
                                   nn.BatchNorm3d(inplanes)) #+x

    def forward(self, x ,presqu, postsqu):
        
        out  = self.conv1(x) #in:1/4 out:1/8
        pre  = self.conv2(out) #in:1/8 out:1/8
        # If postsqu(next square) is None, then just relu
        # if postsquare exist, add them and relu.
        if postsqu is not None:
           pre = F.relu(pre + postsqu, inplace=True)
        else:
           pre = F.relu(pre, inplace=True)

        out  = self.conv3(pre) #in:1/8 out:1/16
        out  = self.conv4(out) #in:1/16 out:1/16

        if presqu is not None:
           post = F.relu(self.conv5(out)+presqu, inplace=True) #in:1/16 out:1/8
        else:
           post = F.relu(self.conv5(out)+pre, inplace=True) 

        out  = self.conv6(post)  #in:1/8 out:1/4

        return out, pre, post
```

여기서 pre, post가 무슨 말인가 싶을텐데, 

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210109-14.png)

 위 그림에서 layer 끼리 화살표가 그려져있는데, 이전의 layer를 pre에 저장하고, 이후 layer를 post에 저장해놓게 된다. 그래서 pre, post를 조금씩 더하게 되는 것을 코드에서 볼 수 있다. 그리고 다시 원래 PSMNet class로 돌아와서 forward 부분을 살펴보면 다음과 같다.

 바로 처음에서 refimg_fea(reference image feature)와 targetimg_fea(target image feature)를 뽑으면서 서로 weight를 공유하고, cost volume 초기화 하고 등의 과정이 담겨져있다.

```python
    def forward(self, left, right):

        refimg_fea     = self.feature_extraction(left)
        targetimg_fea  = self.feature_extraction(right)

        #matching
        # Variable means it would be like weight something.
        cost = Variable(torch.FloatTensor(refimg_fea.size()[0], refimg_fea.size()[1]*2, self.maxdisp//4,  refimg_fea.size()[2],  refimg_fea.size()[3]).zero_()).cuda()

        # concatenate part
        for i in range(self.maxdisp//4):
            if i > 0 :
             cost[:, :refimg_fea.size()[1], i, :,i:]   = refimg_fea[:,:,:,i:]
             cost[:, refimg_fea.size()[1]:, i, :,i:] = targetimg_fea[:,:,:,:-i]
            else:
             cost[:, :refimg_fea.size()[1], i, :,:]   = refimg_fea
             cost[:, refimg_fea.size()[1]:, i, :,:]   = targetimg_fea
        """
        contiguous(memory_format=torch.contiguous_format) → Tensor
        
        Returns a contiguous in memory tensor containing the same data as self tensor.
        If self tensor is already in the specified memory format, this function returns the self tensor.
        """
        cost = cost.contiguous()

        cost0 = self.dres0(cost)
        cost0 = self.dres1(cost0) + cost0

        out1, pre1, post1 = self.dres2(cost0, None, None) 
        out1 = out1+cost0

        out2, pre2, post2 = self.dres3(out1, pre1, post1) 
        out2 = out2+cost0

        out3, pre3, post3 = self.dres4(out2, pre1, post2) 
        out3 = out3+cost0

        cost1 = self.classif1(out1)
        cost2 = self.classif2(out2) + cost1
        cost3 = self.classif3(out3) + cost2

        if self.training:
            cost1 = F.upsample(cost1, [self.maxdisp,left.size()[2],left.size()[3]], mode='trilinear')
            cost2 = F.upsample(cost2, [self.maxdisp,left.size()[2],left.size()[3]], mode='trilinear')

            cost1 = torch.squeeze(cost1,1)
            pred1 = F.softmax(cost1,dim=1)
            pred1 = disparityregression(self.maxdisp)(pred1)

            cost2 = torch.squeeze(cost2,1)
            pred2 = F.softmax(cost2,dim=1)
            pred2 = disparityregression(self.maxdisp)(pred2)

        cost3 = F.upsample(cost3, [self.maxdisp,left.size()[2],left.size()[3]], mode='trilinear')
        cost3 = torch.squeeze(cost3,1)
        pred3 = F.softmax(cost3,dim=1)
        #For your information: This formulation 'softmax(c)' learned "similarity" 
        #while 'softmax(-c)' learned 'matching cost' as mentioned in the paper.
        #However, 'c' or '-c' do not affect the performance because feature-based cost volume provided flexibility.
        pred3 = disparityregression(self.maxdisp)(pred3)

        if self.training:
            return pred1, pred2, pred3
        else:
            return pred3
```

 쉽게 쓰여져 있기에 code를 읽는것에서 큰 부담 없이 넘어갈 수 있다.



### References

Chang, Jia-Ren, and Yong-Sheng Chen. "Pyramid stereo matching network." *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*. 2018.

PSMNet github, [https://github.com/JiaRenChang/PSMNet](https://github.com/JiaRenChang/PSMNet)