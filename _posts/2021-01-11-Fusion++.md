---
layout : post
title : "Fusion++"
date : 2021-01-11 +1415
description : Mask R-CNN과 traditional method를 SLAM에 적용해서 효과적인 결과를 이끌어 낸 Fusion++, Volumetric Object-Level SLAM 논문의 간단한 리뷰입니다.
tag : [PaperReview]
---

### Fusion++: Volumetric Object-Level SLAM



### Abstract

 RGB-D Camera가 indoor scene을 막 채워넣으면, Mask-RCNN으로 object마다의 TSDF를 만들어 object 크기에 의존하는 resolution과 3D foreground mask를 만든다. 이렇게 만들어진 object들은 최적화 가능한 6DoF pose graph에 저장이 된다. 또한 이러한 object들은 depth fusion, tracking, relocalisation, loop closure detection에 사용이 되며, 각 object들은 semantic information을 가져 실시간의 흐름과 existence probability에 따라 refine 될 수 있다.

 이러한 것들을 우리는 손으로 들고다닐 수 있는 RGB-D sequence로 시연을 하였다.



### Introduction

 Indoor scene의 이해와 3D mapping은 real-world에서의 자율 주행에 있어서 기본적인 기술이다. 이러한 것을 가능하게 하도록, map representation을 신중하게 선택하는 것이 중요하다. 그 중에서 특히 쓸모있는 것으로는 object-oriented map이다.  이러한 것이 매우 기본적인 것이라고 생각하고 있다.

 object level map에서는 object를 구성하는 geometric elements들이 서로 instance로 묶여 그들의 단위로 label이 되는 반면, surfel이나 point에 의한 것들은 label dense geometry에 대해 독립적인 적립적이라고 할 수 있다. 우리 system은 현재 정적인 환경에서만 유효하고, 움직이는 물체들의 track에 대해서는 아직 aim하고 있지 않다.

 우린 이 논문에서 RGB-D data에 기반한 object-oriented SLAM system을 보여줄 것이다. 우리가 목표로 하는 것은 어떠한 사전 지식 없이 semantically label된 TSDF reconstruction이다. 우린 2d instance mask예측을 위해 Mask RCNN을 사용했고, 이를 3D voxel mask와 함께 TSDF reconstruction으로 융합한다.

 기존에 사용되어왔던 dense reconstruction system들과는 달리, 우리는 전체적인 dense representation을 keep하는 것은 시도하지 않았다. 우리가 만드는 map은 단지 object instance를 recon하는 것에만 신경을 쓴다. 이러한 점들 덕분에 rigid TSDF volume을 high-quality recon으로 할 수 있도록 했다. 각 object들은 separate volume을 가지고, 그들의 작은 부분보다는 큰 부분을 적은 정확도의 TSDF volume으로 만들 수 있게 한다.(??) 미확인 구조들의 TSDF를 버리는 것은 tracking과 model의 occlusion을 해결하는 것에 도움을 주었다.

 경우에 따라 약하게 제한된 ICP tracking의 조건 하에 indoor office scene을 반복했다. real-time operation에 최적화는 아니지만 4~8Hz의 performance를 보여주었다.

 우리는 아래 contribution를 따랐다.

- 다양한 해상도의 3D instance recon을 행하는 Object-Oriented SLAM
- Voxel foreground mask와 existence probability를 이용한 per-frame instance들의 robust한 fuse
- high quality object reconstruction within loop-closed object SLAM



### Related Work

 TSDF formulation에 있어서는 아래 논문을 참조했다.

B. Curless and M. Levoy. A volumetric method for building complex models from range images. In Proceedings of SIGGRAPH, 1996

R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim, A. J. Davison, P. Kohli, J. Shotton, S. Hodges, and A. Fitzgibbon. KinectFusion: Real-Time Dense Surface Mapping and Tracking. In Proceedings of the International Symposium on Mixed and Augmented Reality (ISMAR), 2011

  SLAM++는 object detection에는 point pair feature를 이용했고 global optimisation으로 pose graph를 사용했다.

 object discovery에서는 이전에 여러 방법들이 있었으나, 보통 TSDF가 아닌 point-cloud를 이용했고, object detector를 학습시킨 것이 아닌 unsupervised segmentation을 진행했다.

  같은 장면에서 성공한 mapping들 사이에서 dense change detection로 접근한 방법들도 있으나, 이러한 system들과는 달리 우리는 online 사용이 가능하다는 점과 새로운 object가 detect되기 전 까지는 장면에서 어떠한 변화도 필요없다는 점이다.

 RGB-only SLAM으로 ORB-SLAM이라던지 MO-SLAM등이 있었다. 하지만 이러한 ORB descriptor들이 우리가 사용한 BRISK feature와 유사하지만, depth로 확장하진 않았다.

  우리랑 비슷하게 한 것으로 아래 논문이 있으나, 방법이 조금 다름.

N. Sunderhauf, T. T. Pham, Y. Latif, M. Milford, and I. Reid. ¨ Meaningful maps with object-oriented semantic mapping. In Proceedings of the IEEE/RSJ Conference on Intelligent Robots and Systems (IROS), 2017.

 그 외에도 여러 방법이있으나, high quality가 아니라던지 등의 문제가 많았음.



### Method

  ![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210111-1.PNG)

 우리의 pipeline은 위와 같음. RGB-D input에서, local tracking과 occlusion handling을 위한 background TSDF를 대충 초기화하고, pose가 충분히 바뀐다면 relocalisation + graph optimisation이 된 후, TSDF를 reset함. 각 RGB frame에서 Mask R-CNN을 씌워 object를 detect하고 existing map이랑 match가 되는지 확인함. 기존에 있던 object와 matching이 일어나지 않는다면, 새로운 TSDF object를 만들고, map에 집어넣은 다음 위 과정을 반복함. 이러한 foreground detection은 이후 foreground mask와 융합됨.

__3.1 TSDF Object Instances__

 우리의 map은 각각의 TSDF로 구성된 object instance들로 구성이 되어 있다.

__Initializing and resizing__

 k번째 frame에서 각각의 object detection i는 binary mask인 M_i^k를 제공한다. 우리는 이 mask image를, 해당 coordinate에서의 depth map D_k를 이용해서 world frame에 projection 한다.

  ![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210111-2.PNG)

 K는 3x3 intrinsic camera matrix이고, T는 camera pose estimation을 의미한다. 여기서 p는 world frame point로 되어있는 (x,y,z)라고 보면 된다. 즉, 위 식은 해당 coordinate에서의 depth map에 camera parameter를 넣고 coordinate을 곱해 world frame에서의 point를 찾아내는 것이다.

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210111-3.PNG)

 위 문단은 다른 point cloud를 기반으로 TSDF를 만든 network를 참고해야 할 것 같다. min max를 왜 취하는 지를 잘 모르겠다. (고로 추후 수정 예정)

 각각의 instance TSDF는 given axis에 대해서 정해진 resolution을 가지게 되는데, 이를 이용해서 만든 voxel의 size(v_0)는 s_0/r_0이다. 이러한 방법으로 인해 작은 object는 fine detail을 가지게 되고, 조금 더 큰 object들은 coarse하게 된다. 이러한 방법은 memory에 있어서 도움이 된다.

 추가적인 공간을 포함해서 object에 대해 새로운 detection이 발생되면, resize에 대해서 고려를 하게 된다. 이러한 것을 위해, 현재의 TSDF reconstruction에서 만들어진, 비슷한 형태로 뭉개진 point cloud와 합치게 된다.  3D volume은 위 두 가지를 모두 고려해서 계산이 된다. resizing하면서 aliasing을 피하기 위해, v_0를 multiply하고 r_0를 증가시킨다. 우린 maximum voxel resolution을 128로 제한했다.

 instance를 initialising하기 전에, 우리는 volume center를 카메라에서 5m 이내로 할 필요를 느꼈다. object center가 움직임에 따라 pose graph node와 연관된 measurement들이 따라서 update된다.

__Integration__

 depth map D에서 TSDF로 surface 요소들을 integrating하기 위해, 우리는 아래 논문에서 사용하는 방법을 따랐다.

R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim, A. J. Davison, P. Kohli, J. Shotton, S. Hodges, and A. Fitzgibbon. KinectFusion: Real-Time Dense Surface Mapping and Tracking. In Proceedings of the International Symposium on Mixed and Augmented Reality (ISMAR), 2011

 각 voxel point에서 TSDF는 truncated signed distance value와 weight를 가지고 있다. 만약 측정된 depth 값 + truncation distance mu보다 작은 depth value를 가진 voxel이 camera frame pixel로 project된다면, 이 voxel은 volume으로 합쳐질 것이다. 이러한 integration은 TSDF pixel의 50% 이상이 유효하고 ICR RMSE < 0.03 조건을 만족하면 매 frame마다 진행이 된다.

 위 과정은 mask region이던 아니던, entire volume에 대해서 integration을 진행할지 말지도 중요하다. 어떤 voxel이 intsance의 foreground와 연관성이 있는지 저장하기 위해, 우리는 instance mask detection도 융합했다. 어떤 voxel이 foreground에 속하느냐 안속하느냐를 binomial trial sampled된 결과에서 positive와 negative를 설정했다. 이러한 값들은 (alpha, beta)로 저장이 되고 그 꼴은 처음에 (1,1)로 initialized되어 있다가 beta distribution conjugate prior를 따라간다.

 미니님이 작성하신 conjugate prior 내용, [https://blog.acronym.co.kr/437](https://blog.acronym.co.kr/437)

 beta distribution: [https://en.wikipedia.org/wiki/Beta_distribution](https://en.wikipedia.org/wiki/Beta_distribution)

 binomial distribution: [https://ko.wikipedia.org/wiki/%EC%9D%B4%ED%95%AD_%EB%B6%84%ED%8F%AC](https://ko.wikipedia.org/wiki/%EC%9D%B4%ED%95%AD_%EB%B6%84%ED%8F%AC)

 ![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210111-4.PNG)

 truncation distance가 위 처럼 측정이 되면, 우린 아래처럼 mask i의 연관성을 이용해서 detection count를 update한다.

 F는 foreground인 확률을 의미하고, N은 not foreground의 확률을 의미한다. 여기서 E[p(v)]가 0.5를 초과하면 accept하도록 threshold를 잡았다.



 위 내용이 사실 조금 어려운데, F와 N을 더해서 1이 나오는 확률이 아니다. 둘은 beta distribution에서  alpha, beta에 해당하는데, 이 때 alpha가 커지면 해당 point가 foreground일 확률이 높아지고, beta가 커지면 point가 not foreground일 확률이 높아지게 된다. 아래 그림을 보면 이해가 갈 것.

 ![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210111-5.png)

 왜 두 개를 더해서 1이 되는 베르누이 distribution이 아닌  beta distribution을 썼는지는 나도 잘 모르겠다. 아무튼 Foreground는 Mask를 씌웠을 때 해당 point가 그 안에 존재하면 그만큼 F는 증가하고, N은 그대로 있게 되고, 반대의 경우에는 F가 그대로 있고 N이 증가하게 된다.(Binary mask이니 M의 값은 0 or 1)



 __Raycasting__

 tracking, data association, 시각화를 위해서 우린 depth, normals, vertices, RGB, 그리고 object들을 render한다. 각각의 object volume에 대해서 v_s^o의 size로 ray를 쏘고,(만약 mu로 normalize된 SDF값이 0.8 이하면 0.5 v_s^o의 step으로 ray를 쏨) E[p(v)]가 0.5를 초과하는 경우  point v에서 mu로 normalize된 SDF 값이 zero-crossing point인지 확인한다.(surface를 찾는다는 뜻, 그리고 SDF value는 모두 trilinearly interpolated됨) 우린 이 때의 ray length를 저장해서 another volume의 point를 찾는 행위를 피하도록 했다.

 위와 같은 과정은 ray가 occlude된 부분에서 fail이 일어나지 않도록 만들어 주었다. 만약 background TSDF가 있다면, 그리고 foreground object와의 occlusion이 일어나서 intersection이 없거나 background TSDF 보다 5cm 뒤에 intersection이 잡히게 된다면, background TSDF가 대신 사용되는 것이다.



 위 내용을 조금 정리하면... raycasting이란게 물체를 뚫고 지나가는 광선을 쏘고, 이 때 어떤 물체를 통과한다면 그 물체를 통과하기까지의 거리(즉 물체의 두께)를 return하는데, 이 때 raycasting이 return하는 두께와 SDF값이 0인(surface)값을 적절히 조절해서 물체의 TSDF를 결정하게 됨. 이 때 물체의 두께를 저장해놔서, 다른 frame에서 같은 장소를 반복측정하는 것을 피하는 것을 말하는 것 같음.

 raycasting에 대한 내용은 아래 ZZEN님이 작성하신 포스팅 참조.

[https://nuguziii.github.io/cg/CG-001/](https://nuguziii.github.io/cg/CG-001/)



__Existence Probability__

 잘못 detect하는 경우를 고려하여, 해당 instance의 존재 확률을 beta distribution으로 결정했다. 만약 어떤 물체가 특정 volume이상을 가지고 있다면, existence count e_0를 증가시키고, 가지고 있지 않다면 non-existence count d_0를 증가시킨다. 만약에 E[p(o)]가 0.1 밑으로 떨어진다면, 해당 instance는 삭제해버린다. 이는 pose graph에서도 삭제되게 된다.

__Semantic Labels__

 각 TSDF는 해당 class l_o에 따라서 probability distribution을 저장한다. Mask R-CNN은 given image __*I_k*__에서 해당 label이 존재할 확률인 p를 반환한다. 우린 이를 standard multiplicative bayesian update scheme을 찾았다.

 ![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210111-6.PNG)

 위 식에서 Z는 normalising constant이다. 근데 우린 이거 안썼음.~~(이럴거면 왜썼냐?)~~

![img7](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210111-7.PNG)

 위 식처럼 단순하게 average를 사용했음.

__3.2 Detection and Data Association__

 Mask R-CNN model로 detection한 결과는 어떤 instance(i)인가, binary Mask value(M), class probability distribution p를 가지고 있다.  그런데 이 과정에서 시간이 엄청 오래 걸림. 우린 GPU memory efficiency를 위해, 100개의 detection(이 100개만 잡는 기준은 어떤 논문을 참조했음, 중요하지 않아보여서 여기선 생략)을 잡고, 여기에서 image border 근처가 아닌 경우(20px 넘게 떨어진 경우)를 거르고 max prob가 0.5 초과, Mask 값의 합이 50^2이 넘는 경우만을 고려했다.

 local tracking 이후 우리는 camera pose와 TSDF를 estimate했다. 이 때, detection area의 두 부분이 교차점을 계산하고, 이러한 점에 argmax를 날렸을 때 instance o가 0.2를 초과하는 값을 가지면 assign하고 그렇지 않다면 쳐냈다. 이러한 integration step 덕분에 같은 instance를 map하는 detection이 detection mask과 class probability의 평균과 합쳐질 수 있었다.

__3.3 Layered Local Tracking__

 우린 occlusion의 경우를 고려하고, instance가 없을 때 지역적인 frame-to-model tracking을 돕기 위해 background TSDF를 유지하려고 했다.(?)  대충 해석해보자면... 2.56m 떨어져 있는 경우로 intialization을 해 두고, 2.56m 떨어진 거리를 계속해서 계산 했을 때 이 값이 1.28m를 넘어가면 다시 초기화 하도록 했다.

 ICP value(r_icp)와 point-to-plane error(E_icp)는 다음과 같다.

![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210111-8.PNG)

 위에서 r은 reference frame, l은 live frame을 의미한다. V_valid는 다음 식을 만족하는 u_l들의 집합을 의미한다.

![img9](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210111-9.PNG)

 우린 이러한 non-linear least square를 gauss-newton algorithm을 이용해서 minimize 시켰다. 

![img10](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210111-10.PNG)

 대충 위와 같은 방법을 이용해서 minimize했다고 함.

 추가로 live frame에서의 pixel에 대해서도 같은 system을 추가하기 위해, 다음과 같은 ICP RMSE의 error metric을 써서 pose graph에 사용했음.

![img11](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210111-11.PNG)

__3.4 Relocalisation__

 만약 system이 coarse TSDF를 잃거나 reset을 한다면, 현재 frame을 기준으로 instance나 frame을 기준으로 relocalisation을 진행함. 이러한 relocalisation에 있어서 몇몇 좋은 알고리즘들이 있지만, 우린 여기서 BRISK feature를 사용했음. 만약 어떤 물체를 기준으로 15도를 기울였을 때 새로운 snapshot이 나온다면, 우린 이 때 해당 pose에서의 object snapshot을 추가함.

 여기서 class distribution prediction이 0.6을 넘는다면, 3D-3D RANSAC을 사용했고, inlier feature를 뽑는 것에 있어서 OpenGV를 사용했다.

 여튼 새로운 pose가 잡히게 되면 3D-3D RANSAC과 OpenGV를 이용해서 relocalisation을 진행하였음.

__3.5 Object-Level Pose Graph__

 우리가 사용한 Pose Graph는 SLAM++에서 사용한 것과 유사함. every frame마다 Mask R-CNN이 detection한 것을 camera pose node에 추가하고, 새로운 instance가 발견이 되면 초기화한다. 첫번째 camera pose node는 fixed 되어있고, world frame의 origin으로 잡히게 된다. 각 node들은 object to world 혹은 camera to world의 transformation 내용을 담고 있다.

 각 노드들 사이의 값은 ICP error term만 가지고 있다. 이 때 graph factor의 error로는,

![img12](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210111-12.PNG)

  그리고 perturbation(매우 작은 변화. 카메라가 움직이면서 촬영을 한다고 할 때, 각 frame마다의 변화는 상당히 작은 편인데, 이 조금의 변화를 perturbation이라 함) 이 때 pose graph와 icp값의 관계는 아래와 같다. (어렵다. 해석 생략)

![img13](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210111-13.PNG)

 여하튼, 위와 같은 방법을 거치고 나면, 결국 아래와 같은 결과가 나온다.(Final error to be minimised in the pose graph)

![img14](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210111-14.PNG)



### Experiments

 우리의 Mask-RCNN은 ResNet 101 base model을 사용했고, COCO pretraining을 썼다.

__4.1 Loop Closure and Map Consistency__

__4.2 Reconstruction Quality__

__4.3 RGB-D SLAM Benchmark__

__4.4 Memory and Run-time Analysis__



### Conclusions



### References

McCormac, John, et al. "Fusion++: Volumetric object-level slam." *2018 international conference on 3D vision (3DV)*. IEEE, 2018.

B. Curless and M. Levoy. A volumetric method for building complex models from range images. In Proceedings of SIGGRAPH, 1996

R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim, A. J. Davison, P. Kohli, J. Shotton, S. Hodges, and A. Fitzgibbon. KinectFusion: Real-Time Dense Surface Mapping and Tracking. In Proceedings of the International Symposium on Mixed and Augmented Reality (ISMAR), 2011

N. Sunderhauf, T. T. Pham, Y. Latif, M. Milford, and I. Reid. ¨ Meaningful maps with object-oriented semantic mapping. In Proceedings of the IEEE/RSJ Conference on Intelligent Robots and Systems (IROS), 2017.

