---
layout : post
title : "T2T-ViT"
date : 2021-05-22 +0900
description : ViT에 효율적인 backbone과 Tokens-to-Token 개념을 도입하여 transformer-based model의 무게를 줄이고 성능을 향상시킨 T2T-ViT을 소개한 논문의 리뷰입니다.
tag : [PaperReview]
---

### Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet



### Abstract

 언어 모델링에서 인기가 많은 Transformer는 최근 vision task를 해결하는 것에 많이 이용이 되고 있다. 좀 쉽게 말하자면, image classification에 사용되는 Vision Transformer(ViT) 이다. ViT 모델은 classification에서 global relation을 설계할 수 있도록 각 image를 정해진 길이를 가진 일련의 token으로 나누고 여러개의 transformer layer에 적용을 시킨다. 하지만, ViT는 imagenet과 같은 중간 크기의 dataset에서 학습을 시키는 경우 inferior한 performance를 보여주는 것을 확인하였다. 우리는 이러한 이유가,

- 단순히 input image를 tokenization하는 것은 모델이 edge나 line등과 같은 중요한 local structure를 model하는 것에 실패해서 low training sample efficiency가 나오는 것.
- 여럿 쓸모 없는 attention backbone이 정해진 computation budget을 위해 feature richness가 제한되고 이로 인해 training sample이 제한 되는 것.

 이러한 한계를 극복하기 위해, 우리는 Tokens-To-Token Vision Transformer라는 T2TViT를 제안한다. 이는,

- 주변 token들을 하나의 token으로 recursive하게 합침으로써 image to token의 구조화를 위한 __layer-wise Tokens-to-Token transformation__ , 즉 token에 둘려싸여진 local structure가 model될 수 있고 token의 길이가 감소하는 것
- empirical study를 통해 CNN architecture desgin에 영감을 받은 깊고 얇은 효율적인 ViT backbone

 먼저 알려주자면, T2T-ViT는 평범한 ViT에 비해 parameter의 수와 MACs(Multiply-accumulate 의 약자이다. 곱하는 연산이 얼마나 들어가는가.. 를 의미하는 듯) 가 반틈임에도 불구하고 ImageNet dataset에서 scratch로 실행한 결과 3%의 성능 향상이 있었다. 다른 ResNet이나 MobileNets와 비교하였을 때에도 성능 향상이 있었다.



### Introduction

 Transformer와 같은 언어 모델을 위한 self-attention model들은 최근 image classification, object detection, denoising, super-resolution 등과 같은 여러 분야에서 잘 적용이 되고 있다. 그들 중에서, ViT가 처음으로 full-transformer model을 image classification task에 적용했다. 일반적으로, ViT는 각 image를 14x14 혹은 16x16의 patch(즉, token)으로 쪼개어 language model처럼 transformer에게 전해주게 된다. ViT는 이러한 layer들을 토대로 classification이 될 수 있도록 이러한 token들 사이의 global relation을 찾게 된다.

 ViT의 full-transformer architecture가 vision task에서 중요하다는 것을 증명을 하였지만, 여전히 그 성능은 CNN에 비해서는 그 성능이 크게 좋지 못했다.(중간 사이즈의 dataset에서 학습을 진행한 경우) 우리는 이러한 performance gap이 다음과 같은 2가지 이유라고 생각했다.

- 직접적으로 image를 tokenization하는 것은 ViT가 edge나 line과 같은 local structure를 설계하는 것을 매우 힘들게 하고 이로 인해 매우 많은 data를 필요로 하게 된다.
- ViT의 attention backbone은 vision task에서의 CNN처럼 잘 설계된 구조가 아니다. 이는 feature의 richness를 제한하고 model training을 어렵게 한다.

 우리의 가설을 증명하기 위해, 우리는 T2TViT(논문에서 제안하는 것), ViT-L/16, ResNet에서의 조사를 다음 그림과 같이 진행하였다.

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210522-1.PNG)

 우리는 위 그림에서 ResNet이 desired local structure(line, edge)를 잘 잡아내고 있다는 것을 bottom layer(conv1)에서 middle layer(conv25)로 가면서 확인할 수 있다. 하지만, ViT의 경우에는 조금 다르다. global relation이 모두 attention block에 잡혔지만 structure information의 경우 poor한 것을 확인할 수 있다. 이러한 관찰은 일반적인 ViT는 token이 정해진 길이로 직접 splitting images가 되어 transformer의 input이 될 때 local structure를 무시할 수 있다는 것을 의미한다. 하지만, 우리는 ViT의 많은 channel에 0 value로 가득찬 것을 확인할 수 있었다. ViT backbone은 ResNet만큼 효율적이지 않으면서 training sample이 충분하지 않을 때에는 feature richness가 제한되는 점이 있다.

 우리는 새로운 full-transformer vision model을 이러한 limitation을 극복하는 방면에서 접근해보았다.

- naive tokenization을 사용하는 것이 아니라, Tokens to Token을 이용한, progressive tokenization module을 제안한다. 구체적으로, Token-to-Token step에서 transformer layer의 token output들은 서로 겹쳐진 형태로 reconstruction이 된다. 그러므로 주변의 patch들에서 오는 local structure가 다음 transformer layer의 input이 될 수 있도록 token에 embedding이 된다. T2T를 iterative하게 진행하면서, local structure는 token에 새겨지게 되고, aggregation process를 통해 token의 크기는 점점 줄어들게 된다.
- vision transformer에 적합한 효율적인 backbone을 찾기 위해 우리는 CNN에서 feature richness를 챙길 수 있는 transformer를 만드려고 deep-narrow architecture를 찾았다. 이는 적은 채널을 가지고 있지만 더 많은 layer를 가지고 있다. 이러한 결과로 만들어진 ViT는 적은 연산수에도 더 좋은 성능을 냈다.

 구체적으로, 우리는 Wide-ResNets, DenseNet, ResneXt, Ghost operation, channel attention 등을 참고하여 deep-narrow structure가 ViT에 가장 효율적인 것임을 깨닫게 되었다. 이는 CNN에서 효율적인 구조라면 Transformer에서도 효율적인 구조가 될 수 있음을 의미한다.

 T2T model과 deep-narrow backbone architecture를 기반으로, 우리는 T2T-ViT를 만들었다. 그 성능비교는 아래와 같다.

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210522-2.PNG)

 요약하자면, 우리의 contribution은 다음과 같다.

- T2T module과 efficient backbone
- novel progressive tokenization for ViT
- 우리가 제안한 모델이 feature richness와 redundancyㄹ를 감소시키는 것에 효과적.

### Related Work

__Transformers in Vision__

__Self-attention in CNNs__



### Toknes-to-Token ViT

 간단한 tokenization의 한계와 ViT backbone의 비효율적인 것을 극복하기 위해, 우리는 점진적으로 image를 token화 시키고 효율적인 backbone을 가진 T2T-ViT를 제안한다. T2T-ViT는 크게 2가지의 main component를 가지고 있다. 

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210522-3.PNG)

- layer-wise __Tokens-to-Token module__ 
- 효율적인 T2T-ViT backbone

이다. 우리는 쓸모없는 부분을 없애고 몇몇 CNN architecutre에서 연구된 feature의 richness를 향상시키기 위해 deep-narrow structure를 사용한다.

__3.1. Tokens-to-Token: Progressive Tokenization__

 T2T module은 단순한 tokenization의 한계를 극복하기 위해 만들어졌다. 이는 점진적으로 이미지를 tokenization하고, local structure information을 model하는 것으로 token의 길이를 점차 줄일 수 있었다. 각 T2T process는 2 step으로 이루어져있다. Reconstruction과 Soft Split이다.

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210522-4.PNG)

__Reconstruction__

 Figure 3에 나와있는 것과 같이, 주어진 일련의 tokens T는 self-attention block에 의해 변하게 된다.

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210522-5.PNG)

 위 식에서 MSA는 multihead self attention을, MLP는 multilyaer perceptron을 의미한다. 그리고 난 이후 우리는 결과물인 token을 spatial dimension으로 다시 재구성한다.

![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210522-6.PNG)

__Soft Split__

 Figure 3에 나와있는 것과 같이, 재구조화된 이미지 I를 얻고 난 이후, 우리는 local structure information과 length of token을 줄이기 위해 soft split을 적용한다. 구체적으로, 재구조화된 이미지에서 token을 만들어내는 도중에 information의 loss를 피하기 위해 우리는 이를 overlapping해서 patch를 나눈다. 이러한 방법을 통해, 각 patch는 주변의 patch들과 연관성을 가지고, 주변의 token들 사이에서 더 강한 correlation을 만들수 있게 된다. patch로 나누어진 token들은 다른 token들로 합치게 되고, 그러므로 local information은 주변의 pixel과 patch와 융합이 된다.

 soft split을 수행하면서, 각 patch의 size는 $k\times k$ 가 되고, $s$ 의 overlapping과 $p$ 의 padding을 가지고 있다고 하면, $k-s$ 는 stride와 같게 된다. 그래서 우리는, soft split을 지난 이후의 length가 다음과 같이 되는 것을 확인할 수 있다.

![img7](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210522-7.PNG)

 이렇게 나뉘어진 patch는 각각 $k\times k \times c$ 의 크기를 가지게 된다. 이는 잘 펴져서 다음 transformer의 input으로 들어가게 된다.

__T2T moudle__

 위의 re-structurization과 soft split을 반복적으로 수행함으로써, T2T module은 점진적으로 token의 길이를 줄이고 spatial structure를 변환한다. T2T module의 iterative process는 다음과 같다.

![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210522-8.PNG)

주어진 input image에 대해서, 우리는 처음 soft split을 진행하고 마지막의 output은 일반적인 token을 가진 채 나오게 된다.

 추가적으로, normal case의 ViT보다 T2T module의 token 길이가 더 길기 때문에, MACs와 memory usage는 더 크게 된다. 이를 해결하기 위해, T2T module에서는 T2T layer의 channel을 줄이고, 몇몇 transformer layer를 performer layer로 변경해서 memory usage와 GPU memory limited를 해결했다. 우리는 standard transformer layer와 performer layer를 적용했을 때의 차이를 실험에서 보여준다.

__3.2. T2T-ViT Backbone__

 평범한 ViT에서는 backbone의 많은 channel 수가 invalid함에 따라, 우리는 효율적인 bacbkone을 찾을 계획을 세웠다. 우리는 서로 다른 architecture design을 시험해보았고 CNN에서 효율과 성능을 올릴 수 있는 몇몇 구조에 대한 연구도 진행했다. 각 transformer layer는 ResNet과 같이 skip connection을 가지고 있었기 때문에, 바로 생각해볼 수 있는 것으로는 dense-connection이나 Wide-ResNet이나 ResNeXt 구조이다. 우리는 다음과 같은 5가지의 CNN architecture를 ViT에 적용시켜봤다.

- Dense connection
- Deep-narrow vs shallow-wide structure as in Wide ResNets
- Channel attention as SE Net
- more split heads in MHA layer as ResNeXt
- Ghost operations as GhostNet

 이들에 대한 자세한 구조는 appendix에 나와있다. 우리는 section 4.2에서 extensive experiments를 수행한다. 경험적인 결과를 통해 우리는,

- deep-narrow가 가장 좋았음
- SE가 ViT에 효과적이긴 하였으나, deep-narrow structure를 사용하는 것보단 덜함

 이러한 발견을 바탕으로, 우리는 deep-narrow architecture를 바탕으로 우리의 T2T-ViT backbone을 설계하였다. 구체적으로, 적은 channel 수와 hidden dimension을 가지지만 더 많은 layer를 가지게 된다. T2T module의 마지막 layer에서 나오는 정해진 길이의 token을 위해, 우리는 class token을 합치고 난 이후 Sinusoidal Position Embedding을 수행한다.

![img9](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210522-9.PNG)

 위 식에서 E는 sinusoidal position embedding을, LN은 layer normalization을, fc는 fully connected layer를, y는 output prediction을 의미한다.

__3.3. T2T-ViT Architecture__

 T2T-ViT는 두 가지 part로 나뉘어져 있다. 먼저 T2T module과 T2T backbone이다. 우리는 T2T module에 대해서 다양한 실험을 진행할 수 있는데, Figure 4에서 n은 2로 두었다. three soft split을 위한 patch size P는 [7, 3, 3] 을 가지게 되고, overlapping의 크기는 [3, 1, 1] 이 된다. 식(3)에 의해, output의 size는 14x14가 된다.(input은 224x224)

 T2T-ViT backbone은 정해진 길이의 input을 token으로 받게 되지만, deep-narrow architecture가 다르다. 예를 들자면, T2T-ViT-14는 14개의 transformer layer를 가지며 hidden dimension은 384를 가지게 되는데, ViT-B/16의 경우에는 12개의 transformer layer를 가지면서 768개의 hidden dimension을 가지게 된다. 이는 T2T-ViT 14보다 3배 큰 parameter와 MACs를 가진다.

 그 외 다른 수많은 architecutre와 비교를 했다.

![img10](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210522-10.PNG)



### Experiments

 우리는 T2T-ViT 실험을 ImageNet에서 진행했다.

- T2T-ViT가 scratch부터 ImageNet으로 train이 가능하다는 것을 보여주고, 다른 network들과 비교한다.
- 우리는 다섯가지의 CNN architecture에 영향을 받은 T2T-ViT backbone architecture의 성능에 대한 연구를 했다.
- Ablation study 진행

__4.1. T2T-ViT on ImageNet__

__T2T-ViT vs. ViT__

![img11](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210522-11.PNG)

__T2T-ViT vs. ResNet__

![img12](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210522-12.PNG)

__T2T-ViT vs. MobileNets__

![img13](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210522-13.PNG)

__Transfer learning__

![img14](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210522-14.PNG)

__4.2. From CNN to ViT__

__Deep-narrow structure benefits ViT:__

__Dense connection hurts performance of both ViT and T2T-ViT:__

__SE Block improves both ViT and T2T-ViT:__

__ResNeXt structure has few effects on ViT and T2T-ViT:__

__Ghost can further compress model and reduce MACs of T2T-ViT:__

![img15](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210522-15.PNG)

__4.3. Ablation study__

__T2T module__

__Deep-narrow structure__

![img16](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210522-16.PNG)



### Conclusion



### Reference

Yuan, Li, et al. "Tokens-to-token vit: Training vision transformers from scratch on imagenet." *arXiv preprint arXiv:2101.11986* (2021).

