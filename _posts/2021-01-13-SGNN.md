---
layout : post
title : "SG-NN"
date : 2021-01-13 +1227
description : SG-NN, Sparse Generative Neural Networks for Self-Supervised Scene Completion of RGB-D Scans의 간단한 리뷰입니다.
tag : [PaperReview]
---

### SG-NN: Sparse Generative Neural Networks for Self-Supervised Scene Completion of RGB-D Scans, CVPR 2020



 요약: '덜 완성된' input A를 받는데, 여기서 '더욱 덜 완성된' 가상의 input a를 만들어서, a가 A에 가까워지도록 학습시켜보니 결과가 훨씬 좋았음. model의 무게도 가벼워지고 여튼 좋았다.

 한계점으로는 coloring등을 추가해서 semseg을 하는 것이 더 좋아보인다.

 건질만한 내용으로는, ablation test에서 확인한 결과 TSDF꼴을 사용하는 것이 성능이 좋았다 라는 것 정도.



### Abstract

 우리가 이번에 제안하는 것들은 실제 세상이나 imcomplete scene에도 적용이 가능한 model이다. training set을 통틀어 generalization을 진행하여, 전체적으로 완성된 geometry의 3D scan 없이도 3D Scene을 예측할 수 있다. 우리가 제안하는 모델은 coarse-to-fine 에서도 사용이 가능하다.



### Introduction

 Kinect, Tango, RealSense와 같은 RGB-D sensor를 이용한 RGB-D reconstruction의 진행을 관찰해왔다. 하지만 이러한 발전 중에서도, scanning process의 물리적인 한계로 인해 occlusion이 일어아는 공간에서는 부적절한 3D recon이 이루어지는 것은 여전히 남아있다. 이러한 문제점은 video game, AR/VR과 같은 3D content 뿐 아니라 robotics 등에도 적합하지 않은 결과를 내게 된다.

 이러한 점들을 극복하기 위해 여러 geometric 기술들이 발전되어 왔다. poisson surface, CAD shape-fitting techniques 등등. 엄청난 양의 dataset인 ShapeNet이나 SUNCG등으로 학습하는 것에 의존하지만 아무튼 성능은 좋았음. 문제점이라면 역시 이걸 다 complete하게 scan해야 하는 점.(아마도 data의 질이나 양을 말하는 듯하다)

 이러한 점을 개선하기 위해 partial real world data만으로도 학습이 가능한 모델을 제시함. unknown area를 mask로 벗겨내서 조금 더 complete한 3D model을 생성하는 것이 main idea이다.

 loss function은 서로 다른 부분을 잘라 낸 것에 대해서 correlate을 할 수 있다.(??)

 이와 더불어 sparse generative neural network architecture를 새로 제안한다. 처음에는 low resolution으로 3D scene을 예측하고, training process에 따라서 surface resolution을 증가시킨다.

 아래 메인 contribution이 있다.

- incompleteness의 차이에서 pattern을 찾아 부적절한 dataset 혹은 scan data에도 조금 더 완전한 scene을 만들 수 있는 방법
- high resolution에서는 sparse TSDF를 만들기 위한, sparse convolution을 위한 generative formulation



### Related Work

__RGB-D Reconstruction__

 VoxelHashing이나 BundleFusion처럼 KinectFusion을 이용해서 recon을 하는 방법엔 surface recon을 하기 위해 TSDF를 적용하는 등의 움직임이 있었다.

 하지만 여전히 이러한 sensor들로는 perfect scan이 힘들다는 점이 존재한다.

__Deep Learning on 3D Scans__

 ScanNet이나 Matterport같은 large-scale 3D scan의 이용은 deep learning에 있어서 더욱 진보를 가져왔다.

 이러한 data들은 대개 2D data로 저장이 되어 있으나, 3D로 확장이 되면 DF와 같은 grid representation을 따르거나 occupancy grid를 조작해야했다. 이러한 것들은 높은 연산량과 memory cost가 바탕이 되었기에, 3D data에서 어떻게 sparse한 data를 받아 학습 시키는 것이 좋은가에 대한 연구가 되고 있다. PointNet은 semseg과 classificiation task로 point cloud data를 학습시키는 DNN 을 소개했다. Octree는 매우 memory efficient하지만, single ShapeNet-style object에서만 유효했다. 

 이들과 다른 방법으로는 3D CNN을 sparse하게 적용시키는 option이 있다.

__Shape and Scene Completion__

 3D scan을 하는 방법은 잘 연구되고 있다. Poisson surface recon과 같은 전통적인 방법도 있고, 최근에는 DNN을 기반으로 하는 방법들이 많이 소개되고 있다. SSCNet이나 ScanComplete 같은 내용들. 하지만 이러한 network들은 output resolution의 제한이 있음. 더욱이 complete 3D scan data에서 supervised되기 때문에, synthetic 3D scene data를 학습시킬 필요가 있음.

 DeepSDF나 Occupancy Network와 같은 대안들도 있으나, varying-sized environment에선 그 문제가 큼.



### Method Overview

 RGB-D input은 TSDF로 표현이 됨. output 역시 sparse TSDF로 표현이 됨.(marching cube를 통해서 만들어짐) 

 occlusion이나 physical sensor limitation으로 complete한 3D data를 얻을 수 없으니, mask를 이용해서 unobserved space는 날려버리고 잘 된 부분만 학습시키는 것으로 성능 향상.

 high-resolution output을 위한 우리의 모델 이름은 Sparse Generative Neural Networks(SG-NN)이라 함. surface geometry만에 의해서 compelling한 semseg을 만듬(?)

 

### Self-Supervised Completion

 data의 특성이 남을 뿐 아니라, 조금 '더 부정확한 scan'에서 '그나마 나은, 그래도 부정확한 scan'으로 가는 input과 target 사이의 연관성을 유지하도록 input을 만든다. 

 ![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210113-1.PNG)

 위 그림에서 given data는 좌측이고, 여기서 몇몇 depth frame을 지워서  more incomplete data를 만든다. 이렇게 만들어진 dataset에 mask를 씌워 해당 부분은 제거를 한다.(mask는 orange색) 이러한 방법을 거치면 더 완전한 prediction이 나온다.(고 한다)

__4.1 Data Generation__

 우리가 사용하는 dataset은 Matterport3D에서 사용된 건데, 얘내들은 raw RGB-D data를 씀. frame에서 주어진 것 중 약 50%를 마구잡이로 날리고(Section 6에서 부가설명 예정) 이를 씀.



### Generating a Sparse 3D Scene Representation

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210113-2.PNG)

 모델은 위와 같다. TSDF로 표현된 partial scan은 3D sparse convolution으로 encode되고, 이러한 TSDF value들은 input feature로 입력이 된다. encoder convolution에서는 그 크기가 2배씩 줄어들고, 이러한 것은 dense 3D grid로 변환된다(?) 

 우린 coarse occupancy O_o와 TSDF S_o에서 feature map F_o를 생성한다. 이후 coarse occupancy O_o를 base로 scene을 예측한 sparse representation을 만든다. 다음 feature input은 F, O, S의 concat인데, coarse occupancy의 sigmoid가 0.5를 초과하는 경우만 해당한다. 

 마지막 계층의 output인 O, S, F는 output SDF를 예측하고 정제하기 위해 sparse convolution set의 input으로 들어간다.

__Sparse skip connections__

 U-Net에서 했던 것 처럼, spatial resolution이 같은 곳으로 skip connection을 썼다. 근데 우리의 경우 same set of sparse location이 아닌데,(둘 다 엄청 sparse하기 때문에) 그렇기에 destination location을 공유하는 set들의 feature를 concatenate했다.

__Progressive Generation__

 조금 더 효과적이고 안정적인 학습을 위해, 우린 low resolution에서 시작하고 N iteration 이후 연이은 계층 level을 소개했다. 각 계층 레벨은 다음 level에 TSDF가 얼마나 차 있는지 확인하고, figure 4에서 확인할 수 있듯이 coarse prediction에서 refinement를 가능하게 한다.

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210113-3.PNG)

 __Loss__

 우린 마지막에 예측된 TSDF location과 그 값들을 loss로 함수화시켰다.(target location에 있는 TSDF의 value와의 l1 loss를 측정.) surface geometry의 정확도를 높이기 위해l1 loss를 쓰기 전에 TSDF에다 log-transform을 시켰다. 또한, 우린 각각의 target TSDF 값에 해당하는 l1과 target occupancy의 binary cross entropy를 사용하여 output O와 TSDF S의 proxy loss를 추가했다. 이러한 방법은 예측되지 않은 geometry에 대해서 zero loss를 반환하는 문제를 피할 수 있게 만들어주었다.(왜...?)

__5.1 Training__



### Results and Evaluation

 ![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210113-4.PNG)

__Can self-supervision predict more complete geometry than seen during training?__

 잘됬음. 논문의 Figure 7 참조.

__Comparison of our self-supervision approach to masking out by random crops__

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210113-5.PNG)

 __What's the impact of the input/output representation?__

ablation study가 위 그림에 나와있으니 참조.

TSDF로 output 한 것이 훨씬 결과가 좋았음.

__What's the impact of the degree of completeness of the target data during training?__

![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210113-6.PNG)

incompleteness에 따른 loss

 __Limitations__

 full 3D scene을 만들기 위해서는 coloring이 필요함(흠?)  scene에서 물체를 분별하는 작업이 필요한듯.(아무래도 물체마다 tsdf가 있는 것이 조금 더 합리적인 결과를 낼 수 있기 때문에.)



### Conclusions



### Acknowledgement



### References

Dai, Angela, Christian Diller, and Matthias Nießner. "SG-NN: Sparse Generative Neural Networks for Self-Supervised Scene Completion of RGB-D Scans." *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2020.

