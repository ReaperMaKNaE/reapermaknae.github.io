---
layout : post
title : "Mesh RCNN"
date : 2021-02-28 +1600
description : Mesh R-CNN논문의 간단한 리뷰입니다.
tag : [PaperReview]
---

### Mesh R-CNN, ICCV 2019



__SUMMARY__ : Mask RCNN으로 구한 object들을 이용하여 3D Mesh를 image에 입히는 방식.



### Abstract

 2D perception의 급속 발전은 시스템들이 real-world image에서 object를 detect하는 것을 정확하게 만들어 주게 되었다. 하지만, 이러한 2D로 예측된 것들은 실제 세계에서 3D 구조를 무시하게 된다. 현재, 3D 형태 예측의 진보는 인공적인, 독립된 object들에 대한 것들로 focus되어있다. 우리는 두 가지 영역에서 advance를 융합했다. real-world image들 안에서 object를 detect하는 system과 각각 detect된 object의 3D 형태를 만들 수 있는(triangle mesh로) system을 제안한다. 우리의 system인, Mesh R-CNN은, mesh의 vertex와 edge위에서 graph convolution network를 refine하고 mesh로 변환해주는 coarse voxel representation의 첫번째 예측에 의한, 다양한 위상학적 구조들과 mesh를 만들어내는 mesh prediction branch를 추가한 Mask R-CNN의 확장형이다. 우리는 우리의 data를 ShapeNet으로 학습시켰고, Pix3D에 적용시켜보았다.



### Introduction

 ![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210228-1.PNG)

 ![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210228-2.PNG)



### Related Work

__2D Object Recognition__

__Single-View Shape Prediction__

__Multi-View Shape Prediction__

__3D Inputs__

__Datasets__



### Method

 우리의 목표는 single image input에서 모든 object를 detect하고 output으로 category label, bbox, segmentation amsk와 3D triangle mesh를 내뱉는 것이다. 우리의 system은 real world image에서도 적용이 가능해야 하고, end-to-end로 train이 되어야 한다. 우리의 output mesh는 복잡한 real-world obejct들의 다양한 변화에도 적용이 가능하도록 어떤 국한된 위상학적 형태에만 제한되어서는 안된다. 우리는 이러한 목표를 현재 존재하는 2D sota model과 3d shape prediction을 합침으로써 달성하였다.

 구체적으로, 우리는 2D perception system으로 Mask RCNN을 사용하였다. Mask RCNN은 end-to-end로 object를 detect하게 되는 Mask R-CNN이다. input은 single RGB image이고, output으로 bbox, category label, segmentation mask이다. image는 처음 backbone network인 FPN과 이후 RPN을 지나 object classification과 mask prediction branch로 진행하게 된다.

 Mask RCNN의 성공의 일부분은 final prediction branch에서도 사용이 되는 feature와 input image사이의 alignment를 유지하면서 image feature에서 region feature를 추출해내는 ROIAlign이다. 우리는 3D shape을 뽑아내면서 비슷한 feature alignment를 유지하는 것에 목적을 두었다.

 우리는 새로운 mesh predictor인, voxel branch와 mesh refinement branch를 소개한다. voxel branch는 처음 object의 coarse voxelization을 예측하고 mesh refinement branch가 초기 mesh릐 vertex position을 조정한다.(mesh의 가장자리에서 작동하는 graph convolution layer의 sequence를 사용함으로써)

 voxel branch와 mesh refinement branch는 Mask RCNN의 box branch와 mask branch의 쌍과 상응한다. 모든 input image-aligned feature는 RPN 제안과 동일하다. voxel과 mesh loss는 아래에 자세히 서술되겠지만, box와 mask loss에 더해지며 모든 system은 end-to-end로 training이 가능하다. output은 predicted object score, mask, 3D shape으로 합쳐진 box들의 집합이다. 우리는 이러한 system을 Mesh RCNN이라 부르고, 그 구조는 아래 Figure 3에 나와있다.

 ![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210228-3.PNG)

 우리는 다음으로 voxel branch와 mesh refinement branch로 이루어진 우리의 mesh predictor에 대해 설명할 것이다.

__3.1 Mesh Predictor__

 우리 system에서 가장 중요한 것은 object의 bbox에 align된 convolutional feature를 받아서 full 3D shape의 triangle mesh를 output으로 내뱉는 mesh predictor이다. Mask R-CNN과 같이, 우리는 input image와 ROIAlign, VertAlign의 process에서 사용되는 feature 사이의 연관성을 유지한다. 우리의 목표는 image의 모든 object들의 instance-specific 3D shape을 잡아내는 것이다. 그러므로, 각 예측된 mesh는 반드시 instance specific topology와 geometry를 가지고 있어야 한다.

 우리는 shape inference operations의 set을 배치시키면서 다양한 mesh topology를 예측한다. 첫번째로, voxel branch는 mask rcnn의 mask branch와 유사하게 obejct의 형태를 voxelized shape으로 예측한다. 이러한 prediction은 mesh refinement head를 이용하여 mesh로 변환되고, 수정되며 이후 마지막으로 predicted mesh를 내뱉게 된다.

 mesh predictor의 마지막 output은 triangle mesh로, $T=(V,F)$이다.

 여기서 $V=\{v_i \in \mathbb{R}^3\}$ 인 vertex position의 set이고, $F \subseteq V\times V\times V$ 인 triangular face의 set이다.

__3.1.1 Voxel Branch__

 voxel branch는 각 detect된 object의 coarse 3D shape을 주는 voxel occupancy probability grid를 예측한다. 이는 mask rcnn의 mask prediction branch의 3D analogue버전으로 보여질 수 있다. Mask RCNN에서 예측하는 $M \times M$ 이라기 보다, full 3D shape의 $ G \times G \times G$를 예측한다.

 Mask RCNN처럼, ROAlign의 결과로 받은 input feature map에 작은 fully convolutional network을 적용하여 input feature와 predicted voxel 사이의 연관성을 유지한다. 이러한 network는 input의 각 position에 해당하는 voxel occupancy score의 column을 주는 G channels로 이루어진 feature map을 만든다.

 ![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210228-4.PNG)

  image와 우리의 예측 사이에서 pixelwise 연관성을 유지하는 것은 복잡하다. 왜냐하면 camera에서 멀어질수록 object의 크기가 작아지기 떄문이다. Figure 4에 보이는 것 처럼, 우리는 이를 camera의 intrinsic matrix를 이용하여 frustum-shaped voxel을 예측했다.

__Cubify: Voxel to Mesh__

 voxel branch는 object의 coarse shape으로 주어지는 occupancy probability의 3D grid를 제공한다. 조금 더 양질의 3D shape을 예측하기 위해, 우리는 이러한 voxel prediction을 triangle mesh로 변환한다.(mesh refinement branch를 통과하기 위해)

 우리는 이러한 틈을 cubify라 불리는 작업으로 이었다. input은 voxel occupancy probability과 binary화 된 voxel occupancy를 threshold로 받는다. 각 occupied voxel은 8 vertices, 18 edges, 12 faces로 구성된 cuboid triangle mesh로 대체된다. adjacent occupied voxel 사이에서 공유된 vertice와 edge가 병합되고, 공유된 interior face는 제거된다. 이것은 voxel prediction에 의존하는 topology가 빈틈없는 mesh를 만들어낼 수 있도록 한다.

 Cubify는 효율적이고 batch형태로 되어야 한다. 이는 사소한 방법이 아니기때문에, 우리는 appendix에서 자세한 방법을 설명할 것이다. 비슷한 것으로 marching cube가 voxel grid에서 isosurface를 추출하는데, 이는 상당히 복잡하다.

__Voxel Loss__

 Voxel branch는 predicted voxel ouccpancy probability와 true voxel occupancy 사이의 binary cross entropy를 최소화 하도록 훈련된다.

__3.1.2 Mesh Refinement Branch__

 Cubified mesh는 coarse 3D shape만을 제공하기 떄문에, chair leg 처럼 정교한 형태의 model을 만들 수는 없다. mesh refinement branch는 처음 cubified mesh를 진행하고, 몇몇 refinement stage를 통해 vertex position을 refine한다. Pixel2Mesh와 유사하게, 각 refinement stage는 3가지 작동을 포함한다.

- vertex alignment: vertex를 만들기 위한 image feature extract
- graph convolution: mesh edge 사이 정보를 전바
- vertex refinement: vertex position update

 각 network의 layer는 각 mesh vertex의 3D position $v_i$ 와 feature vector $f_i$ 를 유지한다.

__Vertex Alignment__

 vertex alignment는 각 mesh vertex에서 image-aligned feature vector를 얻는 것이다. 우리는 camera intrinsic matrix를 사용해서 image plane에 각 vertex를 project 시킨다. 주어진 feature map을 이용해, 우리는 각 project된 vertex position에서 bilinearly interpolated image feature를 계산한다.

 mesh refinement branch의 첫번째 stage에서 VertAlign은 each vertex의 initial feature vector를 뽑아낸다. 그 이후의 stage에서 VertAlign은 이전 stage에서 온 vertex feature를 concat하게 된다.

__Graph Convolution__

 graph convolution은 mesh edge 사이의 정보를 전파하게 된다. 주어진 input vertex feature $\{f_i\}$ 는 update할 때 다음과 같은 식을 기준으로 한다.
$$
f_i' = ReLU(W_0 f_i + \sum_{j\in\mathcal{N}(i)}W_1 f_j)
$$
 위 식에서 $\mathcal{N}(i)$ 는 mesh에서 $i$ 번째 vertex 근처의 neighbor를 말하고, $W_0$와 $W_1$ 은 learned weight matrices이다. mesh refinement branch의 각 stage에서는 local mesh region에 대한 정보를 aggregate하기 위해 several graph convolution layer를 사용한다.

__Vertex Refinement__

 vertex refinement는 아래 식을 기준으로 vertex position을 update한다.
$$
v_i'=v_i+tanh(W_{vert}[f_i; v_i])
$$
 위 식에서 $W_{vert}$는 learned weight matrix이다. 이러한 update는 topology를 유지한 채 mesh geometry를 update하게 된다. 각 mesh refinement branch의 stage들은 vertex refinement를 끝내고 다음 stage에서 refine이 되는 mesh output을 내게 된다.

__Mesh Losses__

 triangle mesh에서 자연적으로 loss를 정의하는 것은 꽤나 도전적이다. 그래서 우리는 수많은 point들로 정의된 loss function을 대신해서 사용한다. 우리는 mesh를 surface로 sampling한 point cloud를 소개한다. 결과적으로, pointcloud loss가 전체적인 형태의 loss를 결정한다. mesh의 surface에서 오는 sample point를 위해 우리는 differentiable mesh sampling operation을 사용한다. 우리는 이를 efficient batched sampler로 해결하였다.(자세한 것은 appendix 참조) 우리는 이러한 operation을 gt mesh에서 pointcloud $P^{gt}$ 와 우리의 model에서 intermediate mesh prediction에서 온 point cloud $P^i$ 를 sample하기 위해 사용한다.

 normal vectors와 주어진 두 point cloud인 $P, Q$에 대해서, $\Lambda_{P,Q} = \{(p, argmin_q ||p-q||) : p \in P\}$ 를 Q 안에 있는 p의 nearest neighbor q로 이루어진 (p,q) 의 set으로 표현하고, $u_p$를 $p$의 unit normal이라고 하자. 그렇게 되면, point cloud P와 Q 사이의 chamfer distance(1)과 normal distance(2)는 다음과 같다.
$$
\mathcal{L}_{cham}(P,Q)=|P|^{-1} \sum_{(p,q)\in \Lambda_{P,Q}}||p-q||^2+|Q|^{-1} \sum_{(q,p) \in \Lambda_{Q,P}}||q-p||^2 \qquad (1)
$$

$$
\mathcal{L}_{norm}(P,Q)=-|P|^{-1} \sum_{(p,q)\in \Lambda_{P,Q}}|u_p \cdot u_q| - |Q|^{-1} \sum_{(q,p)\in \Lambda_{Q,P}}|u_q \cdot u_p | \qquad (2)
$$

  chamfer와 normal distance는 두 point cloud에서 position과 normal의 mismatch에 penalty를 받게 되지만, degenerate mesh를 통해 이러한 distance를 최소화 하게 된다.(해당 내용은 Figure 5에서 visualization된 것을 참조)

 높은 퀄리티의 mesh prediction은 추가적인 shape regularizers를 필요로 한다. 그래서 우리는 edge loss를 사용했다.
$$
\mathcal{L}_{edge}(V,E)= \frac{1}{|E|}\sum_{(v,v')\in E}||v-v'||^2 \qquad where \quad E\subseteq V \times V
$$
 위 edge loss는 predicted mesh의 edge에 사용된다. 대체적으로, Laplacian loss 역시 smoothness constraint를 수행한다.

 $i$ 번째 stage에서의 mesh loss는 위 loss들의 weight sum으로 이루어져 있다.



### Experiments

 우리는 ShapeNet에서 우리의 mesh predictor를 SOTA model들과 비교해봤고, Pix3D dataset에서 우리의 full Mesh R-CNN을 평가했다.

__4.1 ShapeNet__

__Evaluation__

__Implementation Details__

__BaseLines__

__Best vs Pretty__

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210228-5.PNG)

__Comparison with Prior Work__

 ![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210228-6.PNG)

__Ablation Study__

 ![img7](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210228-7.PNG)

 ![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210228-8.PNG)

__4.2. Pix3D__

__Evaluation__

__Implementation details__

__Comparison to Baselines__

 ![img9](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210228-9.PNG)

 ![img10](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210228-10.PNG)

 ![img11](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210228-11.PNG)

 ![img12](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210228-12.PNG)



### Discussion



### Reference

Gkioxari, Georgia, Jitendra Malik, and Justin Johnson. "Mesh r-cnn." *Proceedings of the IEEE/CVF International Conference on Computer Vision*. 2019.
