---
layout : post
title : "SPPNet"
date : 2020-12-17 +0900
description : SPPNet으로 알려진 Network의 논문인 Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition의 리뷰입니다.
tag : [PaperReview]
---

### SPPNet(Spatial Pyramid Pooling Net)



 RCNN을 Review 한 이후, 바로 Fast RCNN으로 넘어가려했으나, Fast RCNN에서 SPPNet의 구조를 많이 언급하기에 리뷰를 하게 되었다.



 전반적으로 요약하면, __고정된 input image의 크기를 don't care 하게 만든 Network의 시발점__ 이라고 할 수 있다.



 논문에도 나와있지만, 이전 RCNN의 경우도 마찬가지로, Convolutional Layer들 자체에는 input image의 size가 어떻게 되든 상관이 없었다. 그럼에도 불구하고 input image의 size가 fixed였던 이유는 fully connected layer(이하 fc layer)들 때문이다. (뭐 어떻게 하면 바꿀수 있지 않느냐! 할 수도 있는데, tensorflow나 pytorch 코드로 짜여진 걸 보면 안되는 걸 알 수 있다.)



 이걸 어떻게 바꾸었느냐 하면, 아래와 같다.

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20201217-1.png)

 기존 CNN based Network들은  input image size를 224x224(혹은 227x227)로 고정되도록 crop 혹은 warp 한 다음 conv layers, fc layers를 거친 후 feature vector(쉽게 말해 output)을 만들어 낸다.

 변환된 구조에서는, __한 개의 이미지를 그냥 통째로__ conv layers 로 넘겨버리고, 이 conv layers를 넘어온 다음 SPP 구조를 거쳐 fc layer에 적합한 input size로 feature vector들을 넘겨주고, 이 값들은 fc layer를 거쳐 output으로 나오게 된다.

 이 때, image size의 size에 상관없이 SPP를 통과하고 나온 feature vector의 크기는 항상 같게 된다. 어떻게? 아래 그림을 보자.

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20201217-2.png)

 input image가 convolutional layers를 거치고 나온 size가 SPP layer로 들어가게 되면, 미리 정해진 pooling layer로 들어가게 된다. 위 그림을 예로 하면, 4x4 pooling layer, 2x2 pooling layer, 1x1 pooling layer이다.

 이 pooling layer의 size는 상당히 가변이다. 5x5를 써도 되고, 3x3을 써도 되고, 125x125를 써도 되고, 129122x129122를 써도 된다.~~(뇌절)~~

 아무튼 예시만 바라보면, 1x1 pooling layer, 2x2 pooling layer, 4x4 pooling layer 처럼 이러한 구조를 spatial pyramid 라 하고, 그림에서 slice되어 작아진 네모 한칸을 bin이라고 한다.

 즉 위 그림에는 4x4 + 2x2 + 1x1 = 21개의 bin들이 존재한다.(fc layers로 연결되는 feature vector들)

 그래서 어떻게 input size에 상관없이 fixed-length representation(SPP layer의 output, 이하 fc input)를 만들 수 있는데요?

 ![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20201217-3.png)

 위 글을 해석하면, SPP layer의 input(conv layers를 거치고 나온 결과물)의 사이즈가 a x a 이고, pyramid level이 n x n의 bins를 가진다고 한다면, 해당 SPP의 결과물을 만들기 위한 window size와 stride는 각각

 __window size = ceiling(a/n)__

 __stride = floor(a/n)__

 이 된다. ceiling은 올림, floor는 내림이다.

 예를 들어서 13x13이 SPP layer의 input이고, (a = 13), SPP 안의 layer가 각각 4x4, 2x2, 1x1이라 하자. 그럼 4x4 bin의 경우, window size = ceiling(13/4) = 4, stride = floor(13/4) = 3 이 되고, 2x2 bin의 경우 window size = ceiling(13/2) = 7, stride = floor(13/2) = 6.... 이런식으로 된다.

 난 여기서 살짝 궁금한 점이 들었는데, input layer가 10x10이면 전부를 커버치지 못해서 아다리가 안맞지 않나...? 라는 생각이 들었다. 근데 그 아다리 안맞는건 그냥 버리고 가는 것 같다.

 요건 일단 아래에서 이야기를 해보자. 아무튼 이런 방식으로 하게 되면 input image size에 상관없이 output은 똑같은 크기를 가지게 되고, SVM을 이용해서 이를 구분하고 있다.

 object detection의 경우에는 GT(ground truth)를 positive로 잡고, negative는 positive와의 IoU가 30% 이하인 경우에 다른 negative와 70% 이상 overlapped 된 경우 제거하고 진행하였다고 한다. object detection의 경우 NMS는 threshold를 30%로 잡았다.

 그 이후 대부분은 어떤 대회에서 어떤 방식의 network를 썼고, 학습하는데 얼마가 걸렸고, 그 결과로 몇%의 mAP를 얻었다 등등이 내용인데, 논문의 내용도 그렇고 대부분 224x224에 가까운 input size일 수록 성능이 좋았다고 한다.



 이는 윗 문단에서 __근데 그 아다리 안맞는건 그냥 버리고 가는 것 같다__와 왠지 연결되는 것 같다.~~(뇌피셜)~~ 

 13x13이 SPP layer의 input이 되는 경우는 input image size가 224x224인 경우이고, 10x10이 SPP layer의 input이 되는 경우는 input image size가 180x180인 경우이다. 그래서 뭐가 중요하느냐?하면, 다른 input image scale에도 동일한 network를 썼다는 것이 맞다면, pooling 되지 못하는 feature vector들이 존재한다.(conv layer를 통과하고 SPP layer로 들어가는 feature vector들)

  논문을 살펴보면 저자가 사용한 케이스인 224x224와 180x180의 input image인 경우를 고려하면, 아래와 같은 표를 생각해낼 수 있다.

 ![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20201217-4.png)

 __버리고 간다__ 라는 의미를 살펴보면, 180x180 이 input size인 경우, conv layer를 지나면서 10x10으로 줄어들게 되고, 이 때 SPP에서 n을 생각해서 각자 계산을 때려보면, 6x6 SPP Layer에서는 아다리가 안맞게 된다.(ceiling(10/6) = 2, floor(10/6) = 1 이고, 여기서 이로 인해 덮히는 layer의 크기를 살펴보면 7x7이 되어버림) 결국, 이 때에는 49개의 feature map만 넘겨주게 되고, 그로 인해 나머지 51개(10x10 - 7x7)의 feature map은 버려지게 된다는 것이 나의 해석이다. 그러면 input image가 224x224에 가까울 수록 mAP등이 올라갔다는 것이 약간 설명이 되는 것 같다. 물론 어쩌다 아다리가 맞는 경우(After ConvLay의 feature vector의 크기와, SPP Layer를 통과할 때 filter가 덮게되는 feature vector의 크기가 같은 경우 - 위 표에선 input이 13x13일 때 window size와 stride를 통한 역계산에서 13x13의 모든 feature vector를 다 cover 치는 경우)의 input size가 또 있겠지만, 적어도 저자가 시도한 케이스에는 따로 없었기에 224x224의 size에 가까울 수록 mAP가 높지 않았을까 한다.

 물론 중간에 180x180을 섞어서 더 잘 나왔다는 케이스도 있는데, 이건 dropout이랑 어찌보면 비슷하다고 봐야하지 않을까? 했을 때 더 잘 될수도 있고, 더 안 될 수도 있었을테니.

 

### Reference

He, Kaiming, et al. "Spatial pyramid pooling in deep convolutional networks for visual recognition." *IEEE transactions on pattern analysis and machine intelligence* 37.9 (2015): 1904-1916.

갈아먹는 머신 러닝 님의 SPPNet 리뷰, [https://yeomko.tistory.com/14](https://yeomko.tistory.com/14)