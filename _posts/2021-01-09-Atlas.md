---
layout : post
title : "Atlas"
date : 2021-01-09 +1035
description : RGB Input만으로 3D Recon을 성공한 Atlas, End-to-End 3D Scene Reconstruction from Posed Images논문의 간단한 리뷰입니다.
tag : [PaperReview]
---

### Atlas: End-to-End 3D Scene Reconstruction from Posed Images



 이전의 3D 이미지 구축에서는 대부분 RGB-D를 쓰거나, 여기에 Camera의 Pose를 추가하여 조금 더 안정적인 형태를 추구하였으나, 최초로 Depth value없이 RGB와 Camera의 Pose만으로 3D Reconstruction을 이룬 내용이 담겨져있습니다.



 그 성능은 아직 탁월하지 않으나, 개선은 충분히 가능한 것으로 보입니다.



 추가로 해당 논문의 network를 구현한 코드는 아래 github에서 확인이 가능합니다.

[https://github.com/magicleap/Atlas](https://github.com/magicleap/Atlas)



### Abstract

Posed RGB Image에서 TSDF를 regressing하여 3D reconstruction을 목표로 하는, End-to-End Network를 보여줌.



### Introduction

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210109-1.png)

 3D Recon에서는 굉장히 다양한 발전이 있어왔음. 자율주행이나 증강현실 등, 많은 곳에서 정확한 3D Recon이 필요한데, 이럴때 사용되는 Depth 카메라나 LiDAR는 그 가격이 일반 RGB카메라에 비해 매우 비쌈. 제일 문제는 Occlusion이나 고르지 못한 surface로 인해 생기는 오류가 있음.

 이러한 연구를 monocular, binocular, multivew등으로 접근하는 경우가 많았음.

 우린 여기에 sequence of RGB images로 full 3D model을 만들어낼 수 있는 trainable 모델을 만들었음. 우리가 영감을 받은 2가지의 메인은 다음과 같음.

첫번째는 Cost volume based multi vew stereo(DPSNet, MVSNet)이고,

두번째는 TSDF refinement임(SGNN, Scancomplete - 둘 다 아직 안읽었음.)

(TSDF에 대한 내용은 이전 포스팅에서의 TSDF 관련 내용을 참조)



### Related Work

__3D Reconstruction__

 depth accumulation에서 흔하게 사용된 방법은 depth accumulation은 TSDF fusion을 이용한 voxel volume인데, 요새는 surfels(oriented point clouds)가 인기를 조금씩 얻고 있음.(Surfelmeshing, ElasticFusion)

 depth estimation은 큰 성장을 해왔지만, 아직도 한참 멀었음. SLAM에서도 비슷한 걸 해오고있음. COLMAP이나 CNN-SLAM. DeepMVS, MVDepthNet 등등 참조해보셈.

 R-MVSNet의 경우는 MVSNet에서 먹는 메모리를 줄여주었음.(3D CNN을 recurrent CNN으로 바꿔서)

 P-MVSNet의 경우는 point flow module을 사용해서 정제하는 방식을 선택.

 TSDF fusion도 여태까지 잘 연구되어왔는데, 사실 썩 그렇게 좋지는 않음.

 한 장, 혹은 그 이상의 이미지를 받아서 3D 형태를 예측하는 것으로 3D-R2N2(추후 리뷰 예정), Octtree-Gen, DeepSDF 등이 있음. 그 외에도 mesh를 응용한 몇몇 network들이 존재함.

__3D Semantic Segmentation__

 우리는 3D recon뿐 아니라, depth sensor 없이 3D semantic segmentation도 할 수 있는 것을 보여줄거임.(이건 현재 3d semantic segmentation이 어떤 식으로 진행되고 있는지 몰라서 얼마나 merit 있는지를 잘 모르겠음.)



### Method

 학습을 시킬 때에는 intrinsic, pose 등을 알고 있는 RGB input을 넣고, image feature들이 3D로 fuse 되면 3D CNN을 이용해서 TSDF를 regress함. 물론 3D semantic semgentation(이하 3D semseg)을 위해 따로 head를 추가했음.(여기서 Head는 결과물을 내는, 교체가 가능한 네트워크의 일부분을 의미. 네트워크 내의 Head 부분에 TSDF Head를 끼우면 TSDF output이 나오고, semseg Head를 끼우면 semseg output이 나옴)

__Feature Volume Construction__

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210109-2.png)

 대충 형태는 위와 같음.

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210109-3.png)

 이러한 형태를 만드는 것에 필요한 것은 위 식들과 같음.

 특이한 것은 어떤 지점의 Feature vector를 voxel map으로 back project 시킨 다는 점인데, 난 이 부분에서 깊은 깨달음을 얻었다.

 (3)과 (4)는 TSDF fusion을 조금 응용한 케이스인데, weighted average를 이용해서 voxel map을 그려나간다는 것을 확인할 수 있다. Weight는 voxel이 camera frustrum의 inside인지 outside인지 결정해주는데, 이걸 코드에서는 valid로 적어놨더라. 헷갈리게시리.

__3D Encoder-Decoder__

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210109-4.png)

 Encoder-Decoder Network는 대충 위처럼 생겼음. 해당 네트워크는 feature를 refine하고 TSDF를 regress하는 역할을 함. 코드로 보면 정말 알아먹기 힘들게 짜놨더라.



### Implementation Details

Resnet50-FPN을 사용했다고 함. 2D backbone이니 하는 것들이 있는데, 일단 이 부분은 code를 다루는 곳에서 조금 더 자세하게 살펴볼 것임.



### Results

 ScanNet의 dataset으로 평가했음. 비교대상은 COLMAP, MVDepthNet, GPMVS, DPSNet임.

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210109-5.png)

 좀 중요한 것 같은 것은, GT에서 입혀지지 않은 것들은 알아서 메꾸는 것을 확인할 수 있음.

__Inference Time__

 대충 무슨 GPU로 시간 얼마걸린다는 내용.



### Conclusions

 Depth Input없이 한 것은 상당히 의미가 있다고 봄.(나도 그러함)

 근데 semantic segmentation은 꼭 하고싶었나? 를 묻고 싶긴함. 아마 됬으니 그냥 추가한 거일테지만.



### Code Review

일단 시작하기 앞서, 이 친구들은 pytorch_lightning으로 코드를 짰다.

pytorch밖에 모르는(사실 파이토치도 잘 모른다.) 나는 이걸 어떻게 하나 싶었는데, 그냥 pytorch에서 기본적인 구조를 조금 더 간결하게 바꾼 형태라서, 아마 적응한다면 pytorch lightning(이하 pl)이 더 편한 것 같다. 원래 optimizer나 for(epoch)같은 구문들이 def main에 들어가있는데, 이걸 모두 model.py쪽으로 빼버리도록 바꿔서 조금 더 코드 보는 것이 편해진 것 같다.(pl site에서도 살펴보면 좀 더 eco...어쩌고 하는데 이건 잘 모르겠다.)

아무튼 github에서 코드를 받아서 살펴보자.

[https://github.com/magicleap/Atlas](https://github.com/magicleap/Atlas)

전체적으로는 atlas-master/atlas/안의 model.py, config.py, heads2d.py, heads3d.py, backbone2d.py, backbone3d.py 내용들이 주로 network를 이루고 있다. 다른 것들도 있는데, 코드를 짜면서 필요한 것들이나 tsdf형태로 바꿔주는 것들인 것 같으니 일단 생략.

 아래 코드에서 .py 파일들은 모두 atlas-master/atlas 폴더 내의 .py 파일들이다.

 보통 arg parser를 이용해서 조건을 입력받는데, 여기는 config.py 내부에 모든 조건들을 기록해뒀다.

train dataset은 어디에있고, test dataset은 어디에있고, batch size, voxel size, optimizer type, heads는 tsdf인지 3d semseg인지, backbone할 떄의 channel, layer는 어떤지 등등이 다 들어가있으니 읽다가 이게 뭐지? 하면 다시 돌아와서 읽으면 된다.

~~아니 무슨 register level 코딩하는 것도 아니고, 주소값 정리해놓는 것 마냥 해놔서 칩 코딩 하는 것 같다~~

 우리가 주로 살펴볼 내용은 전부 model.py 내부의 class VoxelNet이다.

 보면 __backbone2d, backbone3d, heads2d, heads3d__ 4개가 큰 틀로, 각 파일마다 따로 빼서 네트워크를 만들어 놨다. 추가로 __inference1, inference2__ 도 있는데, 일단 위에 4개부터 먼저 보자.

일단 간단한 요약을 먼저 하면,

__backbone2d__: Detectron2를 이용해서 feature extractor backbone network를 뽑아냄.(FPN)

왜 굳이 detectron2를 썼는지는 모르겠는데, 아무튼 2d feature extractor backbone을 만들었음.

__backbone2d_stride__: feature extractor할 때 사용한 stride.

__backbone3d__ = Encoder Decoder Network을 통과한 결괌루을 냄.

__heads2d__ = 이건 semseg 하는 module.... 흠... 왜 했지 이거?

__heads3d__ = cfg.MODEL.HEADS3D.HEADS에 따라 내뱉는 결과물이 다른데, default는 tsdf임.

__inference1__: 이건 2d image features를 3D로 backproject하고 accumulate하는 과정이 담겨져있음.근데 이 안에서 또 backproject라는 거를 따로 뺐음. 여기서 volume과 valid를 주면, inference에서 이걸 accumulate해주는 것.



__backbone2d__

```python
def build_backbone2d(cfg):
    """ Builds 2D feature extractor backbone network from Detectron2."""

    output_dim = cfg.MODEL.BACKBONE3D.CHANNELS[0]
    norm = cfg.MODEL.FPN.NORM
    output_stride = 4  # TODO: make configurable

    backbone = d2_build_backbone(cfg)
    feature_extractor = FPNFeature(
        backbone.output_shape(), output_dim, output_stride, norm)

    # load pretrained backbone
    if cfg.MODEL.BACKBONE.WEIGHTS:
        state_dict = torch.load(cfg.MODEL.BACKBONE.WEIGHTS)
        backbone.load_state_dict(state_dict)

    return nn.Sequential(backbone, feature_extractor), output_stride


class FPNFeature(nn.Module):
    """ Converts feature pyrimid to singe feature map (from Detectron2)"""
    
    def __init__(self, input_shape, output_dim=32, output_stride=4, norm='BN'):
        super().__init__()

        # fmt: off
        self.in_features      = ["p2", "p3", "p4", "p5"]
        feature_strides       = {k: v.stride for k, v in input_shape.items()}
        feature_channels      = {k: v.channels for k, v in input_shape.items()}
        # fmt: on

        self.scale_heads = []
        for in_feature in self.in_features:
            head_ops = []
            head_length = max(
                1, int(np.log2(feature_strides[in_feature]) - np.log2(output_stride))
            )
            for k in range(head_length):
                conv = Conv2d(
                    feature_channels[in_feature] if k == 0 else output_dim,
                    output_dim,
                    kernel_size=3,
                    stride=1,
                    padding=1,
                    bias=not norm,
                    norm=get_norm(norm, output_dim),
                    activation=F.relu,
                )
                weight_init.c2_msra_fill(conv)
                head_ops.append(conv)
                if feature_strides[in_feature] != output_stride:
                    head_ops.append(
                        nn.Upsample(scale_factor=2, mode="bilinear", align_corners=False)
                    )
            self.scale_heads.append(nn.Sequential(*head_ops))
            self.add_module(in_feature, self.scale_heads[-1])

    def forward(self, features):
        for i, f in enumerate(self.in_features):
            if i == 0:
                x = self.scale_heads[i](features[f])
            else:
                x = x + self.scale_heads[i](features[f])
        return x
```

 좀 궁금한건 여기서 detectron을 썼는데, 왜 썼는지는 잘 모르겠다. 그냥 편해서 이걸 썼겠지?

 여튼 심플하게 detectron2에서 뽑아온 FPN Network로 feature를 뽑아냄.

__backbone3d__

```python
def build_backbone3d(cfg):
    return EncoderDecoder(
        cfg.MODEL.BACKBONE3D.CHANNELS, cfg.MODEL.BACKBONE3D.LAYERS_DOWN,
        cfg.MODEL.BACKBONE3D.LAYERS, cfg.MODEL.BACKBONE3D.NORM,
        cfg.MODEL.BACKBONE3D.DROP, True, cfg.MODEL.BACKBONE3D.CONDITIONAL_SKIP
    )
```

 처음엔 여기서 시작함. Encoder Decoder는, feature volume refine하는 것에 씀.

내부에 basicblock3d가 있는데, 3x3x resnet basic block임. 이건 해당 코드를 직접 참조.(여기에 없음!)

```python
class EncoderDecoder(nn.Module):
    """ 3D network to refine feature volumes"""

    def __init__(self, channels=[32,64,128], layers_down=[1,2,3],
                 layers_up=[3,3,3], norm='BN', drop=0, zero_init_residual=True,
                 cond_proj=True):
        super(EncoderDecoder, self).__init__()

        self.cond_proj = cond_proj

        self.layers_down = nn.ModuleList()
        self.proj = nn.ModuleList()

        self.layers_down.append(nn.Sequential(*[
            BasicBlock3d(channels[0], channels[0], norm=norm, drop=drop) 
            for _ in range(layers_down[0]) ]))
        self.proj.append( ConditionalProjection(channels[0], norm, cond_proj) )
        for i in range(1,len(channels)):
            layer = [nn.Conv3d(channels[i-1], channels[i], 3, 2, 1, bias=(norm=='')),
                     get_norm_3d(norm, channels[i]),
                     nn.Dropout(drop, True),
                     nn.ReLU(inplace=True)]
            layer += [BasicBlock3d(channels[i], channels[i], norm=norm, drop=drop) 
                      for _ in range(layers_down[i])]
            self.layers_down.append(nn.Sequential(*layer))
            if i<len(channels)-1:
                self.proj.append( ConditionalProjection(channels[i], norm, cond_proj) )

        self.proj = self.proj[::-1]

        channels = channels[::-1]
        self.layers_up_conv = nn.ModuleList()
        self.layers_up_res = nn.ModuleList()
        for i in range(1,len(channels)):
            self.layers_up_conv.append( conv1x1x1(channels[i-1], channels[i]) )
            self.layers_up_res.append(nn.Sequential( *[
                BasicBlock3d(channels[i], channels[i], norm=norm, drop=drop) 
                for _ in range(layers_up[i-1]) ]))

        # Zero-initialize the last BN in each residual branch,
        # so that the residual branch starts with zeros, and each 
        # residual block behaves like an identity. This improves the 
        # model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677
        if zero_init_residual:
            for m in self.modules():
                if isinstance(m, BasicBlock3d):
                    nn.init.constant_(m.bn2.weight, 0)


    def forward(self, x):
        if self.cond_proj:
            valid_mask = (x!=0).any(1, keepdim=True).float()


        xs = []
        for layer in self.layers_down:
            x = layer(x)
            xs.append(x)

        xs = xs[::-1]
        out = []
        for i in range(len(self.layers_up_conv)):
            x = F.interpolate(x, scale_factor=2, mode='trilinear', align_corners=False)
            x = self.layers_up_conv[i](x)
            if self.cond_proj:
                scale = 1/2**(len(self.layers_up_conv)-i-1)
                mask = F.interpolate(valid_mask, scale_factor=scale)!=0 
            else:
                mask = None
            y = self.proj[i](xs[i+1], x, mask)
            x = (x + y)/2
            x = self.layers_up_res[i](x)

            out.append(x)

        return out
```

 PSM에서는 되게 잘 구현해놨는데, 여기서만 인지 모르겠지만 아무튼 생각보다 좀 복잡함.

__Heads2d__

```python
class PixelHeads(nn.Module):
    """ Module that contains all the 2D output heads
    
    Features extracted by the 2D network are passed to this to produce 
    intermeditate per-frame outputs. Each type of output is added as a head 
    and is responsible for returning a dict of outputs and a dict of losses.
    """

    def __init__(self, cfg, stride):
        super().__init__()
        self.heads = nn.ModuleList()

        if "semseg" in cfg.MODEL.HEADS2D.HEADS:
            self.heads.append(SemSegHead(cfg, stride))

    def forward(self, x, targets=None):
        outputs = {}
        losses = {}

        for head in self.heads:
            out, loss = head(x, targets)
            outputs = { **outputs, **out }
            losses = { **losses, **loss }

        return outputs, losses
```

2D output에 head를 달아서 output이랑 loss를 뽑음. semseg을 추가하는데, 2D image에서도 semseg을 추가하는 것 같음. 실제로 나중에 보면 비교할 때 2D semseg도 비교를 함. 개인적인 생각인데 굳이 필요한가 싶음.

__Heads3d__

```python
class VoxelHeads(nn.Module):
    """ Module that contains all the 3D output heads
    
    Features extracted by the 3D network are passed to this to produce the
    final outputs. Each type of output is added as a head and is responsible
    for returning a dict of outputs and a dict of losses
    """

    def __init__(self, cfg):
        super().__init__()
        self.heads = nn.ModuleList()

        if "tsdf" in cfg.MODEL.HEADS3D.HEADS:
            self.heads.append(TSDFHead(cfg))

        if "semseg" in cfg.MODEL.HEADS3D.HEADS:
            self.heads.append(SemSegHead(cfg))

        if "color" in cfg.MODEL.HEADS3D.HEADS:
            self.heads.append(ColorHead(cfg))


    def forward(self, x, targets=None):
        outputs = {}
        losses = {}

        for head in self.heads:
            out, loss = head(x, targets)
            outputs = { **outputs, **out }
            losses = { **losses, **loss }

        return outputs, losses
```

head가 뭐냐에 따라서 리턴하는 값이 달라지게 됨. default가 tsdf이므로 TSDFHead를 쓰는데, 코드가 너무 길어서 직접 참조하는 것이 좋아보임.

 tsdf value를 구하고 regress한 다음, loss를 구하는 과정이 나타나있음.

사실 이걸로 끝은 아니고, 다음에 postprocess를 거쳐야 tsdf가 제대로 나옴. 이 tsdf를 mesh로 따로 저장하는 것은 training_step에서 확인할 수 있음.

```python
    def postprocess(self, batch):
        """ Wraps the network output into a TSDF data structure
        
        Args:
            batch: dict containg network outputs

        Returns:
            list of TSDFs (one TSDF per scene in the batch)
        """
        
        key = 'vol_%02d'%self.voxel_sizes[0] # only get vol of final resolution
        out = []
        batch_size = len(batch[key+'_tsdf'])

        for i in range(batch_size):
            tsdf = TSDF(self.voxel_size, 
                        self.origin,
                        batch[key+'_tsdf'][i].squeeze(0))

            # add semseg vol
            if ('semseg' in self.voxel_types) and (key+'_semseg' in batch):
                semseg = batch[key+'_semseg'][i]
                if semseg.ndim==4:
                    semseg = semseg.argmax(0)
                tsdf.attribute_vols['semseg'] = semseg

            # add color vol
            if 'color' in self.voxel_types:
                color = batch[key+'_color'][i]
                tsdf.attribute_vols['color'] = color
            out.append(tsdf)

        return out
```

~~쓰다보니 code review가 아니라 나중에 되돌아서 읽기 편하게 대충 정리해놓은 느낌~~





### References

Murez, Zak, et al. "Atlas: End-to-End 3D Scene Reconstruction from Posed Images." *arXiv preprint arXiv:2003.10432* (2020).

Atlas Github, [https://github.com/magicleap/Atlas](https://github.com/magicleap/Atlas)