---
layout : post
title : "Pixel2Mesh++"
date : 2021-02-14 +2000
description : Pixel2Mesh++, Multi-View 3D Mesh Generation via Deformation 논문의 간단한 리뷰입니다.
tag : [PaperReview]
---

### Pixel2Mesh++: Multi-View 3D Mesh Generation via Deformation, ICCV 2019



### Abstract

 우리는 known camera pose와 몇몇 color image들로 부터 3D mesh를 구현하는 작업을 연구해왔다. 많은 이전의 연구들의 방법과는 달리, 우리는 graph CNN을 이용한 cross-view information을 이용하여 shape quaility를 improve하는 방법에 의지한다. image에서 3D shape으로 바로 mapping하는 것 대신에, 우리의 model은 coarse shape을 개선하기 위해 deformation 여러개를 반복적으로 예측한다.(??) traditional한 multiple view geometry 방법들에 영감을 받아, 우리의 network는 초기 mesh의 vertex 근처를 sample하고 multiple input image에서 만들어지는 perceptual feature statistics를 사용한 최적 변형을 찾아간다. 실험 결과는 우리의 모델이 정확한 3D shape을 찾아낼 뿐 아니라 임의의 viewpoint에서 촬영된 것도 mesh를 만들어낼 수 있음을 보여주었다. 물리적으로 구현된 architecture들의 도움으로, 우리의 model은 다른 category의 item이나, input image의 개수 등에 상관없이 작동하는 것을 확인할 수 있었다.



### 1. Introduction

 한 장의 input만으로 3D shape을 구현하는 것은 굉장히 어려운 일이다.(겹치는 부분이나 부족한 정보들로 인해)

 하지만 3~5장 정도의 사진으로는 충분히 3D shape을 만들 수 있다. 이는 traditional한 multi-view geometry 방법으로도 가능하다. 하지만 이러한 방법에서는 baseline이 커질수록 그 quality는 떨어졌다. 최근 DNN을 이용한 방법으로 3D shape을 복원하는 연구가 진행되고 있다.

![img1](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210214-1.PNG)

 우리는 Pixel2Mesh에서 정확한 3D geometry를 얻기 위해 사용했던 Graph Convolutional Network(GCN)과 함께 Multi-View Deformation Network(MDN)을 이용해서 새로운 architecture를 만들었다.

 

### 2. Related Work

__3D Shape Representations__

__Single View shape generation__

__Multi-view shape generation__



### 3. Methods

![img2](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210214-2.PNG)

 우리의 model은 서로 다른 viewpoint에서 찍힌(with known camera poses) color 이미지들을 받고, world-coordinate에서 3D mesh model을 만든다. 처음은 대충 모델을 만들고, 이후 더욱 detail하게 만들어진다. single image에서 온 3D shape generator들이 존재하기에, 우리는 Pixel2Mesh를 이용해서 coarse shape을 만들고, 이는 우리가 만든 MDN 의 input으로 들어가게 된다. MDN에서 각 vertex는 그 주변의 영역들을에서 오는 deformation hypotheses들을 sample한다.(아래 그림의 (a)) 각 hypothesis는 perceptual network에서 온 cross-view perceptual feature를 당기면서 feature resolution이 더 높아지고 low-level geometry infromation을 가지게 된다. 이러한 feature들은 vertex를 움직이는, 가장 적합한 deformation을 찾기 위해 network로부터 조절이 된다. MDN은 여러번 적용이 가능하여 그 적용 수가 많을수록 좋은 shape을 가지게 된다.

![img3](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210214-3.PNG)

__3.1. Multi-View Deformation Network__

 이번 section에서 우리는 MDN을 소개한다. 우리의 모델은 처음에 coarse한 형태의 shape을 내놓게 된다. 우리가 만든 model은 GCN이여서, 다른 GCN과도 같이 학습이 된다.(특히 Pixel2Mesh)

__3.1.1 Deformation Hypothesis Sampling__

 우리는 deformation을 하는 방법으로 Figure 3에 나와있는 방법을 이용했다. 각 vertex마다 크기 0.02의 20면체(icosahedron)를 만들어, 어떤 면으로 deformation이 일어나야하는지를 각 vertex마다 저장하게 된다. 42개의 hypothesis position이 만들어지게 되고, 여기서 43개의 node와 162개의 edge를 가진 graph가 만들어진다. 이렇게 만들어진 각 vertex마다의 local graph는 GCN의 input으로 들어간 이후 vertex의 움직임을 결정한다.

(icosahedron은 20면체이고, edge이 30개, vertex가 12개라서 42개의 hypothesis position이 만들어진다고 하는 것 같다. 이후 center point를 포함한 43개의 node와 162개의 edge를 가진 graph를 만든다고 하는데, 여기서 162 앞에 왜 120+42를 해놓았는지는 잘 모르겠다. edge 30, vertex 12이면 42에, 120은 위키에서 찾아보니 mesh로 나타날 때 120개의 triangle을 찾는다고 하는데, 여기에서 온 것인지는 확실하지 않다.)

Regular_icosahedron, wikipedia, [https://en.wikipedia.org/wiki/Regular_icosahedron](https://en.wikipedia.org/wiki/Regular_icosahedron)

__3.1.2 Cross-View Perceptual Feature Pooling__

 다음 step은 color image의 multiple input에서 온 각 node feature들을 정렬하는 것이다. Pixel2Mesh에 영감을 받아, 우리는 VGG-16 architecture를 이용해서 perceptual feature를 뽑아내기로 했다. 우리는 camera pose를 알고 있기 때문에, 각 vertex와 hypothesis는 그들의 projection point를 찾을 수 있게 된다.(Figure 3의 아래) 이 때 image의 scale이 다르기 때문에, 우리는 bilinear interpolation을 사용했다. Pixel2Mesh와 다른 점은 조금 더 좋은 shape을 배우기 위해 VGG의 마지막 layer에서 high-level feature를 사용한 반면 MDN에서는 early layer에서 feature를 뽑아내어 조금 더 높은 resolution을 가지고 detail한 information을 가지고 있다.

 multiple feature를 합하기 위해, concat이 자주 사용되어 왔다.(아마도 densenet에서 사용하는 방법인 듯) 하지만 결국에는 마지막 dimension이 input image의 수에 맞추어지도록 바꿔야했다. 이러한 문제를 해결하기 위해 multiple view shape recognition에서 statistics feature가 제안되었다. 이것에 영감을 받아, 우리는 각 vertex에서 모든 view로 부터 온 feature를 몇몇 statistics로 concat을 했다.(mean, max, std) 이러한 방법은 서로 다른 input의 순서에도 상관이 없고, 다양한 input에도 큰 문제가 되지 않는다. 또한, 각 feature vector가 아니라 cross-view feature에서 학습이 되도록 장려된다. image feature에 더해, 우리는 feature vector에 3-dimensional vertex coordinate을 합치게 된다. 전부 다 더하게 된다면, 우리는 각 vertex와 hypothesis에 대해서 총 1347개의 dimension feature vector를 계산한다.(어떻게 1347이 나왔지..?)

__3.1.3 Deformation Reasoning__

 다음 step으로는 hypotheses에서 온 vertex에서 적절한 deformation을 추론하는 것이다. best hypothesis는 argmax를 통해서 이루어지게 되고, 몇몇 neural network에서 사용되는 방법들을 적용했다. 그런데 몇몇 방법들 대신에 우리는 다른 것을 적용했다.(differentiable network을 위해서) 자세한 사항은 아래 그림에 나타나있다.

![img4](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210214-4.PNG)

 __3.2. Loss__

  우리는 supervised learning이고 3D CAD model을 GT로 썼다. 우리의 loss function은 Pixel2Mesh에서 왔으나, CD(Chamfer Distance)를 추가한 확장형이다. 여기서 우리는 chamfer loss를 계산할 때 re-parameterization trick을 사용한, random re-sample predicted mesh를 제안한다. 구체적으로, 3개의 vertices로 구성된 triangle인 $\{ v_1, v_2, v_3 \} \in \mathbb{R}^3$ 이 있다고 하면, uniform sampling은 다음과 같은 방법으로 진행이 된다.
$$
s = (1-\sqrt{r_1})v_1 + (1-r_2) \sqrt{r_1 v_2}+\sqrt{r_1}r_2v_1
$$
 에서 $s$는 triangle 내부의 point를 말하고, $r_1, r_2 \sim U[0,1]$ 이다.(Uniform distribution between 0 and 1 을 의미)

__3.3 Implementation Details__



### 4. Experiments

__4.1. Experimental setup__

__Dataset__

__Evaluation Metric__

__4.2 Comparison to Multi-view Shape Generation__

![img5](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210214-5.PNG)

![img11](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210214-11.PNG)

__4.3. Generalization Capability__

__4.3.1. Semantic Category__

![img6](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210214-6.PNG)

__4.3.2 Number of Views__

![img7](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210214-7.PNG)

__4.3.3 Initialization__

![img8](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210214-8.PNG)

__4.4. Ablation Study__

__4.4.1 Statistical Feature__

![img9](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210214-9.PNG)

__4.4.2 Re-sampled Chamfer Distance__

![img10](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210214-10.PNG)

__4.4.3 Number of Iteration__

![img12](https://raw.githubusercontent.com/ReaperMaKNaE/reapermaknae.github.io/main/assets/img/20210214-12.PNG)



### 5. Conclusion



### Reference

Wen, Chao, et al. "Pixel2mesh++: Multi-view 3d mesh generation via deformation." *Proceedings of the IEEE/CVF International Conference on Computer Vision*. 2019.

Regular_icosahedron, wikipedia, [https://en.wikipedia.org/wiki/Regular_icosahedron](https://en.wikipedia.org/wiki/Regular_icosahedron)